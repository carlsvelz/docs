

# SQL Server al Data Lake

## ğŸš€ **Pipeline: De SQL Server al Data Lake**

Ahora que âœ… hemos creado exitosamente nuestro **runtime** y probado las conexiones entre **Azure Data Factory (ADF)** y **SQL Server local**, ademÃ¡s de haber ejecutado una prueba con `Debug` para asegurarnos de que todo estÃ© funcionando correctamente ğŸ§ª, es momento de dar el siguiente gran paso:

â¡ï¸ **Crear el pipeline que moverÃ¡ toda nuestra base de datos de SQL Server al Data Lake.**

### ğŸ”¹ **Limpieza Inicial: Eliminando el archivo de prueba**



ğŸ§¹ Antes de continuar, vamos a eliminar el archivo `SalesLT.Address.parquet` que usamos en la prueba anterior. Este paso es importante para mantener nuestro entorno limpio y evitar confusiones al validar los resultados finales del pipeline completo.

#### âœ… Pasos para eliminar el archivo `.parquet` de prueba:

1. ğŸ–±ï¸ DirÃ­gete al contenedor del Data Lake donde se encuentra el archivo.
2. ğŸ” Busca el archivo: `SalesLT.Address.parquet`
3. ğŸ—‘ï¸ Selecciona el archivo y haz clic en **Delete**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img101.png)

4. ğŸ“¤ Se abrirÃ¡ una ventana emergente solicitando confirmaciÃ³n para eliminar el archivo.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img102.png)

5. âœ… Confirma la acciÃ³n.

ğŸ‰ Â¡Listo! El archivo fue eliminado exitosamente. Ahora estÃ¡s listo para construir tu pipeline.

---

#### ğŸ“ **Nota**

> Siempre que trabajes con pruebas previas, asegÃºrate de limpiar los resultados antes de pasar a procesos productivos. Esto evita datos duplicados, errores y confusiones durante la validaciÃ³n final.

---

## ğŸ“ **Copiar Todas las Tablas al Data Lake** 

Ahora sÃ­, Â¡es momento de la verdad! Vamos a crear un pipeline que copie **todas las tablas** desde **SQL Server** hasta el **Data Lake** utilizando **Azure Data Factory**. ğŸŒğŸ’¡

---

ğŸ› ï¸ **Pasos para Crear el Pipeline** ğŸ’»

### 1ï¸âƒ£ **Crear un Nuevo Pipeline**:

* DirÃ­gete a **Factory Resources** en el panel de Azure Data Factory.
* Haz clic en **Pipelines**, luego selecciona **Actons** y finalmente en **New Pipeline** ğŸ†•.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img103.png)

### 2ï¸âƒ£ **AÃ±adir la Actividad Lookup**:

* En **Activities**, ve a **General** y selecciona **Lookup**.
* Arrastra el **Lookup** al panel de trabajo.
* En la secciÃ³n de **Properties**, cambia el nombre del pipeline a **copy\_all\_tables** ğŸ“.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img104.png)

---

ğŸ” **Â¿QuÃ© Hace la Actividad Lookup?** ğŸ¤”

La actividad **Lookup** en **Azure Data Factory** se utiliza para recuperar o consultar datos desde una **fuente externa** (como una base de datos SQL o archivos). Esto nos permite obtener informaciÃ³n Ãºtil que podemos usar mÃ¡s adelante en el pipeline. ğŸ§‘â€ğŸ’»

**Â¿CÃ³mo Ayuda a Copiar Todas las Tablas al Data Lake?** ğŸ—‚ï¸

Para el pipeline **`copy_all_tables`**, el **Lookup** realizarÃ¡ lo siguiente:

1. **Consultar las Tablas Disponibles**:

   * El **Lookup** se conecta a la base de datos **AdventureWorksLT2022** y ejecuta una consulta para obtener el listado de **todas las tablas** que queremos copiar al **Data Lake**.

2. **Obtener los Nombres de las Tablas**:

   * La consulta devuelve una lista de **nombres de tablas**, lo que es crucial para procesarlas de forma dinÃ¡mica, sin necesidad de definir cada tabla manualmente.

3. **Iterar sobre las Tablas**:

   * Con los **nombres de las tablas**, podemos construir un flujo **dinÃ¡mico** que copie cada tabla a su archivo correspondiente en el **Azure Data Lake**.

4. **AutomatizaciÃ³n del Proceso**:

   * El uso de **Lookup** permite **automatizar** el proceso de copiar todas las tablas sin necesidad de escribir un proceso para cada tabla manualmente. El **Lookup** solo necesita configurarse para devolver los nombres de las tablas, y luego estas se procesan en un ciclo.

---

ğŸ“Œ **Notas Importantes** 

* **AutomatizaciÃ³n**: Usar **Lookup** nos ayuda a evitar la necesidad de escribir cÃ³digo para cada tabla, lo que hace el proceso mucho mÃ¡s eficiente y fÃ¡cil de mantener.
* **DinÃ¡mico**: Si se agregan nuevas tablas a la base de datos, el pipeline las copiarÃ¡ automÃ¡ticamente sin intervenciÃ³n manual.

---

âœ… **Pasos para Configurar el Pipeline**

| **Paso**                         | **AcciÃ³n Realizada** ğŸ“                                                                 | **Â¿Completado?** âœ… |
| -------------------------------- | --------------------------------------------------------------------------------------- | ------------------ |
| Crear Pipeline Nuevo             | Ir a **Factory Resources > Pipelines** y crear un nuevo pipeline.                       | âœ…                  |
| Configurar la Actividad Lookup   | Arrastrar **Lookup** al panel y cambiar el nombre del pipeline a **copy\_all\_tables**. | âœ…                  |
| Establecer ConexiÃ³n a SQL Server | Conectar el **Lookup** a la base de datos **AdventureWorksLT2022**.                     | âŒ                  |
| Consultar Tablas Disponibles     | Ejecutar la consulta para obtener los nombres de las tablas.                            | âŒ                  |
| Iterar sobre las Tablas          | Usar los nombres de las tablas para copiarlas al Data Lake.                             | âŒ                  |

---


### 3ï¸âƒ£**Configurar el Lookup**:

- **ConexiÃ³n de base de datos**: 
	- **General** cambia el nombre a **look for all tables** 

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img106.png)
- **Settings** Configura un nuevo **source dataset** busca `sql` y selecciona **SQL Server** 

	![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img105.png)

	- el Lookup para que se conecte a la base de datos **AdventureWorksLT2022** en **SQL Server**.
	- En **Set Properties**, agrega el nombre `SqlServerTables`. En **Linked Service**, selecciona la conexiÃ³n creada en los pasos anteriores (**onpremsqlserver**). En **Table name**, dÃ©jalo vacÃ­o, ya que no seleccionaremos una sola tabla, sino todas luego clic en `OK`

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img107.png)

- **Consulta**: Ahora, vamos a crear una consulta que debe obtener la lista de las tablas. PodrÃ­as utilizar una consulta SQL como la siguiente:

```sql
SELECT
    s.name AS SchemaName,
    t.name AS TableName
FROM sys.tables t
INNER JOIN sys.schemas s
    ON t.schema_id = s.schema_id
WHERE s.name = 'SalesLT';
```

- Esta query trae los nombres de las tablas y esquemas del esquema `SalesLT`.

- Ahora, probemos esta query en **DBeaver**: crea un nuevo script y ejecuta el cÃ³digo anterior.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img108.png)

- Esto seleccionarÃ¡ los nombres de todas las tablas en la base de datos.

âš™ï¸ **ConfiguraciÃ³n de la actividad Lookup**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img109.png)

1. **Selecciona la opciÃ³n "Query"**:
    
    - En lugar de seleccionar una tabla completa, elige la opciÃ³n **Query** para ejecutar una consulta SQL personalizada. Esto te permitirÃ¡ obtener los datos especÃ­ficos de la base de datos segÃºn la consulta que escribas.  
        ğŸ”½ **Â¿Por quÃ©?**  
        Esto es Ãºtil para trabajar con datos dinÃ¡micos o personalizados que no se limitan a una sola tabla.
        
2. **Pega tu consulta SQL**:
    
    - Copia y pega la consulta SQL que deseas ejecutar.
        
3. **Desmarcar "First row only"**:
    
    - En las opciones de la actividad, desmarca la opciÃ³n **First row only**.  
        ğŸ” **Â¿QuÃ© hace esto?**  
        Si dejas esta opciÃ³n seleccionada, ADF solo traerÃ¡ la primera fila de resultados. DesmarcÃ¡ndola, aseguras que **todas las filas** que devuelva la consulta sean obtenidas. Esto es fundamental si tu consulta retorna mÃºltiples filas de datos.
        
4. **Activar "Preview data"**:
    
    - Haz clic en el botÃ³n **Preview data** para **previsualizar los resultados** antes de ejecutar el pipeline completo.
    ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img110.png)
        
    - Esto es crucial para verificar que la consulta estÃ© funcionando correctamente y que los resultados sean los esperados.

**Debug:** Haz clic en el BotÃ³n **Debug** para **ejecutar el pipeline en modo prueba**, sin necesidad de publicarlo ni programarlo. Con esto puedes validar al instante si las actividades estÃ¡n configuradas correctamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img111.png)

En este caso, la actividad **Lookup** con la query devuelve todas las tablas del esquema `SalesLT`
El recuadro de la actividad **Lookup** muestra un **check verde**, indicando que la ejecuciÃ³n fue **exitosa**
Esto confirma que la query se ejecutÃ³ en SQL Server, que trajo los datos y que ADF pudo procesarlos sin errores

ğŸ”¶ Bloque: Iconos **Input / Output**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img112.png)


Estos iconos aparecen junto al nombre de la actividad despuÃ©s de ejecutar el pipeline y permiten abrir ventanas con la **informaciÃ³n detallada** de lo que ocurriÃ³ en esa actividad.

1. **â¡ï¸ Input Izquierda**
    
    - Muestra **quÃ© configuraciones y parÃ¡metros entraron a la actividad**.
        
    - Ejemplo:
        
        - El dataset usado (`SqlServerTables`).
            
        - La query ejecutada (`SELECT ... FROM sys.tables ...`).
            
        - Si â€œFirst row onlyâ€ estaba marcado o no.
            
        - Valores de parÃ¡metros (si los hubieras usado).
            
    - En pocas palabras: â€œlo que se le pasÃ³ a la actividad antes de ejecutarseâ€.
        
2. **â¬…ï¸ Output Derecha**
    
    - Muestra **los resultados devueltos por la actividad**.
        
    - En este caso, serÃ¡n las filas obtenidas por la consulta SQL: una lista (array) de objetos con `SchemaName` y `TableName`.


### 4ï¸âƒ£ **Usar los resultados**

Ahora que ya tienes lista la **actividad Lookup** que devuelve todas las tablas del esquema `SalesLT`, el siguiente paso es usar un **ForEach** en **Azure Data Factory** para iterar sobre cada una de esas tablas.

â•Agregar la actividad ForEach

- En  el panel de **actividades** en **Iteration & conditionals**:  
    âœ Arrastra y suelta **ForEach** y renombralo a **ForEach Shema Table**
    ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img113.png)
    
- ConÃ©ctalo a la salida del **Lookup** que acabas de configurar. **Selecciona el Ã­cono verde "Success" (âœ…)** desde **Lookup**. Este Ã­cono indica que puedes conectar otra actividad. Simplemente, arrastra con clic sostenido hacia **ForEach** y suelta para que se cree la conexiÃ³n automÃ¡ticamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img114.png)
>Esto activarÃ¡ el bloque **ForEach**, el cual recorrerÃ¡ la lista de tablas devuelta por la actividad **Lookup**. En nuestro caso, como **Lookup** encontrÃ³ **10 tablas**, **ForEach** ejecutarÃ¡ **10 iteraciones**, una por cada tabla, aplicando dentro del bucle la actividad que configuremos.

- Perfecto ğŸ‘Œ, ahora procedamos con la configuraciÃ³n de **ForEach** en la pestaÃ±a **Settings**, donde vamos a definir el contenido que se va a iterar (los Ã­tems).  

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img115.png)

1ï¸âƒ£ **Ir a Settings**

- Selecciona la actividad **ForEach**.
    
- Abre la pestaÃ±a **Settings**.
    

---

2ï¸âƒ£ **Items â†’ Add dynamic content**

- En el campo **Items** aparece el mensaje _This property should be parameterized_.
    
- AquÃ­ es donde debes indicar la colecciÃ³n que el ForEach recorrerÃ¡.
    
- Haz clic en **Add dynamic content** (o presiona `Alt+Shift+D`).
    

---

3ï¸âƒ£ **Pipeline Expression Builder (ventana derecha)**

- Se abre la ventana del **Expression Builder** para escribir expresiones dinÃ¡micas.
    
- En la lista de la derecha selecciona la salida del Lookup (resaltado en tu captura):  
    ğŸ‘‰ _Look for all tables â†’ Look for all tables activity output_.
    

---

4ï¸âƒ£ **Ajustar la expresiÃ³n**

- Al seleccionar esa opciÃ³n, ADF inserta en la caja de expresiÃ³n:
    
    ```json
    @activity('Look for all tables').output
    ```
    
- Sin embargo, necesitamos que ForEach recorra el **array de filas** devuelto, no todo el objeto output.
    
- Por eso, debes agregar `.value` al final:
    
    ```json
    @activity('Look for all tables').output.value
    ```
    
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img116.png)
    

âœ… Esto asegura que ForEach iterarÃ¡ sobre cada elemento de la colecciÃ³n (cada tabla encontrada en el esquema).

- Lookup devuelve algo asÃ­:
    
    ```json
    {
      "count": 3,
      "value": [
        { "SchemaName": "SalesLT", "TableName": "Customer" },
        { "SchemaName": "SalesLT", "TableName": "Product" },
        { "SchemaName": "SalesLT", "TableName": "SalesOrderHeader" }
      ]
    }
    ```
    
- `@activity('Look for all tables').output.value` â†’ entrega directamente el array dentro de `value`.
    
- ForEach recorrerÃ¡ ese array â†’ 3 iteraciones en este ejemplo.
    

---

ğŸ‘‰ Con esto ya tienes el ForEach listo para iterar por cada tabla.  

---

### 5ï¸âƒ£ **Configurar la Actividad Copy Data dentro del ForEach**

Una vez que tenemos el ciclo **ForEach**, que recorre todas las tablas obtenidas mediante el **Lookup**, debemos editar su contenido para aÃ±adir la actividad **Copy Data**.

A continuaciÃ³n, agregaremos y configuraremos la actividad **Copy Data** dentro del ciclo **ForEach**, con el objetivo de copiar cada tabla desde **SQL Server** hacia el **Data Lake**.

---

ğŸ§­ **Paso a paso:**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img119.png)

---

#### 1. **Accede al interior del ForEach**:

* Haz clic sobre el contenedor **ForEach**, y luego en el Ã­cono del **lÃ¡piz âœï¸** dentro del contenedor **ForEach Schema Table**.

* TambiÃ©n puedes hacer clic en la pestaÃ±a **Activities (0)** en la parte inferior y luego en el lÃ¡piz de la columna **Activity**.

* Esto abrirÃ¡ el diseÃ±o interno del ciclo **ForEach**, donde podrÃ¡s definir quÃ© actividades se ejecutarÃ¡n para cada tabla.

---

#### 2. **Agregar la actividad Copy Data**:

* En el panel de actividades dentro del **ForEach**, selecciona **Copy Data** y arrÃ¡strala al Ã¡rea de trabajo.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img120.png)

* Cambia el nombre de la actividad a **Copy Each Table**.

---

#### 3. **Configurar la Fuente (Source)**:

* En la pestaÃ±a **Source** de la actividad **Copy Data**, selecciona **New** para crear un nuevo **dataset** de **SQL Server**. Completa los campos con los siguientes valores:

  * [ ] **Name**: `SqlServerCopy`
  * [ ] **Linked service**: `onpremsqlserver`
  * [ ] **Table name**: dÃ©jalo vacÃ­o

* Luego haz clic en **OK** para guardar los cambios.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img121.png)

* En el campo **Query**, haz clic en **Add dynamic content**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img122.png)

Esto abrirÃ¡ el editor de expresiones dinÃ¡micas. AllÃ­ ingresaremos la siguiente expresiÃ³n:

```json
@{concat('SELECT * FROM ', item().SchemaName, '.', item().TableName)}
```

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img130.png)

ğŸ“Œ Â¿QuÃ© hace esta expresiÃ³n?

* `item()` hace referencia al **elemento actual** del ciclo **ForEach** (una tabla).
* `item().SchemaName`: obtiene el nombre del esquema.
* `item().TableName`: obtiene el nombre de la tabla.
* `concat(...)`: concatena el texto para formar una sentencia SQL vÃ¡lida del tipo:

```sql
SELECT * FROM schema_name.table_name
```

âœ… **Resultado esperado**:
Con esta configuraciÃ³n, la actividad **Copy Data** ejecutarÃ¡ dinÃ¡micamente una consulta distinta para cada tabla devuelta por el **Lookup**, permitiÃ©ndote copiar mÃºltiples tablas sin tener que configurarlas una por una.

Haz clic en **OK** para guardar la expresiÃ³n.

---

#### 4. **Configurar el Sink (Destino)**:

* En la pestaÃ±a **Sink**, selecciona el **New** de destino correspondiente al **Data Lake**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img124.png)

Selecciona **Parquet**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img131.png)

Configura los campos de la siguiente forma:

* [ ] **Name**: `parquetTables`
* [ ] **Linked service**: `AzureDataLakeStorage1`
* [ ] **File path**: `Bronze`

Haz clic en **OK** para guardar los cambios.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img125.png)









---


#### 5. **Estructura de carpetas en el Data Lake**

Ahora debemos generar la siguiente **estructura de carpetas en el Data Lake** para guardar los archivos `.parquet`:

```
bronze/SchemaName/TableName/TableName.parquet
```

Ejemplo concreto:

```
bronze/SalesLT/Address/Address.parquet
```

Para lograr esto en **Azure Data Factory (ADF)** al configurar el **Sink (Destino)** de tu actividad **Copy Data**, debes usar contenido dinÃ¡mico (Dynamic Content) en la propiedad **File path** del dataset de destino (tipo Azure Data Lake Storage Gen2).

---

#### âœ… **Configurar la Ruta DinÃ¡mica en el Sink:**

Ahora haz clic **Open** en la pestaÃ±a **Sink** (el que apunta al Data Lake **ParquetTables**), en el campo y lo que te llevarÃ¡ a nueva pestaÃ±a correspondiente al Dataset **ParquetTables**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img127.png)

ğŸ§­ Pasar los valores desde el pipeline

1. Vuelve a la actividad **Copy Each Table** en tu pipeline.
    
2. En la pestaÃ±a **Sink**, al usar el dataset `parquetTables`, verÃ¡s que se piden los parÃ¡metros `schemaname` y `tablename`.
    
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img128.png)

Una vez creados los parÃ¡metros `schemaname` y `tablename` dentro del dataset `parquetTables`, ahora debemos pasar sus valores dinÃ¡micamente desde la actividad **Copy Each Table**, para generar rutas dinÃ¡micas por tabla y esquema.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img129.png)


1. En la pestaÃ±a **Sink** de la actividad **Copy Each Table**, asegÃºrate de tener seleccionado el dataset `parquetTables`.
    
2. En la secciÃ³n **Dataset properties**, verÃ¡s los dos parÃ¡metros que creaste:
    
    - `schemaname`
        
    - `tablename`
        


âœ… **Configurar los parÃ¡metros del Sink (Dataset `parquetTables`)**

---

ğŸ”¶ **Paso 1: Configurar el parÃ¡metro `schemaname`**

1. Ve a la pestaÃ±a **Sink** de la actividad **Copy Each Table**.
    
2. AsegÃºrate de que el **Sink dataset** seleccionado sea `parquetTables`.
    
3. En la secciÃ³n **Dataset properties**, ubica el campo llamado `schemaname`.
    
4. Haz clic en el enlace **"Add dynamic content"** debajo del campo `Value`.
    
5. En la ventana del **Pipeline Expression Builder**, escribe o selecciona la siguiente expresiÃ³n:
    
    ```json
    @item().SchemaName
    ```
    
1. Haz clic en **OK** para guardar la expresiÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img132.png)

âœ… Con esto, el parÃ¡metro `schemaname` tomarÃ¡ el valor dinÃ¡mico de la propiedad `SchemaName` para cada tabla que se estÃ¡ procesando en el ciclo `ForEach`.

---

ğŸ”· **Paso 2: Configurar el parÃ¡metro `tablename`**

1. En la misma secciÃ³n de **Dataset properties**, ahora ubica el campo `tablename`.
    
2. Haz clic en **"Add dynamic content"** justo debajo del campo `Value`.
    
3. En la ventana del **Expression Builder**, escribe o selecciona la expresiÃ³n:
    
    ```json
    @item().TableName
    ```
    
1. Haz clic en **OK** para guardar la expresiÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img133.png)

âœ… Ahora, el parÃ¡metro `tablename` tambiÃ©n se llenarÃ¡ dinÃ¡micamente para cada tabla en el `ForEach`.

---

ğŸ¯ Resultado

Con ambos parÃ¡metros configurados dinÃ¡micamente, tu dataset `parquetTables` tendrÃ¡ suficiente informaciÃ³n para construir rutas del estilo:

```
bronze/SalesLT/Address/Address.parquet
```

Todo de manera automÃ¡tica, para **cada tabla** obtenida en el `Lookup`.

ğŸ§­ **Paso a paso para crear la ruta dinÃ¡mica en el Sink (Data Lake)**

ğŸ¯ Objetivo:

Lograr que los archivos `.parquet` se guarden con esta estructura:

```
bronze/SchemaName/TableName/TableName.parquet
```

Ejemplo:

```
bronze/SalesLT/Address/Address.parquet
```

ğŸ”¸ Paso 1: Abrir el Sink Dataset

En la actividad **Copy Each Table**:

1. Ve a la pestaÃ±a **Sink**.
    
2. Haz clic en el botÃ³n **Open** junto al dataset de salida llamado `parquetTables`.



![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img135.png)

ğŸ”¶ Esto abrirÃ¡ una nueva pestaÃ±a con la configuraciÃ³n del dataset `parquetTables`.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img134.png)

ğŸ”¸ Paso 2: Configurar la Ruta del Archivo DinÃ¡micamente

En la pestaÃ±a del dataset `parquetTables`:

1. Ve a la pestaÃ±a **Connection**.
    
2. En el campo **File path**, en la parte de **Directory**, haz clic en **Add dynamic content**.
    
3. En el editor de expresiones, escribe lo siguiente:
    

```json
@{concat(dataset().schemaname, '/', dataset().tablename)}
```

ğŸ“Œ Â¿QuÃ© hace esta expresiÃ³n?

- `dataset().schemaname` â†’ toma el valor del parÃ¡metro `schemaname`.
    
- `'/'` â†’ separa las carpetas.
    
- `dataset().tablename` â†’ toma el valor del parÃ¡metro `tablename`.
    

âœ… Resultado:  
Esto crea un path como:

```
bronze/SalesLT/Address
```

ğŸ”¸ Paso 3: Definir el Nombre del Archivo `.parquet`

En el mismo dataset, en el campo **File name**:

1. Haz clic en **Add dynamic content**.
    
2. Ingresa la siguiente expresiÃ³n:
    

```json
@{concat(dataset().tablename, '.parquet')}
```

ğŸ“Œ Â¿QuÃ© hace esta expresiÃ³n?

- Toma el nombre de la tabla (`tablename`).
    
- Le concatena la extensiÃ³n `.parquet`.
    

âœ… Resultado:  
El archivo se nombrarÃ¡ dinÃ¡micamente como:

```
Address.parquet
```
---

ğŸ“¦ Resultado final en Data Lake:

Con esta configuraciÃ³n, al ejecutar la actividad **Copy Each Table**, los archivos se copiarÃ¡n al Data Lake siguiendo esta estructura:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img136.png)

```
bronze/
â””â”€â”€ SchemaName/
    â””â”€â”€ TableName/
        â””â”€â”€ TableName.parquet
```

Ejemplo:

```
bronze/SalesLT/Address/Address.parquet
```



Ahora que hemos terminado de crear el Pipeline solo debemos guardar los cambios

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img137.png)



 **Prueba del Pipelin(Opcional)**

Una vez que todo estÃ© configurado, puedes utilizar el modo **Debug** en Azure Data Factory para ejecutar el pipeline de manera de prueba. Esto te permitirÃ¡ verificar que cada actividad estÃ© configurada correctamente y que el pipeline estÃ© copiando las tablas de SQL Server al Data Lake de manera exitosa.

---

### 6ï¸âƒ£ Ejecutar el pipeline con **Trigger now**

Este paso es clave para validar si tu pipeline estÃ¡ funcionando correctamente una vez que ha sido **publicado**. Ahora te explico cÃ³mo hacerlo:

---

1. AsegÃºrate de haber publicado tu pipeline

Antes de poder ejecutar tu pipeline con un trigger manual, debes hacer clic en **"Publish all"** (en la esquina superior izquierda) para guardar y publicar los cambios realizados en tu pipeline.

---

2. Haz clic en **"Add trigger"**

En la parte superior del editor, ubica el botÃ³n **Add trigger** (Ã­cono de rayo) y haz clic en Ã©l.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img138.png)


---

4. Selecciona la opciÃ³n **Trigger now**

Se desplegarÃ¡ un pequeÃ±o menÃº. Haz clic en la opciÃ³n **Trigger now**, como se muestra en la imagen.  
ğŸ”¸ Esta opciÃ³n ejecutarÃ¡ el pipeline inmediatamente, usando su configuraciÃ³n actual y sin necesidad de crear un trigger programado.

5. Confirma la ejecuciÃ³n del pipeline

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img139.png)

ğŸ“Œ En la ventana **Pipeline run**, verÃ¡s un mensaje que dice:

**"Trigger pipeline now using last published configuration."**

Esto indica que el pipeline se ejecutarÃ¡ con la Ãºltima versiÃ³n publicada y sin ningÃºn parÃ¡metro adicional (en este caso, no hay registros en la tabla de parÃ¡metros).

---

6. Haz clic en **OK** para continuar

âœ… Solo necesitas confirmar haciendo clic en el botÃ³n azul **OK** (como se resalta en la imagen).

Esto lanzarÃ¡ la ejecuciÃ³n del pipeline inmediatamente y verÃ¡s una ventana emergente con el estado **Running** lo que confirma que el Pipeline esta en ejecuciÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img140.png)

---

#### ğŸ” 6. Monitorea la ejecuciÃ³n

â¡ï¸ DirÃ­gete a la secciÃ³n **"Monitor"** en el menÃº izquierdo de Azure Data Factory para verificar el progreso del pipeline.

ğŸ“Œ AquÃ­ podrÃ¡s:

* Visualizar si todo se ejecutÃ³ correctamente.
* Identificar si hubo errores.
* Verificar duraciÃ³n, origen y estado de ejecuciÃ³n.

âœ… Para hacerlo, navega a:
**`Monitor > Pipeline runs`**

ğŸ”„ AquÃ­ observarÃ¡s el estado actual de tus ejecuciones:

* ğŸŸ¡ **In progress**: En ejecuciÃ³n
* âœ… **Succeeded**: EjecuciÃ³n completada exitosamente
* âŒ **Failed**: EjecuciÃ³n fallida

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img141.png)

---

ğŸ“Š Â¿QuÃ© informaciÃ³n verÃ¡s en esta secciÃ³n?

ğŸ§¾ En esta vista encontrarÃ¡s un historial de ejecuciones del pipeline. Por ejemplo:

ğŸ“ **Ejemplo de ejecuciÃ³n:**

| Campo              | Valor                                             |
| ------------------ | ------------------------------------------------- |
| ğŸ·ï¸ Pipeline       | `copy_all_tables`                                 |
| ğŸ•’ Hora de inicio  | 03/09/2025 - 11:52:31 AM                          |
| ğŸ”„ Estado          | `In progress` (en progreso al momento de captura) |
| â±ï¸ DuraciÃ³n actual | 24 segundos                                       |
| ğŸ”˜ Trigger type    | `Manual trigger`                                  |
| ğŸ†” Run ID          | Identificador Ãºnico para esta ejecuciÃ³n           |

âœ”ï¸ Esta vista permite:

* Dar seguimiento al estado del pipeline en tiempo real.
* Validar la duraciÃ³n de ejecuciÃ³n.
* Identificar el origen del trigger.
* Obtener detalles Ãºtiles para monitoreo y depuraciÃ³n (**debugging**).

---

#### âœ… ActualizaciÃ³n: Pipeline completado

ğŸ“¥ Luego de actualizar la vista, se puede verificar que el pipeline `copy_all_tables` **ya finalizÃ³ su ejecuciÃ³n**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img142.png)

ğŸ“Œ Detalles de la ejecuciÃ³n final:

* â–¶ï¸ Ejecutado manualmente el **3 de septiembre de 2025 a las 11:52:31 AM**
* â¹ï¸ Finalizado a las **11:53:24 AM**
* â±ï¸ DuraciÃ³n total: **53 segundos**
* âœ… Estado: **"Succeeded"** (completado sin errores)
* ğŸ§© Triggered by: **Manual trigger**
* ğŸ†” Run ID: identificador Ãºnico generado automÃ¡ticamente

---

ğŸ“ **NOTAS:**

* Esta secciÃ³n es crucial para el **monitoreo post-ejecuciÃ³n**.
* Permite detectar errores de forma temprana o confirmar que el proceso corriÃ³ exitosamente.
* Ideal para entornos de **despliegue**, **QA**, y **producciÃ³n** donde la trazabilidad y visibilidad son clave.

---

ğŸ’¡ **TIP**: Puedes hacer clic sobre cualquier ejecuciÃ³n para ver un desglose completo de las actividades dentro del pipeline, con tiempos individuales y logs detallados.



### ğŸ“‚ **Data Lake ValidaciÃ³n**  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img143.png)

âœ… Una vez que el pipeline finaliza correctamente, pasamos a validar que los datos hayan llegado al destino correcto: nuestro Data Lake.

ğŸ“ Ingresamos al recurso de almacenamiento `z2hdatalakesuffix`  
ğŸ“ Navegamos al **Storage Browser**  
ğŸ“ Seleccionamos el contenedor **bronze**  
ğŸ“ Entramos a la carpeta **SalesLT** (donde configuramos que se guarden las tablas)

âœ”ï¸ AllÃ­ encontramos las siguientes carpetas, que representan las tablas copiadas:

- ğŸ“ Address
    
- ğŸ“ Customer
    
- ğŸ“ CustomerAddress
    
- ğŸ“ Product
    
- ğŸ“ ProductCategory
    
- ğŸ“ ProductDescription
    
- ğŸ“ ProductModel
    
- ğŸ“ ProductModelProductDescription
    
- ğŸ“ SalesOrderDetail
    
- ğŸ“ SalesOrderHeader
    

ğŸ“NOTA: La hora de modificaciÃ³n de todas estas carpetas coincide con la ejecuciÃ³n del pipeline, lo que confirma que la operaciÃ³n se realizÃ³ de forma exitosa.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ“„ **VALIDACIÃ“N FINAL: CONTENIDO DE LAS CARPETAS**  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img144.png)

ğŸ” Finalmente, ingresamos dentro de una de las carpetas generadas, en este caso:  
ğŸ“ **Address**

âœ”ï¸ Dentro de ella encontramos el archivo:  
ğŸ“„ `Address.parquet`  
ğŸ“… Fecha: 9/3/2025 - 11:53:13 AM  
ğŸ”¥ Access tier: Hot  
ğŸ“¦ Tipo: Block blob  
ğŸ“ TamaÃ±o: 35.08 KiB

ğŸ“NOTA: Este archivo en formato `.parquet` contiene los datos de la tabla `Address` extraÃ­dos desde la base de datos SQL y almacenados de forma Ã³ptima para anÃ¡lisis o transformaciÃ³n posterior.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  
ğŸ’¡ **Â¿QuÃ© logramos hasta ahora?**  
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ”ï¸ Automatizamos la extracciÃ³n y carga de todas las tablas del esquema `SalesLT` desde SQL a Data Lake.  
âœ”ï¸ Utilizamos un pipeline parametrizado que recorre las tablas dinÃ¡micamente.  
âœ”ï¸ Validamos la ejecuciÃ³n en tiempo real y monitoreamos el proceso desde Azure Data Factory.  
âœ”ï¸ Confirmamos que los datos ya se encuentran organizados en nuestro contenedor `bronze`.

---

### **Flujo del Pipeline:**

- **Lookup** obtiene las tablas.
    
- **ForEach** (u otra actividad de iteraciÃ³n) se usa para procesar cada tabla.
    
- **Copy Activity** se utiliza para copiar los datos de cada tabla al Data Lake.
    

---

AnÃ¡lisis:

1. **Lookup**  
    âœ”ï¸ Correcto, esta actividad se usa para obtener la lista de tablas desde la fuente.
    
2. **ForEach**  
    âœ”ï¸ Correcto, es la actividad que permite iterar sobre cada tabla obtenida por el Lookup.
    
3. **Copy Activity**  
    âœ”ï¸ Correcto, es la actividad que efectivamente realiza la copia de datos desde la tabla fuente al Data Lake.
    

---
### ğŸš€ **Â¿EstÃ¡ completo el flujo?** ğŸš€

El flujo bÃ¡sico para copiar mÃºltiples tablas de una fuente a un Data Lake estÃ¡ correctamente planteado âœ….

---

ğŸ” **Pero, segÃºn buenas prÃ¡cticas y escenarios reales, se podrÃ­an agregar mÃ¡s elementos para mayor robustez:**

â¡ï¸ **Control de errores (Error handling):**  
â“ Â¿QuÃ© pasa si falla alguna tabla?  
â¡ï¸ Agrega actividades para manejo de errores o lÃ³gica condicional para continuar o alertar.

â¡ï¸ **Validaciones o transformaciones:**  
ğŸ› ï¸ En algunos pipelines se incluye una actividad previa o posterior para validar o transformar datos.

â¡ï¸ **ParÃ¡metros dinÃ¡micos:**  
ğŸ›ï¸ Para hacer el pipeline mÃ¡s reusable y dinÃ¡mico.

â¡ï¸ **Triggers:**  
â° Para automatizar la ejecuciÃ³n (aunque aquÃ­ fue manual, puede estar presente).

---

ğŸ“‹ **Resumen en tabla:**

|Elemento|Â¿Incluido?|Comentarios|
|---|---|---|
|Lookup|âœ…|Obtiene las tablas desde la fuente|
|ForEach|âœ…|Itera sobre cada tabla|
|Copy Activity|âœ…|Copia los datos al Data Lake|
|Control de errores|âŒ|Recomendado para robustez|
|Validaciones/Transform|âŒ|Opcional, pero Ãºtil para limpieza o ajuste|
|ParÃ¡metros dinÃ¡micos|âŒ|Facilita reutilizaciÃ³n|
|Triggers|âŒ|Automatiza ejecuciÃ³n|

---

ğŸ“ **NOTAS:**

- Este flujo es **bÃ¡sico y funcional**, ideal para principiantes en IngenierÃ­a de Datos en la nube.
    
- MÃ¡s adelante, replicaremos este proyecto a nivel de cÃ³digo Python para que aprendas desde cero y con un stack open source cÃ³mo replicar todo esto 100% en Python y SQL.
	
- Las herramientas No-Code como Azure Data Factory abstraen el cÃ³digo, pero para entender a fondo y controlar mejor los pipelines, es fundamental aprender a programar los flujos.
	
- Conocer Python para construir pipelines te da flexibilidad y poder para optimizar y personalizar tu soluciÃ³n.

---

âœ… **ConclusiÃ³n:**  
Este pipeline bÃ¡sico funciona y es el primer paso para entender el proceso. Pero para flujos mÃ¡s robustos y reales, implementar manejo de errores, validaciones, parametrizaciÃ³n y automatizaciÃ³n es clave.


