

# SQL Server al Data Lake

## üöÄ **Pipeline: De SQL Server al Data Lake**

Ahora que ‚úÖ hemos creado exitosamente nuestro **runtime** y probado las conexiones entre **Azure Data Factory (ADF)** y **SQL Server local**, adem√°s de haber ejecutado una prueba con `Debug` para asegurarnos de que todo est√© funcionando correctamente üß™, es momento de dar el siguiente gran paso:

‚û°Ô∏è **Crear el pipeline que mover√° toda nuestra base de datos de SQL Server al Data Lake.**

### üîπ **Limpieza Inicial: Eliminando el archivo de prueba**



üßπ Antes de continuar, vamos a eliminar el archivo `SalesLT.Address.parquet` que usamos en la prueba anterior. Este paso es importante para mantener nuestro entorno limpio y evitar confusiones al validar los resultados finales del pipeline completo.

#### ‚úÖ Pasos para eliminar el archivo `.parquet` de prueba:

1. üñ±Ô∏è Dir√≠gete al contenedor del Data Lake donde se encuentra el archivo.
2. üîç Busca el archivo: `SalesLT.Address.parquet`
3. üóëÔ∏è Selecciona el archivo y haz clic en **Delete**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img101.png)

4. üì§ Se abrir√° una ventana emergente solicitando confirmaci√≥n para eliminar el archivo.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img102.png)

5. ‚úÖ Confirma la acci√≥n.

üéâ ¬°Listo! El archivo fue eliminado exitosamente. Ahora est√°s listo para construir tu pipeline.

---

#### üìù **Nota**

> Siempre que trabajes con pruebas previas, aseg√∫rate de limpiar los resultados antes de pasar a procesos productivos. Esto evita datos duplicados, errores y confusiones durante la validaci√≥n final.

---

## üìù **Copiar Todas las Tablas al Data Lake** 

Ahora s√≠, ¬°es momento de la verdad! Vamos a crear un pipeline que copie **todas las tablas** desde **SQL Server** hasta el **Data Lake** utilizando **Azure Data Factory**. üåêüí°

---

üõ†Ô∏è **Pasos para Crear el Pipeline** üíª

### 1Ô∏è‚É£ **Crear un Nuevo Pipeline**:

* Dir√≠gete a **Factory Resources** en el panel de Azure Data Factory.
* Haz clic en **Pipelines**, luego selecciona **Actons** y finalmente en **New Pipeline** üÜï.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img103.png)

### 2Ô∏è‚É£ **A√±adir la Actividad Lookup**:

* En **Activities**, ve a **General** y selecciona **Lookup**.
* Arrastra el **Lookup** al panel de trabajo.
* En la secci√≥n de **Properties**, cambia el nombre del pipeline a **copy\_all\_tables** üìù.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img104.png)

---

üîç **¬øQu√© Hace la Actividad Lookup?** ü§î

La actividad **Lookup** en **Azure Data Factory** se utiliza para recuperar o consultar datos desde una **fuente externa** (como una base de datos SQL o archivos). Esto nos permite obtener informaci√≥n √∫til que podemos usar m√°s adelante en el pipeline. üßë‚Äçüíª

**¬øC√≥mo Ayuda a Copiar Todas las Tablas al Data Lake?** üóÇÔ∏è

Para el pipeline **`copy_all_tables`**, el **Lookup** realizar√° lo siguiente:

1. **Consultar las Tablas Disponibles**:

   * El **Lookup** se conecta a la base de datos **AdventureWorksLT2022** y ejecuta una consulta para obtener el listado de **todas las tablas** que queremos copiar al **Data Lake**.

2. **Obtener los Nombres de las Tablas**:

   * La consulta devuelve una lista de **nombres de tablas**, lo que es crucial para procesarlas de forma din√°mica, sin necesidad de definir cada tabla manualmente.

3. **Iterar sobre las Tablas**:

   * Con los **nombres de las tablas**, podemos construir un flujo **din√°mico** que copie cada tabla a su archivo correspondiente en el **Azure Data Lake**.

4. **Automatizaci√≥n del Proceso**:

   * El uso de **Lookup** permite **automatizar** el proceso de copiar todas las tablas sin necesidad de escribir un proceso para cada tabla manualmente. El **Lookup** solo necesita configurarse para devolver los nombres de las tablas, y luego estas se procesan en un ciclo.

---

üìå **Notas Importantes** 

* **Automatizaci√≥n**: Usar **Lookup** nos ayuda a evitar la necesidad de escribir c√≥digo para cada tabla, lo que hace el proceso mucho m√°s eficiente y f√°cil de mantener.
* **Din√°mico**: Si se agregan nuevas tablas a la base de datos, el pipeline las copiar√° autom√°ticamente sin intervenci√≥n manual.

---

‚úÖ **Pasos para Configurar el Pipeline**

| **Paso**                         | **Acci√≥n Realizada** üìù                                                                 | **¬øCompletado?** ‚úÖ |
| -------------------------------- | --------------------------------------------------------------------------------------- | ------------------ |
| Crear Pipeline Nuevo             | Ir a **Factory Resources > Pipelines** y crear un nuevo pipeline.                       | ‚úÖ                  |
| Configurar la Actividad Lookup   | Arrastrar **Lookup** al panel y cambiar el nombre del pipeline a **copy\_all\_tables**. | ‚úÖ                  |
| Establecer Conexi√≥n a SQL Server | Conectar el **Lookup** a la base de datos **AdventureWorksLT2022**.                     | ‚ùå                  |
| Consultar Tablas Disponibles     | Ejecutar la consulta para obtener los nombres de las tablas.                            | ‚ùå                  |
| Iterar sobre las Tablas          | Usar los nombres de las tablas para copiarlas al Data Lake.                             | ‚ùå                  |

---


### 3Ô∏è‚É£**Configurar el Lookup**:

- **Conexi√≥n de base de datos**: 
	- **General** cambia el nombre a **look for all tables** 

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img106.png)
- **Settings** Configura un nuevo **source dataset** busca `sql` y selecciona **SQL Server** 

	![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img105.png)

	- el Lookup para que se conecte a la base de datos **AdventureWorksLT2022** en **SQL Server**.
	- En **Set Properties**, agrega el nombre `SqlServerTables`. En **Linked Service**, selecciona la conexi√≥n creada en los pasos anteriores (**onpremsqlserver**). En **Table name**, d√©jalo vac√≠o, ya que no seleccionaremos una sola tabla, sino todas luego clic en `OK`

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img107.png)

- **Consulta**: Ahora, vamos a crear una consulta que debe obtener la lista de las tablas. Podr√≠as utilizar una consulta SQL como la siguiente:

```sql
SELECT
    s.name AS SchemaName,
    t.name AS TableName
FROM sys.tables t
INNER JOIN sys.schemas s
    ON t.schema_id = s.schema_id
WHERE s.name = 'SalesLT';
```

- Esta query trae los nombres de las tablas y esquemas del esquema `SalesLT`.

- Ahora, probemos esta query en **DBeaver**: crea un nuevo script y ejecuta el c√≥digo anterior.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img108.png)

- Esto seleccionar√° los nombres de todas las tablas en la base de datos.

‚öôÔ∏è **Configuraci√≥n de la actividad Lookup**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img109.png)

1. **Selecciona la opci√≥n "Query"**:
    
    - En lugar de seleccionar una tabla completa, elige la opci√≥n **Query** para ejecutar una consulta SQL personalizada. Esto te permitir√° obtener los datos espec√≠ficos de la base de datos seg√∫n la consulta que escribas.  
        üîΩ **¬øPor qu√©?**  
        Esto es √∫til para trabajar con datos din√°micos o personalizados que no se limitan a una sola tabla.
        
2. **Pega tu consulta SQL**:
    
    - Copia y pega la consulta SQL que deseas ejecutar.
        
3. **Desmarcar "First row only"**:
    
    - En las opciones de la actividad, desmarca la opci√≥n **First row only**.  
        üîç **¬øQu√© hace esto?**  
        Si dejas esta opci√≥n seleccionada, ADF solo traer√° la primera fila de resultados. Desmarc√°ndola, aseguras que **todas las filas** que devuelva la consulta sean obtenidas. Esto es fundamental si tu consulta retorna m√∫ltiples filas de datos.
        
4. **Activar "Preview data"**:
    
    - Haz clic en el bot√≥n **Preview data** para **previsualizar los resultados** antes de ejecutar el pipeline completo.
    ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img110.png)
        
    - Esto es crucial para verificar que la consulta est√© funcionando correctamente y que los resultados sean los esperados.

**Debug:** Haz clic en el Bot√≥n **Debug** para **ejecutar el pipeline en modo prueba**, sin necesidad de publicarlo ni programarlo. Con esto puedes validar al instante si las actividades est√°n configuradas correctamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img111.png)

En este caso, la actividad **Lookup** con la query devuelve todas las tablas del esquema `SalesLT`
El recuadro de la actividad **Lookup** muestra un **check verde**, indicando que la ejecuci√≥n fue **exitosa**
Esto confirma que la query se ejecut√≥ en SQL Server, que trajo los datos y que ADF pudo procesarlos sin errores

üî∂ Bloque: Iconos **Input / Output**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img112.png)


Estos iconos aparecen junto al nombre de la actividad despu√©s de ejecutar el pipeline y permiten abrir ventanas con la **informaci√≥n detallada** de lo que ocurri√≥ en esa actividad.

1. **‚û°Ô∏è Input Izquierda**
    
    - Muestra **qu√© configuraciones y par√°metros entraron a la actividad**.
        
    - Ejemplo:
        
        - El dataset usado (`SqlServerTables`).
            
        - La query ejecutada (`SELECT ... FROM sys.tables ...`).
            
        - Si ‚ÄúFirst row only‚Äù estaba marcado o no.
            
        - Valores de par√°metros (si los hubieras usado).
            
    - En pocas palabras: ‚Äúlo que se le pas√≥ a la actividad antes de ejecutarse‚Äù.
        
2. **‚¨ÖÔ∏è Output Derecha**
    
    - Muestra **los resultados devueltos por la actividad**.
        
    - En este caso, ser√°n las filas obtenidas por la consulta SQL: una lista (array) de objetos con `SchemaName` y `TableName`.


### 4Ô∏è‚É£ **Usar los resultados**

Ahora que ya tienes lista la **actividad Lookup** que devuelve todas las tablas del esquema `SalesLT`, el siguiente paso es usar un **ForEach** en **Azure Data Factory** para iterar sobre cada una de esas tablas.

‚ûïAgregar la actividad ForEach

- En  el panel de **actividades** en **Iteration & conditionals**:  
    ‚ûú Arrastra y suelta **ForEach** y renombralo a **ForEach Shema Table**
    ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img113.png)
    
- Con√©ctalo a la salida del **Lookup** que acabas de configurar. **Selecciona el √≠cono verde "Success" (‚úÖ)** desde **Lookup**. Este √≠cono indica que puedes conectar otra actividad. Simplemente, arrastra con clic sostenido hacia **ForEach** y suelta para que se cree la conexi√≥n autom√°ticamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img114.png)
>Esto activar√° el bloque **ForEach**, el cual recorrer√° la lista de tablas devuelta por la actividad **Lookup**. En nuestro caso, como **Lookup** encontr√≥ **10 tablas**, **ForEach** ejecutar√° **10 iteraciones**, una por cada tabla, aplicando dentro del bucle la actividad que configuremos.

- Perfecto üëå, ahora procedamos con la configuraci√≥n de **ForEach** en la pesta√±a **Settings**, donde vamos a definir el contenido que se va a iterar (los √≠tems).  

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img115.png)

1Ô∏è‚É£ **Ir a Settings**

- Selecciona la actividad **ForEach**.
    
- Abre la pesta√±a **Settings**.
    

---

2Ô∏è‚É£ **Items ‚Üí Add dynamic content**

- En el campo **Items** aparece el mensaje _This property should be parameterized_.
    
- Aqu√≠ es donde debes indicar la colecci√≥n que el ForEach recorrer√°.
    
- Haz clic en **Add dynamic content** (o presiona `Alt+Shift+D`).
    

---

3Ô∏è‚É£ **Pipeline Expression Builder (ventana derecha)**

- Se abre la ventana del **Expression Builder** para escribir expresiones din√°micas.
    
- En la lista de la derecha selecciona la salida del Lookup (resaltado en tu captura):  
    üëâ _Look for all tables ‚Üí Look for all tables activity output_.
    

---

4Ô∏è‚É£ **Ajustar la expresi√≥n**

- Al seleccionar esa opci√≥n, ADF inserta en la caja de expresi√≥n:
    
    ```json
    @activity('Look for all tables').output
    ```
    
- Sin embargo, necesitamos que ForEach recorra el **array de filas** devuelto, no todo el objeto output.
    
- Por eso, debes agregar `.value` al final:
    
    ```json
    @activity('Look for all tables').output.value
    ```
    
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img116.png)
    

‚úÖ Esto asegura que ForEach iterar√° sobre cada elemento de la colecci√≥n (cada tabla encontrada en el esquema).

- Lookup devuelve algo as√≠:
    
    ```json
    {
      "count": 3,
      "value": [
        { "SchemaName": "SalesLT", "TableName": "Customer" },
        { "SchemaName": "SalesLT", "TableName": "Product" },
        { "SchemaName": "SalesLT", "TableName": "SalesOrderHeader" }
      ]
    }
    ```
    
- `@activity('Look for all tables').output.value` ‚Üí entrega directamente el array dentro de `value`.
    
- ForEach recorrer√° ese array ‚Üí 3 iteraciones en este ejemplo.
    

---

üëâ Con esto ya tienes el ForEach listo para iterar por cada tabla.  

---

### 5Ô∏è‚É£ **Configurar la Actividad Copy Data dentro del ForEach**

Una vez que tenemos el ciclo **ForEach**, que recorre todas las tablas obtenidas mediante el **Lookup**, debemos editar su contenido para a√±adir la actividad **Copy Data**.

A continuaci√≥n, agregaremos y configuraremos la actividad **Copy Data** dentro del ciclo **ForEach**, con el objetivo de copiar cada tabla desde **SQL Server** hacia el **Data Lake**.

---

üß≠ **Paso a paso:**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img119.png)

---

#### 1. **Accede al interior del ForEach**:

* Haz clic sobre el contenedor **ForEach**, y luego en el √≠cono del **l√°piz ‚úèÔ∏è** dentro del contenedor **ForEach Schema Table**.

* Tambi√©n puedes hacer clic en la pesta√±a **Activities (0)** en la parte inferior y luego en el l√°piz de la columna **Activity**.

* Esto abrir√° el dise√±o interno del ciclo **ForEach**, donde podr√°s definir qu√© actividades se ejecutar√°n para cada tabla.

---

#### 2. **Agregar la actividad Copy Data**:

* En el panel de actividades dentro del **ForEach**, selecciona **Copy Data** y arr√°strala al √°rea de trabajo.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img120.png)

* Cambia el nombre de la actividad a **Copy Each Table**.

---

#### 3. **Configurar la Fuente (Source)**:

* En la pesta√±a **Source** de la actividad **Copy Data**, selecciona **New** para crear un nuevo **dataset** de **SQL Server**. Completa los campos con los siguientes valores:

  * [ ] **Name**: `SqlServerCopy`
  * [ ] **Linked service**: `onpremsqlserver`
  * [ ] **Table name**: d√©jalo vac√≠o

* Luego haz clic en **OK** para guardar los cambios.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img121.png)

* En el campo **Query**, haz clic en **Add dynamic content**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img122.png)

Esto abrir√° el editor de expresiones din√°micas. All√≠ ingresaremos la siguiente expresi√≥n:

```json
@{concat('SELECT * FROM ', item().SchemaName, '.', item().TableName)}
```

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img130.png)

üìå ¬øQu√© hace esta expresi√≥n?

* `item()` hace referencia al **elemento actual** del ciclo **ForEach** (una tabla).
* `item().SchemaName`: obtiene el nombre del esquema.
* `item().TableName`: obtiene el nombre de la tabla.
* `concat(...)`: concatena el texto para formar una sentencia SQL v√°lida del tipo:

```sql
SELECT * FROM schema_name.table_name
```

‚úÖ **Resultado esperado**:
Con esta configuraci√≥n, la actividad **Copy Data** ejecutar√° din√°micamente una consulta distinta para cada tabla devuelta por el **Lookup**, permiti√©ndote copiar m√∫ltiples tablas sin tener que configurarlas una por una.

Haz clic en **OK** para guardar la expresi√≥n.

---

#### 4. **Configurar el Sink (Destino)**:

* En la pesta√±a **Sink**, selecciona el **New** de destino correspondiente al **Data Lake**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img124.png)

Selecciona **Parquet**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img131.png)

Configura los campos de la siguiente forma:

* [ ] **Name**: `parquetTables`
* [ ] **Linked service**: `AzureDataLakeStorage1`
* [ ] **File path**: `Bronze`

Haz clic en **OK** para guardar los cambios.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img125.png)









---


#### 5. **Estructura de carpetas en el Data Lake**

Ahora debemos generar la siguiente **estructura de carpetas en el Data Lake** para guardar los archivos `.parquet`:

```
bronze/SchemaName/TableName/TableName.parquet
```

Ejemplo concreto:

```
bronze/SalesLT/Address/Address.parquet
```

Para lograr esto en **Azure Data Factory (ADF)** al configurar el **Sink (Destino)** de tu actividad **Copy Data**, debes usar contenido din√°mico (Dynamic Content) en la propiedad **File path** del dataset de destino (tipo Azure Data Lake Storage Gen2).

---

#### ‚úÖ **Configurar la Ruta Din√°mica en el Sink:**

Ahora haz clic **Open** en la pesta√±a **Sink** (el que apunta al Data Lake **ParquetTables**), en el campo y lo que te llevar√° a nueva pesta√±a correspondiente al Dataset **ParquetTables**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img127.png)

üß≠ Pasar los valores desde el pipeline

1. Vuelve a la actividad **Copy Each Table** en tu pipeline.
    
2. En la pesta√±a **Sink**, al usar el dataset `parquetTables`, ver√°s que se piden los par√°metros `schemaname` y `tablename`.
    
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img128.png)

Una vez creados los par√°metros `schemaname` y `tablename` dentro del dataset `parquetTables`, ahora debemos pasar sus valores din√°micamente desde la actividad **Copy Each Table**, para generar rutas din√°micas por tabla y esquema.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img129.png)


1. En la pesta√±a **Sink** de la actividad **Copy Each Table**, aseg√∫rate de tener seleccionado el dataset `parquetTables`.
    
2. En la secci√≥n **Dataset properties**, ver√°s los dos par√°metros que creaste:
    
    - `schemaname`
        
    - `tablename`
        


‚úÖ **Configurar los par√°metros del Sink (Dataset `parquetTables`)**

---

üî∂ **Paso 1: Configurar el par√°metro `schemaname`**

1. Ve a la pesta√±a **Sink** de la actividad **Copy Each Table**.
    
2. Aseg√∫rate de que el **Sink dataset** seleccionado sea `parquetTables`.
    
3. En la secci√≥n **Dataset properties**, ubica el campo llamado `schemaname`.
    
4. Haz clic en el enlace **"Add dynamic content"** debajo del campo `Value`.
    
5. En la ventana del **Pipeline Expression Builder**, escribe o selecciona la siguiente expresi√≥n:
    
    ```json
    @item().SchemaName
    ```
    
1. Haz clic en **OK** para guardar la expresi√≥n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img132.png)

‚úÖ Con esto, el par√°metro `schemaname` tomar√° el valor din√°mico de la propiedad `SchemaName` para cada tabla que se est√° procesando en el ciclo `ForEach`.

---

üî∑ **Paso 2: Configurar el par√°metro `tablename`**

1. En la misma secci√≥n de **Dataset properties**, ahora ubica el campo `tablename`.
    
2. Haz clic en **"Add dynamic content"** justo debajo del campo `Value`.
    
3. En la ventana del **Expression Builder**, escribe o selecciona la expresi√≥n:
    
    ```json
    @item().TableName
    ```
    
1. Haz clic en **OK** para guardar la expresi√≥n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img133.png)

‚úÖ Ahora, el par√°metro `tablename` tambi√©n se llenar√° din√°micamente para cada tabla en el `ForEach`.

---

üéØ Resultado

Con ambos par√°metros configurados din√°micamente, tu dataset `parquetTables` tendr√° suficiente informaci√≥n para construir rutas del estilo:

```
bronze/SalesLT/Address/Address.parquet
```

Todo de manera autom√°tica, para **cada tabla** obtenida en el `Lookup`.

üß≠ **Paso a paso para crear la ruta din√°mica en el Sink (Data Lake)**

üéØ Objetivo:

Lograr que los archivos `.parquet` se guarden con esta estructura:

```
bronze/SchemaName/TableName/TableName.parquet
```

Ejemplo:

```
bronze/SalesLT/Address/Address.parquet
```

üî∏ Paso 1: Abrir el Sink Dataset

En la actividad **Copy Each Table**:

1. Ve a la pesta√±a **Sink**.
    
2. Haz clic en el bot√≥n **Open** junto al dataset de salida llamado `parquetTables`.



![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img135.png)

üî∂ Esto abrir√° una nueva pesta√±a con la configuraci√≥n del dataset `parquetTables`.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img134.png)

üî∏ Paso 2: Configurar la Ruta del Archivo Din√°micamente

En la pesta√±a del dataset `parquetTables`:

1. Ve a la pesta√±a **Connection**.
    
2. En el campo **File path**, en la parte de **Directory**, haz clic en **Add dynamic content**.
    
3. En el editor de expresiones, escribe lo siguiente:
    

```json
@{concat(dataset().schemaname, '/', dataset().tablename)}
```

üìå ¬øQu√© hace esta expresi√≥n?

- `dataset().schemaname` ‚Üí toma el valor del par√°metro `schemaname`.
    
- `'/'` ‚Üí separa las carpetas.
    
- `dataset().tablename` ‚Üí toma el valor del par√°metro `tablename`.
    

‚úÖ Resultado:  
Esto crea un path como:

```
bronze/SalesLT/Address
```

üî∏ Paso 3: Definir el Nombre del Archivo `.parquet`

En el mismo dataset, en el campo **File name**:

1. Haz clic en **Add dynamic content**.
    
2. Ingresa la siguiente expresi√≥n:
    

```json
@{concat(dataset().tablename, '.parquet')}
```

üìå ¬øQu√© hace esta expresi√≥n?

- Toma el nombre de la tabla (`tablename`).
    
- Le concatena la extensi√≥n `.parquet`.
    

‚úÖ Resultado:  
El archivo se nombrar√° din√°micamente como:

```
Address.parquet
```
---

üì¶ Resultado final en Data Lake:

Con esta configuraci√≥n, al ejecutar la actividad **Copy Each Table**, los archivos se copiar√°n al Data Lake siguiendo esta estructura:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img136.png)

```
bronze/
‚îî‚îÄ‚îÄ SchemaName/
    ‚îî‚îÄ‚îÄ TableName/
        ‚îî‚îÄ‚îÄ TableName.parquet
```

Ejemplo:

```
bronze/SalesLT/Address/Address.parquet
```



Ahora que hemos terminado de crear el Pipeline solo debemos guardar los cambios

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img137.png)



 **Prueba del Pipelin(Opcional)**

Una vez que todo est√© configurado, puedes utilizar el modo **Debug** en Azure Data Factory para ejecutar el pipeline de manera de prueba. Esto te permitir√° verificar que cada actividad est√© configurada correctamente y que el pipeline est√© copiando las tablas de SQL Server al Data Lake de manera exitosa.

---

### 6Ô∏è‚É£ Ejecutar el pipeline con **Trigger now**

Este paso es clave para validar si tu pipeline est√° funcionando correctamente una vez que ha sido **publicado**. Ahora te explico c√≥mo hacerlo:

---

1. Aseg√∫rate de haber publicado tu pipeline

Antes de poder ejecutar tu pipeline con un trigger manual, debes hacer clic en **"Publish all"** (en la esquina superior izquierda) para guardar y publicar los cambios realizados en tu pipeline.

---

2. Haz clic en **"Add trigger"**

En la parte superior del editor, ubica el bot√≥n **Add trigger** (√≠cono de rayo) y haz clic en √©l.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img138.png)


---

4. Selecciona la opci√≥n **Trigger now**

Se desplegar√° un peque√±o men√∫. Haz clic en la opci√≥n **Trigger now**, como se muestra en la imagen.  
üî∏ Esta opci√≥n ejecutar√° el pipeline inmediatamente, usando su configuraci√≥n actual y sin necesidad de crear un trigger programado.

5. Confirma la ejecuci√≥n del pipeline

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img139.png)

üìå En la ventana **Pipeline run**, ver√°s un mensaje que dice:

**"Trigger pipeline now using last published configuration."**

Esto indica que el pipeline se ejecutar√° con la √∫ltima versi√≥n publicada y sin ning√∫n par√°metro adicional (en este caso, no hay registros en la tabla de par√°metros).

---

6. Haz clic en **OK** para continuar

‚úÖ Solo necesitas confirmar haciendo clic en el bot√≥n azul **OK** (como se resalta en la imagen).

Esto lanzar√° la ejecuci√≥n del pipeline inmediatamente y ver√°s una ventana emergente con el estado **Running** lo que confirma que el Pipeline esta en ejecuci√≥n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img140.png)

---

#### üîç 6. Monitorea la ejecuci√≥n

‚û°Ô∏è Dir√≠gete a la secci√≥n **"Monitor"** en el men√∫ izquierdo de Azure Data Factory para verificar el progreso del pipeline.

üìå Aqu√≠ podr√°s:

* Visualizar si todo se ejecut√≥ correctamente.
* Identificar si hubo errores.
* Verificar duraci√≥n, origen y estado de ejecuci√≥n.

‚úÖ Para hacerlo, navega a:
**`Monitor > Pipeline runs`**

üîÑ Aqu√≠ observar√°s el estado actual de tus ejecuciones:

* üü° **In progress**: En ejecuci√≥n
* ‚úÖ **Succeeded**: Ejecuci√≥n completada exitosamente
* ‚ùå **Failed**: Ejecuci√≥n fallida

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img141.png)

---

üìä ¬øQu√© informaci√≥n ver√°s en esta secci√≥n?

üßæ En esta vista encontrar√°s un historial de ejecuciones del pipeline. Por ejemplo:

üìù **Ejemplo de ejecuci√≥n:**

| Campo              | Valor                                             |
| ------------------ | ------------------------------------------------- |
| üè∑Ô∏è Pipeline       | `copy_all_tables`                                 |
| üïí Hora de inicio  | 03/09/2025 - 11:52:31 AM                          |
| üîÑ Estado          | `In progress` (en progreso al momento de captura) |
| ‚è±Ô∏è Duraci√≥n actual | 24 segundos                                       |
| üîò Trigger type    | `Manual trigger`                                  |
| üÜî Run ID          | Identificador √∫nico para esta ejecuci√≥n           |

‚úîÔ∏è Esta vista permite:

* Dar seguimiento al estado del pipeline en tiempo real.
* Validar la duraci√≥n de ejecuci√≥n.
* Identificar el origen del trigger.
* Obtener detalles √∫tiles para monitoreo y depuraci√≥n (**debugging**).

---

#### ‚úÖ Actualizaci√≥n: Pipeline completado

üì• Luego de actualizar la vista, se puede verificar que el pipeline `copy_all_tables` **ya finaliz√≥ su ejecuci√≥n**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img142.png)

üìå Detalles de la ejecuci√≥n final:

* ‚ñ∂Ô∏è Ejecutado manualmente el **3 de septiembre de 2025 a las 11:52:31 AM**
* ‚èπÔ∏è Finalizado a las **11:53:24 AM**
* ‚è±Ô∏è Duraci√≥n total: **53 segundos**
* ‚úÖ Estado: **"Succeeded"** (completado sin errores)
* üß© Triggered by: **Manual trigger**
* üÜî Run ID: identificador √∫nico generado autom√°ticamente

---

üìù **NOTAS:**

* Esta secci√≥n es crucial para el **monitoreo post-ejecuci√≥n**.
* Permite detectar errores de forma temprana o confirmar que el proceso corri√≥ exitosamente.
* Ideal para entornos de **despliegue**, **QA**, y **producci√≥n** donde la trazabilidad y visibilidad son clave.

---

üí° **TIP**: Puedes hacer clic sobre cualquier ejecuci√≥n para ver un desglose completo de las actividades dentro del pipeline, con tiempos individuales y logs detallados.



### üìÇ **Data Lake Validaci√≥n**  
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img143.png)

‚úÖ Una vez que el pipeline finaliza correctamente, pasamos a validar que los datos hayan llegado al destino correcto: nuestro Data Lake.

üìç Ingresamos al recurso de almacenamiento `z2hdatalakesuffix`  
üìç Navegamos al **Storage Browser**  
üìç Seleccionamos el contenedor **bronze**  
üìç Entramos a la carpeta **SalesLT** (donde configuramos que se guarden las tablas)

‚úîÔ∏è All√≠ encontramos las siguientes carpetas, que representan las tablas copiadas:

- üìÅ Address
    
- üìÅ Customer
    
- üìÅ CustomerAddress
    
- üìÅ Product
    
- üìÅ ProductCategory
    
- üìÅ ProductDescription
    
- üìÅ ProductModel
    
- üìÅ ProductModelProductDescription
    
- üìÅ SalesOrderDetail
    
- üìÅ SalesOrderHeader
    

üìùNOTA: La hora de modificaci√≥n de todas estas carpetas coincide con la ejecuci√≥n del pipeline, lo que confirma que la operaci√≥n se realiz√≥ de forma exitosa.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üìÑ **VALIDACI√ìN FINAL: CONTENIDO DE LAS CARPETAS**  
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img144.png)

üîç Finalmente, ingresamos dentro de una de las carpetas generadas, en este caso:  
üìÅ **Address**

‚úîÔ∏è Dentro de ella encontramos el archivo:  
üìÑ `Address.parquet`  
üìÖ Fecha: 9/3/2025 - 11:53:13 AM  
üî• Access tier: Hot  
üì¶ Tipo: Block blob  
üìè Tama√±o: 35.08 KiB

üìùNOTA: Este archivo en formato `.parquet` contiene los datos de la tabla `Address` extra√≠dos desde la base de datos SQL y almacenados de forma √≥ptima para an√°lisis o transformaci√≥n posterior.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ  
üí° **¬øQu√© logramos hasta ahora?**  
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úîÔ∏è Automatizamos la extracci√≥n y carga de todas las tablas del esquema `SalesLT` desde SQL a Data Lake.  
‚úîÔ∏è Utilizamos un pipeline parametrizado que recorre las tablas din√°micamente.  
‚úîÔ∏è Validamos la ejecuci√≥n en tiempo real y monitoreamos el proceso desde Azure Data Factory.  
‚úîÔ∏è Confirmamos que los datos ya se encuentran organizados en nuestro contenedor `bronze`.

---

### **Flujo del Pipeline:**

- **Lookup** obtiene las tablas.
    
- **ForEach** (u otra actividad de iteraci√≥n) se usa para procesar cada tabla.
    
- **Copy Activity** se utiliza para copiar los datos de cada tabla al Data Lake.
    

---

An√°lisis:

1. **Lookup**  
    ‚úîÔ∏è Correcto, esta actividad se usa para obtener la lista de tablas desde la fuente.
    
2. **ForEach**  
    ‚úîÔ∏è Correcto, es la actividad que permite iterar sobre cada tabla obtenida por el Lookup.
    
3. **Copy Activity**  
    ‚úîÔ∏è Correcto, es la actividad que efectivamente realiza la copia de datos desde la tabla fuente al Data Lake.
    

---
### üöÄ **¬øEst√° completo el flujo?** üöÄ

El flujo b√°sico para copiar m√∫ltiples tablas de una fuente a un Data Lake est√° correctamente planteado ‚úÖ.

---

üîç **Pero, seg√∫n buenas pr√°cticas y escenarios reales, se podr√≠an agregar m√°s elementos para mayor robustez:**

‚û°Ô∏è **Control de errores (Error handling):**  
‚ùì ¬øQu√© pasa si falla alguna tabla?  
‚û°Ô∏è Agrega actividades para manejo de errores o l√≥gica condicional para continuar o alertar.

‚û°Ô∏è **Validaciones o transformaciones:**  
üõ†Ô∏è En algunos pipelines se incluye una actividad previa o posterior para validar o transformar datos.

‚û°Ô∏è **Par√°metros din√°micos:**  
üéõÔ∏è Para hacer el pipeline m√°s reusable y din√°mico.

‚û°Ô∏è **Triggers:**  
‚è∞ Para automatizar la ejecuci√≥n (aunque aqu√≠ fue manual, puede estar presente).

---

üìã **Resumen en tabla:**

|Elemento|¬øIncluido?|Comentarios|
|---|---|---|
|Lookup|‚úÖ|Obtiene las tablas desde la fuente|
|ForEach|‚úÖ|Itera sobre cada tabla|
|Copy Activity|‚úÖ|Copia los datos al Data Lake|
|Control de errores|‚ùå|Recomendado para robustez|
|Validaciones/Transform|‚ùå|Opcional, pero √∫til para limpieza o ajuste|
|Par√°metros din√°micos|‚ùå|Facilita reutilizaci√≥n|
|Triggers|‚ùå|Automatiza ejecuci√≥n|

---

üìù **NOTAS:**

- Este flujo es **b√°sico y funcional**, ideal para principiantes en Ingenier√≠a de Datos en la nube.
    
- M√°s adelante, replicaremos este proyecto a nivel de c√≥digo Python para que aprendas desde cero y con un stack open source c√≥mo replicar todo esto 100% en Python y SQL.
	
- Las herramientas No-Code como Azure Data Factory abstraen el c√≥digo, pero para entender a fondo y controlar mejor los pipelines, es fundamental aprender a programar los flujos.
	
- Conocer Python para construir pipelines te da flexibilidad y poder para optimizar y personalizar tu soluci√≥n.

---

‚úÖ **Conclusi√≥n:**  
Este pipeline b√°sico funciona y es el primer paso para entender el proceso. Pero para flujos m√°s robustos y reales, implementar manejo de errores, validaciones, parametrizaci√≥n y automatizaci√≥n es clave.


