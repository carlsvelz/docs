
# RunTime

## SQL Server Linux

Validemos nuestro actual escenario:  
Tienes **SQL Server en Linux** (dentro de un contenedor Docker) en una **VM local**, y quieres conectarlo a **Azure Data Factory (ADF)**.

La idea general es que **ADF no se conecta directamente a entornos on-premises** o privados, sino que necesita un puente seguro: **Self-hosted Integration Runtime (SHIR)**.

---

### 1ï¸âƒ£ El problema: SHIR no corre en Linux

El **Self-hosted Integration Runtime** solo estÃ¡ disponible para:

- Windows (nativo)
    
- Windows Server (nativo)
    

No existe versiÃ³n oficial de SHIR para Linux o contenedores Linux.  
Por eso, si quieres conectar ADF a tu SQL Server local (aunque sea Linux), **el runtime debe instalarse en algÃºn host Windows** que pueda ver tu contenedor en la red.

---

### 2ï¸âƒ£ Opciones para tu caso

#### **OpciÃ³n 1 â€“ Instalar SHIR en una mÃ¡quina Windows que tenga acceso a la VM Linux**

- Instalas el SHIR en un servidor o PC Windows en tu red.
    
- Configuras el SHIR en ADF.
    
- Aseguras que desde esa mÃ¡quina Windows se pueda hacer ping/conectar al SQL Server del contenedor (`ip_vm_linux:1433`).
    
- Ventajas:
    
    - FÃ¡cil de implementar.
        
    - 100% soportado por Microsoft.
        
- Desventajas:
    
    - Necesitas tener un equipo Windows encendido.
        

---

#### **OpciÃ³n 2 â€“ Usar una VM Windows como â€œpuenteâ€ en la misma red que la VM Linux**

- Creas una VM Windows (en el mismo host fÃ­sico o en Azure con VPN a tu red).
    
- Instalas el SHIR ahÃ­.
    
- La VM Windows se conecta al contenedor SQL Server en la VM Linux.
    
- Ideal si tu entorno es todo virtualizado (Hyper-V, VMware, Proxmox, etc.).
    

---

#### **OpciÃ³n 3 â€“ SHIR en Windows dentro de un contenedor Docker**

- No hay imagen oficial, pero podrÃ­as usar un contenedor **Windows** con SHIR instalado.
    
- Esto requiere que tu host soporte **Windows Containers** (solo en Windows Server o Windows 10/11 con Hyper-V).
    
- En Linux puro, **no es viable** porque Windows Containers no se ejecutan nativamente en Docker Linux.
    

---

#### **OpciÃ³n 4 â€“ Evitar SHIR usando acceso pÃºblico seguro**

- Expones SQL Server al pÃºblico (muy poco recomendable) o a travÃ©s de un tÃºnel seguro:
    
    - **VPN site-to-site** desde Azure a tu red local.
        
    - **Azure ExpressRoute** (opciÃ³n empresarial).
        
    - **TÃºnel SSH inverso** a una VM en Azure que tenga IP pÃºblica.
        
- Esto permitirÃ­a a ADF conectarse como si fuera un SQL Server en Azure, sin SHIR.
    
- Ventajas:
    
    - No necesitas un host Windows.
        
- Desventajas:
    
    - ConfiguraciÃ³n mÃ¡s compleja.
        
    - Riesgos de seguridad si no se protege bien.
        

---
ğŸ’¡ **RecomendaciÃ³n prÃ¡ctica:**

En este caso usaremos la **opciÃ³n 1**, ya que es la mÃ¡s fÃ¡cil de implementar. Para este caso de uso, **necesitas tener Windows como tu sistema operativo principal**, ya que el SHIR (Self-hosted Integration Runtime) solo se puede instalar en entornos Windows.

Si tienes restricciones para usar Windows directamente, la otra opciÃ³n serÃ­a **montar el SHIR en una mÃ¡quina virtual (VM) con Windows** que tenga acceso de red a tu contenedor de SQL Server. Es el camino mÃ¡s sencillo, soportado y estable.

De esta forma, puedes mantener tu entorno principal en Linux o en la nube, mientras aprovechas las capacidades del SHIR de forma compatible y sin complicaciones de red o compatibilidad.


## **InstalaciÃ³n RUNTIME**


Perfecto ğŸš€, ya tienes la VM Linux con SQL Server en Docker y el reenvÃ­o de puertos hacia tu host Windows (1422 â†’ 1433 del contenedor).  
Ahora solo falta instalar y configurar el **Self-hosted Integration Runtime (SHIR)** en Windows.

---

### ğŸ”¹ 1. Descargar e instalar el SHIR en Windows

1. Inicia sesiÃ³n en el **portal de Azure**.
    
2. Ve a tu **Azure Data Factory (ADF)**.
    
3. En el menÃº lateral, selecciona **Administrar** (icono de engranaje).
    
4. Dentro de **Integration runtimes**, haz clic en **Nuevo**.
    
5. Elige **Azure Self-Hosted** y selecciona **Continuar**.
	
6. Elige **Self-Hosted** y selecciona **Continuar**.
	
7. Asignaremos el nombre por default y creamos la conexiÃ³n
    
8. Validamos los datos del nuevo Runtime 

**PASO 4:** Dentro de **Integration runtimes**, haz clic en **Nuevo**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img42.png)

Como verÃ¡s Integration Runtimes ya viene con un Runtime por defecto llamado: 

**Integration Runtimes en Azure Data Factory (ADF)**

---

ğŸ”§ Â¿QuÃ© es un Integration Runtime (IR)?

â¡ï¸ Es el **motor de ejecuciÃ³n** que permite a Azure Data Factory mover y transformar datos desde y hacia diferentes orÃ­genes y destinos.

---

ğŸ“Œ TIPOS DE INTEGRATION RUNTIME:

ğŸ“Š Tabla resumen:

|Tipo de IR|Â¿QuÃ© hace?|Â¿DÃ³nde se usa?|
|---|---|---|
|AutoResolveIntegrationRuntime|Es el runtime administrado por Azure. Decide la regiÃ³n automÃ¡ticamente.|Para conexiones en la nube pÃºblica.|
|Self-hosted IR (SHIR)|Se instala en tu red local para acceder a datos en entornos on-premise.|Acceso a sistemas internos/privados.|

---

ğŸ§  Â¿QuÃ© es **AutoResolveIntegrationRuntime**?

âœ”ï¸ Es el **runtime por defecto** que viene ya preconfigurado en cada instancia de Azure Data Factory.

âœ”ï¸ No requiere instalaciÃ³n ni configuraciÃ³n manual.

âœ”ï¸ ADF decide automÃ¡ticamente en quÃ© regiÃ³n de Azure ejecutarlo.

âœ”ï¸ Se utiliza principalmente cuando los **orÃ­genes y destinos estÃ¡n accesibles pÃºblicamente** (por ejemplo: Blob Storage, SQL Azure, etc.)

â¡ï¸ Por defecto, ADF intentarÃ¡ usar la **misma regiÃ³n** donde estÃ¡ creada la instancia de Data Factory.  
â¡ï¸ Si no es posible, usarÃ¡ la regiÃ³n mÃ¡s cercana disponible.

---

ğŸ“ NOTA:  
Si solo estÃ¡s trabajando con datos que ya estÃ¡n en Azure (como bases de datos en la nube, almacenamiento en la nube, etc.), **NO necesitas instalar nada adicional**. El AutoResolveIntegrationRuntime hace todo el trabajo por ti.

---

ğŸ’¡ EJEMPLO DE USO:

Imagina que tienes una actividad de copia de datos desde una base de datos SQL en Azure hacia un Blob Storage tambiÃ©n en Azure.

âœ”ï¸ No necesitas configurar un IR manual.  
âœ”ï¸ ADF usarÃ¡ automÃ¡ticamente el **AutoResolveIntegrationRuntime**.  
âœ”ï¸ Se ejecutarÃ¡ en la misma regiÃ³n (si es posible).

---

ğŸ§± DIFERENCIA CLAVE CON SHIR:

ğŸ“‹ Lista comparativa:

- ğŸŒ **AutoResolveIntegrationRuntime**
    
    - âœ… Totalmente administrado por Azure
        
    - ğŸ”’ No accede a redes privadas
        
    - ğŸš€ Ideal para conexiones 100% en la nube
        
- ğŸ–¥ï¸ **Self-hosted Integration Runtime (SHIR)**
    
    - ğŸ§° Necesita instalaciÃ³n local
        
    - ğŸŒ Puede acceder a bases de datos y sistemas en red privada
        
    - ğŸ” Usa una puerta de enlace para conexiÃ³n segura
        

---

âœ”ï¸ CONCLUSIÃ“N:

El **AutoResolveIntegrationRuntime** es perfecto si trabajas solo con servicios pÃºblicos en la nube.  
Si necesitas conectar con recursos locales (como servidores SQL en tu empresa), entonces debes usar el **SHIR**.

---

â—RECUERDA:

- No confundas AutoResolve con IR personalizados: solo AutoResolve estÃ¡ preconfigurado por defecto.
    
- No necesitas pagar extra por usar el runtime automÃ¡tico, viene incluido con ADF.


#### **PASO 5:** Elige **Azure, Self-Hosted** y selecciona **Continuar**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img43.png)

---

#### **PASO 6:** Elige **Self-Hosted** y selecciona **Continuar**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img44.png)

---

#### **PASO 7:** Asignaremos el nombre por default y creamos la conexiÃ³n 

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img45.png)


---

#### **PASO 8:** Validamos los datos del nuevo Runtime y descargamos el instalador (archivo `.msi`) que te darÃ¡ Azure.


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img46.png)

---

#### **Keys de autenticaciÃ³n en Self-hosted Integration Runtime (SHIR)**

ğŸ”‘ Â¿QuÃ© son las Key1 y Key2 en SHIR?

â¡ï¸ Son **claves de autenticaciÃ³n** generadas automÃ¡ticamente por Azure Data Factory cuando creas un **Self-hosted Integration Runtime**.

â¡ï¸ Se utilizan para **registrar e identificar** una instalaciÃ³n local del runtime con el servicio de Azure Data Factory en la nube.

---

ğŸ“¦ Â¿DÃ³nde se usan estas claves?

ğŸ–¥ï¸ Cuando instalas el **Self-hosted IR** en una mÃ¡quina (normalmente con Windows), el instalador te pedirÃ¡ una de estas claves.

âœ… Al pegar la clave en el instalador:

- Se establece una conexiÃ³n segura entre tu mÃ¡quina local y Azure Data Factory.
    
- El runtime queda **vinculado** con el IR que creaste en el portal de ADF.
    

---

ğŸ›¡ï¸ Â¿Por quÃ© existen dos claves?

ğŸ“‹ Tabla de uso de claves:

|Clave|Uso Principal|Â¿Se puede regenerar?|Â¿Interrumpe el servicio?|
|---|---|---|---|
|Key1|Se usa como clave activa o principal|âœ… SÃ­|âŒ No|
|Key2|Clave secundaria o de respaldo para rotaciÃ³n segura|âœ… SÃ­|âŒ No|

ğŸ“ NOTA:  
Tener dos claves permite que puedas **rotar las claves por seguridad** sin tener que detener el servicio ni perder la conexiÃ³n entre tu mÃ¡quina y ADF.

---

ğŸ”„ ROTACIÃ“N DE CLAVES: Â¿CÃ³mo funciona?

1. ğŸ” SupÃ³n que estÃ¡s usando **Key1** activamente.
    
2. ğŸ’¡ Por razones de seguridad, decides regenerar la clave.
    
3. âœ… Vas al portal de Azure y regeneras **Key2**.
    
4. ğŸ› ï¸ Luego, en tu instalaciÃ³n local, cambias la clave usada por la nueva **Key2**.
    
5. ğŸ§¹ Ahora puedes **regenerar Key1** sin afectar la conexiÃ³n ni la ejecuciÃ³n de actividades.
    

âœ”ï¸ Este mecanismo permite una **rotaciÃ³n segura**, similar a cÃ³mo funcionan las claves de conexiÃ³n en muchos servicios de Azure.

---

ğŸ“ NOTA IMPORTANTE:

- Ambas claves son igual de vÃ¡lidas.
    
- Puedes usar cualquiera de las dos para registrar una nueva mÃ¡quina con el mismo SHIR.
    
- Si pierdes ambas claves, tendrÃ¡s que generar nuevas e **instalar o actualizar manualmente** los SHIR que dependan de ellas.
    

---

âš ï¸ SEGURIDAD:

ğŸ”’ Estas claves son **sensibles** y deben mantenerse privadas.  
ğŸ“¤ No las compartas ni las subas a repositorios de cÃ³digo.  
ğŸ” Rota las claves periÃ³dicamente para mantener la seguridad de tu entorno.

---

ğŸ’¬ RESUMEN VISUAL:

âœ”ï¸ 2 claves generadas automÃ¡ticamente  
âœ”ï¸ Se usan en la instalaciÃ³n del SHIR  
âœ”ï¸ Vinculan el runtime local con ADF  
âœ”ï¸ Permiten rotaciÃ³n sin interrupciÃ³n  
âœ”ï¸ Claves sensibles: Â¡protecciÃ³n mÃ¡xima!

---

### ğŸ”¹ 2. Instalar SHIR con tu Data Factory

1. InstalaciÃ³n y configuraciÃ³n **Microsoft Integration Runtime Configuration Manager**.
	
2.  Una vez instalado, abre la aplicaciÃ³n **Microsoft Integration Runtime Configuration Manager**.

#### **PASO 1:** InstalaciÃ³n y configuraciÃ³n **Microsoft Integration Runtime Configuration Manager**.

Hay dos formas de instalar el Self-hosted Integration Runtime (SHIR) en Windows:

Option 1: Express setup ğŸ‘‰ es la forma mÃ¡s rÃ¡pida y automÃ¡tica. Descarga e instala directamente el runtime en el mismo equipo desde el portal de Azure, sin necesidad de copiar manualmente claves de autenticaciÃ³n. Ideal si ya estÃ¡s en la mÃ¡quina donde quieres tener el SHIR.

Option 2: Manual setup ğŸ‘‰ es un proceso mÃ¡s flexible. Te permite descargar el instalador (.msi), ejecutarlo en cualquier servidor/PC Windows y, durante la instalaciÃ³n, registrar el runtime usando las keys de autenticaciÃ³n que te da Azure. Es Ãºtil cuando instalas en otro equipo distinto o cuando prefieres mÃ¡s control en el proceso.

âœ”ï¸Express setup es rÃ¡pido y directo para la mÃ¡quina actual, mientras que el Manual setup es la opciÃ³n recomendada si lo instalarÃ¡s en un servidor dedicado o en una mÃ¡quina distinta a donde estÃ¡s navegando el portal.

ğŸ‘‰En nuestro caso usaremos la **OpciÃ³n Express Setup** solo debes hacer clic sobre esta opciÃ³n descarga el instalador solo debes proceder con la instalaciÃ³n automÃ¡tica y express

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img47.png)

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img48.png)

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img49.png)

En la ventana verÃ¡s la confirmaciÃ³n de que terminaste con Ã©xito la instalaciÃ³n mediante el **Express Setup** del **Self-hosted Integration Runtime (SHIR)** ğŸš€.

El asistente hizo automÃ¡ticamente estos pasos:

1. **Loading configuration** â†’ cargÃ³ la configuraciÃ³n de tu runtime.
    
2. **Downloading Integration Runtime (Self-hosted)** â†’ descargÃ³ los binarios necesarios.
    
3. **Installing Integration Runtime (Self-hosted)** â†’ instalÃ³ el servicio en tu mÃ¡quina Windows.
    
4. **Registering Integration Runtime (Self-hosted)** â†’ registrÃ³ tu runtime con Azure Data Factory, por lo que ya aparece vinculado en el portal.
    

ğŸ‘‰ Ahora, tu SHIR ya estÃ¡ instalado, registrado y listo para usarse.  
El siguiente paso es **crear un Linked Service en ADF** que use este runtime y apunte a tu SQL Server (ejemplo: `localhost,1422`) para probar la conexiÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img50.png)

---


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img51.png)

Â¡Perfecto! Ahora solo debes ejecutar la aplicaciÃ³n reciÃ©n instalada y podrÃ¡s ver la pantalla **Microsoft Integration Runtime Configuration Manager**, y confirma que tu **Self-hosted Integration Runtime (SHIR)** ya estÃ¡ instalado y **conectado correctamente al servicio de Azure Data Factory**.

Los puntos clave de lo que ves:

- âœ… **Self-hosted node is connected to the cloud service** â†’ el runtime estÃ¡ enlazado a tu Data Factory (`z2h-adf-suffix`).
    
- ğŸ”— **Integration Runtime: integrationRuntime1** â†’ este es el nombre del runtime que creaste en el portal.
    
- ğŸ’» **Node: Z2H** â†’ es el nombre de tu mÃ¡quina Windows que estÃ¡ actuando como host del SHIR.
    
- ğŸ” **Data Source Credential** â†’ indica que las credenciales que uses para conectarte a tus orÃ­genes de datos (ej. SQL Server en Linux) se almacenan localmente en esta mÃ¡quina y estÃ¡n sincronizadas con el runtime.
    
- ğŸ“¦ Opciones de **backup/import** â†’ sirven para exportar e importar las credenciales si necesitas migrar o restaurar el runtime en otra mÃ¡quina.
    

ğŸ‘‰ Tu SHIR ya estÃ¡ activo y vinculado.  
El siguiente paso es ir al portal de Azure Data Factory y crear un **Linked Service** que use este runtime para conectarse a tu SQL Server en Linux (`localhost,1422`).


### ğŸ”¹ 3. Configurar la conexiÃ³n al SQL Server (puerto 1433)

1. Desde ADF, crea una actividad **Copy Data**
    
2. En **Servidor**, pon la IP o nombre de tu host Windows (ejemplo: `127.0.0.1,1433`.
    
3. Usuario y contraseÃ±a: las credenciales que definiste al levantar SQL Server en Docker.
    
4. Selecciona el runtime reciÃ©n creado (SHIR).
    
5. Haz clic en **Probar conexiÃ³n**.


#### **Crea una actividad Copy Data**

**Â¿QuÃ© es la actividad Copy Data en Azure Data Factory?**

---

ğŸšš **Copy Data** es una de las actividades mÃ¡s importantes y comunes en ADF.  
Sirve para **mover datos desde un origen hasta un destino (sink)**, ya sea en la nube o local (on-premises).

---

ğŸ” Â¿QuÃ© hace exactamente?

â¡ï¸ Conecta con un origen de datos (ej: SQL Server, Blob Storage, API, etc.)  
â¡ï¸ Lee los datos (usando consulta o tabla completa)  
â¡ï¸ Mapea y transforma columnas (si es necesario)  
â¡ï¸ Escribe los datos en un destino (sink)  
â¡ï¸ Usa un Integration Runtime (AutoResolve o SHIR) para ejecutar la actividad

---

ğŸ“¦ **Soporta mÃºltiples escenarios:**

ğŸ“‹ Lista de capacidades clave:

- ğŸŒ OrÃ­genes y destinos en la nube o en red privada
    
- âš™ï¸ Control de formato (JSON, CSV, Parquet, Avro, etc.)
    
- ğŸ—ºï¸ Mapeo de columnas personalizado
    
- ğŸš€ Paralelismo y particionado (para rendimiento)
    
- ğŸ” Cargas completas o incrementales
    
- ğŸ§¯ Opciones de tolerancia a errores (fault tolerance)
    

---

ğŸ› ï¸ Â¿CuÃ¡ndo usarla?

âœ”ï¸ Ideal para:

- Migraciones simples
    
- Integraciones periÃ³dicas
    
- ETL ligeros (solo movimiento de datos)
    
- Cargas rÃ¡pidas sin transformaciÃ³n compleja
    

âŒ NO es ideal para:

- Procesos complejos de transformaciÃ³n de datos (usa Data Flows o Synapse en ese caso)
    

---

ğŸ“ NOTA:  
Toda actividad **Copy Data** requiere un Integration Runtime:

- ğŸŒ **AutoResolveIntegrationRuntime** si todo estÃ¡ en la nube
    
- ğŸ  **Self-hosted IR (SHIR)** si el origen o destino estÃ¡ en red privada/local
    

---

ğŸ§ª ESCENARIO: **Tu entorno SQL en contenedor Linux (puerto 1422)**

Pasos detallados para ejecutar una Copy Data desde ese SQL:

ğŸ”¢ **0) PRERREQUISITOS (verificados):**

âœ”ï¸ SHIR instalado y conectado  
âœ”ï¸ ReenvÃ­o de puertos configurado:  
â¡ï¸ Windows:1422 â Linux Container:1433  

---

ğŸ“‹ **1) Crear pipeline con actividad Copy Data**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img52.png)

â¡ï¸ Ve a la secciÃ³n **Author > Pipelines > New Pipeline**  
â¡ï¸ Arrastra `drag and drop` la actividad **Copy Data**  al panel


ğŸ“‹ **2) PestaÃ±a General**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img53.png)

Ahora en la  **pestaÃ±a General** de la actividad _Copy data_ en tu pipeline define el **primer paso de configuraciÃ³n de la actividad**, que es bÃ¡sicamente darle identidad y control a la ejecuciÃ³n:

- **Name** â†’ Asigna el nombre _Copy address table_ (serÃ¡ la etiqueta con la que identifiques esta actividad dentro del pipeline).
    
- **Description** â†’ opcional, puedes documentar quÃ© hace esta copia (ej. â€œCopia la tabla Address de SQL Server hacia Blob Storageâ€).
    
- **Activity state** â†’ lo dejaremos en **Activated**, lo que significa que la actividad se ejecutarÃ¡ cuando corra el pipeline. Si la pusieras en **Deactivated**, quedarÃ­a como parte del pipeline pero se saltarÃ­a en la ejecuciÃ³n.
    
- **Timeout** â†’ configurado en `0.12:00:00`, que significa 12 horas mÃ¡ximo antes de que la actividad falle por tiempo excedido.
    
- **Retry** y **Retry interval (sec)** â†’ 0 reintentos, con intervalo de 30 segundos (si pusieras por ejemplo 3, intentarÃ­a 3 veces mÃ¡s en caso de error, esperando 30 segundos entre intentos).
    
- **Secure output** â†’ desmarcado; si lo activas, la salida de esta actividad no se mostrarÃ­a en logs/monitor para proteger datos sensibles.
    

ğŸ‘‰ Este es solo el arranque: aquÃ­ defines cÃ³mo se llama la actividad y cÃ³mo se comporta ante errores o tiempos de espera.  
El siguiente paso serÃ¡ ir a la pestaÃ±a **Source** para configurar desde dÃ³nde vas a copiar los datos (tu SQL Server en Linux a travÃ©s del SHIR).


ğŸ“‹ **3) PestaÃ±a Source**

Ahora vamos a la **pestaÃ±a Source** de la actividad _Copy data_, donde definimos **de dÃ³nde vienen los datos**.  Los pasos para configurarlo con tu SQL Server en Linux (vÃ­a SHIR) son:

---
1. En **Source dataset**, haz clic en **+ New**.
    
2. Se abre la ventana de selecciÃ³n de tipo de dataset â†’ busca **SQL Server**
    
3. Haz clic en **Continue**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img54.png)
    
4. Ahora configura tu dataset:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img56.png)

---

Muy bien ğŸ™Œ, ahora en la ventana **Set properties** al crear el **dataset de origen (Source dataset)**.

AquÃ­ defines cÃ³mo ADF se va a conectar a tu SQL Server. Los pasos son:

---

ğŸ”¹ ConfiguraciÃ³n del Source dataset

1. **Name**
    
    - Escribe un nombre descriptivo, en este caso **address**, que corresponde a la tabla que vas a copiar.
        
    - Este nombre es el identificador del dataset dentro de ADF (no necesariamente el mismo nombre de la tabla, pero conviene mantenerlo claro).
        
2. **Linked service**
    
    - Ahora selecciona el **Linked Service** que conecta ADF con tu SQL Server on-prem.
        
    - Haz clic en **+ New** y:
        
        - Elige tipo **SQL Server**.

3. **Ahora completa los siguientes campos**

- Nombre: `onpremsqlserver`
    
- (Opcional) DescripciÃ³n: `ConexiÃ³n SQL local desde contenedor`
    
- Integration Runtime: `integrationRuntime1` (tu SHIR)
    
- VersiÃ³n: `2.0 (Recommended)`
    
- Server name: `localhost`
    
- Database name: `AdventureWorksLT2022`
    
- Authentication type: `SQL authentication`
    
- User name: `sa`

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img57.png)


4. ğŸ” **Configurar Azure Key Vault en ADF**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img58.png)

- Haz clic en **"Azure Key Vault"** (en vez de escribir la contraseÃ±a manual).
    
- En **AKV linked service**, selecciona o crea un nuevo linked service hacia tu Key Vault.
 
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img59.png)

- **Selecciona** `Azure Key Vault` como tipo de Linked Service.
    
- **Asigna** el nombre, por ejemplo: `AzureKeyVault1`.
    
- **Elige** el mÃ©todo **"From Azure subscription"**.
    
- **Selecciona** la **suscripciÃ³n de Azure** correspondiente.
    
- **Escoge** el **Key Vault**, por ejemplo: `key-vault-suffix`.
    
- **Usa** la opciÃ³n de **autenticaciÃ³n con Managed Identity**.
    
- **Prueba** la conexiÃ³n para verificar que funcione correctamente.
    
- **Haz clic** en **Create** para guardar el Linked Service.

---

Ahora verÃ¡s este error **Loading Fail**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img60.png)

Si haces clic en **More** verÃ¡s los detalles del error

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img61.png)

---

ğŸ”§Â¿QuÃ© significa este error?

Este error indica que Azure Data Factory **no pudo cargar o recuperar los secretos almacenados en el Azure Key Vault** especificado. Por eso no aparecen las opciones disponibles en el desplegable de **Secret name**.

â–¶ï¸Posibles causas comunes:

1. **Permisos insuficientes**
    
    - La identidad utilizada por Azure Data Factory (Managed Identity o Service Principal) **no tiene permisos de acceso (get/list) configurados en el Key Vault**.
        
2. **Nombre del Azure Key Vault incorrecto**
    
    - El Linked Service de Azure Key Vault seleccionado (`AzureKeyVault1`) puede estar mal configurado o apuntar a un Key Vault que no existe o no es accesible.
        
3. **Problemas de red o configuraciÃ³n**
    
    - Restricciones de red o polÃ­ticas de firewall que impiden a ADF acceder al Key Vault.
        
4. **Error temporal o conexiÃ³n interrumpida**
    
    - Puede ser un problema temporal en la conexiÃ³n o servicio.

ğŸ§°Â¿CÃ³mo resolver el problema?

Debes asignar los permisos necesarios para el usuario o la identidad gestionada de Azure Data Factory a travÃ©s de **IAM (Identity and Access Management)** en lugar de usar Access policies:

1. Ve a la opciÃ³n **Access control (IAM)** del Key Vault.
    
2. Haz clic en **Add role assignment**.

Â¿QuÃ© significa esto?

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img62.png)

- El usuario tiene permisos administrativos completos sobre el Key Vault.
    
- Sin embargo, para que Azure Data Factory (o cualquier otro servicio) pueda acceder a los secretos almacenados en el Key Vault, **la identidad gestionada** del servicio tambiÃ©n debe tener los permisos adecuados asignados aquÃ­.
    
- El error "Forbidden" indica que la identidad que intenta acceder a los secretos no tiene los permisos necesarios.

Â¿QuÃ© revisar o hacer?

Verifica cuÃ¡l es la **identidad gestionada** o el principal de servicio que usa Azure Data Factory para acceder al Key Vault.

- Entra al portal de Azure.
    
- Navega a tu **Azure Data Factory**.
    
- En el menÃº de la izquierda, busca y selecciona **Managed identities** o **Identidad gestionada**.
    
- AquÃ­ verÃ¡s si tienes una **identidad gestionada habilitada** (normalmente un principal de servicio que Azure crea para tu Data Factory).
    
- Copia el nombre o el ID de esta identidad.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img63.png)

Â¡Perfecto! Tu instancia de **Azure Data Factory** (`z2h-adf-suffix`) **tiene habilitada su identidad gestionada del sistema** (`System assigned`), y su **Object (principal) ID** es:

```
d17a4c4c-236a-474a-a542-c5803b115a03
```

âœ… Â¿QuÃ© significa esto?

Que Azure Data Factory ya tiene una identidad propia que puedes usar para asignarle permisos a otros recursos, como el **Azure Key Vault**. Y esto es justo lo que debes hacer para resolver el error de acceso a secretos que vimos antes.

---

âœ… Â¿QuÃ© sigue? (resumen de acciones)

Ahora, ve al **Key Vault** y haz lo siguiente:

1. Entra al **Key Vault**.
    
2. Ve a la opciÃ³n **Access control (IAM)**.
    
3. Haz clic en **+ Add > Add role assignment**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img64.png)
    
4. Configura:
    
    - **Rol**: `Key Vault Secrets User` (o cualquier rol que permita leer secretos).
        
    - **Asignado a**: Busca por el ID o nombre del principal `Azure Data Factory` (puedes usar el nombre del recurso o pegar ese ID) luego haz clic en Next

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img65.png)

EstÃ¡s a punto de completar la **asignaciÃ³n de rol** para que tu Azure Data Factory (`z2h-adf-suffix`) pueda acceder a los secretos del **Key Vault**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img66.png)

ğŸ” Observaciones importantes:

- Selecciona correctamente el tipo: `Managed identity`.
	
- Selecciona Data Factory (V2) 
    
- En **search by name** Elige como miembro: `z2h-adf-suffix` (tu instancia de Data Factory) y haz clic en el objeto luego clic en **Select**
	
- Luego continÃºa con:
    
    - ğŸ”¹ `Next > Review + assign`
        
    - ğŸ”¹ `Review + assign` nuevamente para confirmar
        

---

ğŸ¯ Resultado esperado

Una vez que se asigne correctamente el rol, la **identidad gestionada de Azure Data Factory** tendrÃ¡ permiso para **acceder a los secretos** del Key Vault, y el error de â€œForbiddenâ€ deberÃ­a desaparecer. Ahora podrÃ¡s seleccionar el **password**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img67.png)

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img68.png)


Â¡Perfecto! Ahora que ya se ve el nombre del **secreto (`password`)** en la interfaz de configuraciÃ³n de la conexiÃ³n con **Azure Key Vault**, significa que el problema de permisos ya se resolviÃ³. âœ…

---

âœ… Â¿QuÃ© confirma esto?

- El secreto **ya carga correctamente** desde el Key Vault.
    
- La identidad gestionada de **Azure Data Factory (`z2h-adf-suffix`)** ya tiene permisos suficientes, probablemente porque le asignaste correctamente el rol `Key Vault Secrets User`.
    
- Ya puedes usar el secreto (por ejemplo, una contraseÃ±a) en tu conexiÃ³n segura.


Antes de finalizar la configuraciÃ³n de la conexiÃ³n en ADF, puedes probarla desde el Runtime hacia la base de datos. Simplemente completa los campos de conexiÃ³n requeridos y haz clic en **(Test)** para confirmar que la conexiÃ³n desde el Runtime estÃ© funcionando correctamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img70.png)

---

**PrÃ³ximos Pasos**

Para que tu ADF pueda conectarse al SQL Server desde el Self-hosted IR, sigue estos pasos:

1. En el **Linked Service** dentro del portal ADF:
        
    - Pon **Encrypt = Optional** (en vez de â€œMandatoryâ€).
        
    - Marca la casilla **Trust server certificate**.
        
    - Guarda la configuraciÃ³n aunque el botÃ³n â€œTest connectionâ€ muestre error; esa prueba no siempre refleja la conexiÃ³n real por parte del SHIR.

2. Haz clic en **â€œTest connectionâ€** para validar que todo funcione correctamente.
    
3. Si todo estÃ¡ bien, haz clic en **"Save"** para guardar la configuraciÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img69.png)



---

âœ… SelecciÃ³n de tabla 

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img71.png)

Ahora podemos ver la configuraciÃ³n de una **Table name**  y ya puedes:

- Ver el **linked service `onpremsqlserver`** correctamente seleccionado.
    
- Confirmar que se conecta mediante el **Integration Runtime `integrationRuntime1`**, que estÃ¡ **activo y saludable** (Ã­cono verde âœ…).
    
- Ver que **la lista de tablas se carga automÃ¡ticamente** desde la base de datos `AdventureWorksLT2022`, lo cual solo es posible si:
    
    - El **SQL Server estÃ¡ accesible** desde el IR.
        
    - El **usuario (`sa`) tiene permisos para leer metadatos**.
        
    - Los **drivers y configuraciÃ³n del IR estÃ¡n correctos**.

Ahora solo debes seleccionar la tabla **SalesLT.Address** desde el menÃº de opciones y hacer clic en **OK** para guardar la conexiÃ³n.




Ahora podemos ver parte de la configuraciÃ³n de la **actividad de origen (`Source`)**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img72.png)



Ahora haz clic en **`Preview data`**, para validar que:

- El Integration Runtime puede conectarse a SQL Server.
    
- El dataset `Address` estÃ¡ correctamente configurado.
    
- Puedes ver los datos **antes de ejecutar la actividad**, lo cual ayuda mucho en tareas de debugging y validaciÃ³n previa.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img73.png)


ğŸ“‹ **4) PestaÃ±a Sink**

Ahora vamos a crear una sincronizaciÃ³n con **Azure Data Lake**. Recordemos que se crearÃ¡n las tres capas: **bronce**, **plata** y **oro**. En este caso, comenzaremos con la configuraciÃ³n de la **capa bronce**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img74.png)

Perfecto. En la pestaÃ±a **Sink** vamos a prepararnos para crear un dataset de destino (sink dataset) que apunte a **Azure Data Lake Storage (ADLS)** para almacenar la **capa bronce** de la arquitectura de datos.

---

ğŸ§± Â¿QuÃ© es la Capa Bronce?

En un _Data Lakehouse_, la **capa bronce** almacena los datos **en crudo (raw)**, tal como vienen de la fuente (en este caso SQL Server).  
No se transforman, solo se copian. Su objetivo es conservar fidelidad y trazabilidad.

---

ğŸ› ï¸ Pasos para configurar el Sink hacia Azure Data Lake â€“ Capa Bronce

1. ğŸ”˜ Haz clic en **â€œ+ Newâ€** 

Esto abre el asistente para crear un **dataset nuevo de destino**.

---

3. ğŸ“¥ Crea un nuevo servicio **Azure Data Lake Storage Gen2** para agregarlo al grupo de recursos

ğŸª£ **Pasos para crear Azure Data Lake Storage Gen2 en el portal de Azure**

âœ…**Inicia sesiÃ³n** en el portal de Azure y haz clic en **Create a Resource**
    
âœ…En la barra de bÃºsqueda, escribe y selecciona **"Storage accounts"**.
    
âœ…Haz clic en **â€œ+ Createâ€** o **â€œ+ Nueva cuenta de almacenamientoâ€**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img77.png)
    
âœ…Configura lo bÃ¡sico:**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img78.png)
    
- **Subscription**: Selecciona la suscripciÃ³n deseada.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img79.png)
	
- **Resource group**: Usa el existente.
	
- **Storage account name**: `z2hdatalakesuffix` No olvides cambiar el suffix por un mombre Ãºnico (solo minÃºsculas y sin caracteres especiales).
	
- **Region**: Selecciona una regiÃ³n cercana a tus servicios.
	
- **Performance**: Generalmente **Standard**.
	
- **Redundancy**: Por defecto **Locally-redundant storage (LRS)**.
	
âœ…**Habilita Data Lake Gen2:**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img80.png)
    
- Ve a la pestaÃ±a **"Advanced"**.
	
- Activa la opciÃ³n **â€œEnable hierarchical namespaceâ€** â†’ Esto convierte tu cuenta en un **Data Lake Gen2**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img81.png)

âœ…Haz clic en **â€œReview + createâ€** y luego en **â€œCreateâ€**.

Una vez completado este paso podemos pasar el siguiente paso

7. ğŸ“‚ Selecciona el tipo de dataset

En el asistente:

- Selecciona **Azure Data Lake Storage Gen2** y clic en **Continue**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img75.png)
    
- Luego selecciona un **formato de archivo**:
    
    - **DelimitedText** (CSV) âœ… ideal para la capa bronce
        
    - **JSON** o **Parquet** (si ya piensas en eficiencia o analytics, pero usualmente van en plata/oro)

En nuestro caso seleccionamos **Parquet** como formato para el dataset de salida (_sink_) en tu pipeline lo cual es una excelente decisiÃ³n, incluso para la **capa bronce**, por varias razones tÃ©cnicas.

âœ… Â¿Por quÃ© usar **Parquet** en la capa bronce?

Aunque CSV es mÃ¡s simple, **Parquet** ofrece beneficios clave incluso en la capa bronce:

|Ventaja|DescripciÃ³n|
|---|---|
|**Columnar**|Permite leer solo columnas necesarias â†’ eficiencia en consultas.|
|**CompresiÃ³n**|Archivos mÃ¡s pequeÃ±os â†’ menor almacenamiento.|
|**Tipado**|Mantiene tipos de datos â†’ evita problemas con fechas, nÃºmeros, etc.|
|**Compatibilidad**|Compatible con Spark, Synapse, Databricks, Power BI, etc.|

ğŸ”¹ PrÃ³ximo paso

Haz clic en **"Continue"** para seguir con la configuraciÃ³n del dataset Parquet.

---

3. ğŸ”— Crea el **Linked Service**

El linked service serÃ¡ la conexiÃ³n a tu Azure Data Lake.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img76.png)

- CrÃ©alo:
    
    - Elige la cuenta de almacenamiento **z2hdatalakesuffix**
        
    - Usa **key**.
        
    - Prueba la conexiÃ³n âœ….
	    
    - Create

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img82.png)

---

Ahora **crearemos el contenedor `bronze` en Azure Data Lake Storage Gen2**, paso necesario para configurar un **sink dataset en Azure Data Factory (ADF)** y guardar allÃ­ los archivos de la capa bronce.

---

âœ… Pasos para crear el contenedor `bronze`:

1. **Ir a la cuenta de almacenamiento** en el portal de Azure (por ejemplo, `z2hcdatalakesuffix`).
    
2. En el menÃº lateral izquierdo, navegar a:
    
    - **Data storage** > **Containers**
        
3. Hacer clic en **â• Add container** en la parte superior.
    
4. En el panel derecho:
    
    - **Nombre**: Escribir `bronze` (este serÃ¡ el contenedor lÃ³gico donde se almacenarÃ¡n los archivos).
        
    - **Access level**: Dejar en `Private` (por defecto, ya que no necesitas acceso anÃ³nimo para ADF).
        
5. Hacer clic en el botÃ³n **Create**.
    

---

ğŸ“Œ Â¿Por quÃ© es importante este paso?

El contenedor `bronze` es el lugar donde Azure Data Factory escribirÃ¡ los archivos **Parquet** provenientes de tu origen de datos (por ejemplo, SQL Server). Sin este contenedor, el flujo de datos no tendrÃ¡ dÃ³nde guardar la informaciÃ³n extraÃ­da.

Este contenedor es parte de tu arquitectura de datos en capas:

- **Bronce**: Datos crudos (sin transformar).
    
- **Plata**: Datos limpios y enriquecidos.
    
- **Oro**: Datos listos para anÃ¡lisis o consumo.
    

---


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img84.png)

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img86.png)

---


Ahora se debe **definir el _sink dataset_**, especÃ­ficamente para **escribir datos en formato Parquet dentro de Azure Data Lake Storage Gen2**, en la **capa bronce** de tu arquitectura.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img83.png)

---

ğŸ“ Â¿QuÃ© estÃ¡s configurando aquÃ­?

EstÃ¡s creando un dataset de salida que:

- TendrÃ¡ el nombre `address_parquet`.
    
- UtilizarÃ¡ el **servicio vinculado (linked service)** `AzureDataLakeStorage1` (es decir, tu cuenta de Data Lake Gen2).
    
- GuardarÃ¡ archivos en formato **Parquet**.
    
- EstÃ¡s especificando la **ruta destino** donde se escribirÃ¡ el archivo.
    

---

ğŸ§­ Detalle de los campos:

|Campo|DescripciÃ³n|
|---|---|
|**Name**|Nombre interno del dataset (`address_parquet`).|
|**Linked service**|ConexiÃ³n hacia tu cuenta de Azure Data Lake Storage Gen2.|
|**File path**|Ruta lÃ³gica dentro del Data Lake: puedes definir el contenedor (File system), subcarpetas (Directory) y nombre del archivo (`File name`). Ejemplo: `bronze/saleslt/address.parquet`.|
|**Import schema**|En este caso estÃ¡ marcado como `None`, ya que el esquema vendrÃ¡ desde el _source_ (la base de datos SQL).|

---

âœ… Â¿QuÃ© sigue?

1. Completar la ruta de destino (`File path`) en tu Data Lake. Solo debes hacer clic en el icono del folder y seleccionar **From root**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img85.png)
    
2. Selecciona **bronze**
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img87.png)

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img88.png)

 3. Y haz clic en **OK** para confirmar la creaciÃ³n del dataset.


---

ğŸ’¡ Con este paso terminamos de configurar la **sincronizaciÃ³n a la capa bronce**, donde los datos se almacenan **tal como vienen del origen**, sin transformaciones.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img89.png)

---

### ğŸ”¹ 4. Debug

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img90.png)

Es hora de ejecutar un **Debug** en un **pipeline** que contiene una actividad `Copy data` 

---

ğŸ” Â¿QuÃ© es el botÃ³n **Debug**?

El botÃ³n **Debug** te permite **ejecutar el pipeline inmediatamente** para probarlo **sin necesidad de publicarlo ni de configurar un trigger**. Es ideal para validar que:

- La configuraciÃ³n del origen y destino (sink) estÃ¡ correcta.
    
- Las conexiones funcionan.
    
- La transformaciÃ³n (si hubiera) se realiza como se espera.
    

---

âœ… En este caso, Â¿quÃ© se va a probar?

Al hacer clic en `Debug`, ADF va a:

1. Conectarse al origen (SQL Server on-premises, vÃ­a Integration Runtime).
    
2. Leer la tabla `Address`.
    
3. Escribir los datos en formato **Parquet** en el contenedor **`bronze`** de tu **Azure Data Lake Storage Gen2**.
    

---

ğŸ§ª Resultado esperado del Debug

- Si todo estÃ¡ correcto: verÃ¡s un **Ã­cono verde con una paloma (âœ”ï¸)** junto a la actividad.
    
- Si hay errores (conexiÃ³n, formato, permisos, etc.): aparecerÃ¡ una **equis roja (âŒ)** con detalles del error al hacer clic.
    

---

ğŸ“Œ Importante:

- Esta ejecuciÃ³n **no persiste cambios en producciÃ³n** hasta que publiques.
    
- Es Ãºtil para desarrollar y corregir en tiempo real sin necesidad de esperar despliegues.
    

#### **EjecuciÃ³n Debug**

Ahora, simplemente haz clic en 'Debug' para ejecutar el pipeline. VerÃ¡s en la parte inferior el estado de ejecuciÃ³n del pipeline y su monitoreo.

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img91.png)





DespuÃ©s de terminar el debug verÃ¡s un error que se muestra en la ejecuciÃ³n de **Debug** indicando claramente un problema con el entorno de ejecuciÃ³n **Self-hosted Integration Runtime (IR)**. El cual puedes verificar en el icono de **mensaje**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img92.png)

---

#### âŒ **Error: Java Runtime Environment (JRE) no encontrado**

**Mensaje completo del error:**

> `Java Runtime Environment cannot be found on the Self-hosted Integration Runtime machine. It is required for parsing or writing to Parquet/ORC files.`

---

ğŸ“Œ **Â¿QuÃ© significa esto?**

* EstÃ¡s intentando escribir datos en **formato Parquet**, lo cual requiere que **Java estÃ© instalado** en la mÃ¡quina que ejecuta el **Self-hosted Integration Runtime (IR)**.

* El error indica que el archivo **`jvm.dll`** no se encuentra, lo que confirma que **no hay una instalaciÃ³n vÃ¡lida de Java** o que no estÃ¡ configurada correctamente.

---

âœ… **SoluciÃ³n paso a paso**

1. **Instalar Java (JDK o JRE) en la mÃ¡quina del Self-hosted IR:**

* Descarga e instala **Java (JDK 8 o superior)** desde el siguiente enlace:
 [Java Downloads](https://download.oracle.com/java/17/archive/jdk-17.0.12_windows-x64_bin.exe)

2. **Configura las variables de entorno en esa mÃ¡quina:**

* Crea la variable `JAVA_HOME` apuntando a la carpeta raÃ­z del JDK instalado.
* Agrega `JAVA_HOME\bin` al **`PATH`** del sistema.

---

ğŸ”§ **Â¿QuÃ© son las variables de entorno?**

Las **variables de entorno** son configuraciones del sistema operativo que permiten a los programas (como Azure Data Factory) encontrar componentes clave, como **Java**, y ejecutar tareas necesarias como la serializaciÃ³n de datos en formatos como **Parquet** y **ORC**.

---

ğŸ› ï¸ **Paso a paso para configurar `JAVA_HOME` y `PATH`**

> âš ï¸ **Realiza estos pasos en la mÃ¡quina donde estÃ¡ instalado el Self-hosted Integration Runtime.**

---

âœ… 1. **Agrega `%JAVA_HOME%\bin` al `PATH`**

1. Abre **Variables de entorno**.

2. Busca la variable llamada `Path` en **"Variables del sistema"** y haz clic en **Editar...**.

3. En la ventana de ediciÃ³n, haz clic en **"Nuevo"** y agrega:

```
%JAVA_HOME%\bin
```

4. Acepta todos los cambios (OK, OK, OK).

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img93.png)

âœ… 2. **Configura o verifica la variable `JAVA_HOME`**

1. En la secciÃ³n de **Variables del sistema**, asegÃºrate de que exista una variable llamada `JAVA_HOME`, con un valor como:
2. 
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img94.png)

```
C:\Program Files\Java\jdk-XX
```

(Sustituye `jdk-XX` con la carpeta real de tu instalaciÃ³n de Java).

2. Abre la ventana de **CMD** y ejecuta el siguiente comando para verificar la instalaciÃ³n de Java:

```
java -version
```

Si todo estÃ¡ bien, deberÃ­as ver la versiÃ³n de **Java** instalada.

3. **Reinicia el servicio del Self-hosted Integration Runtime** para que detecte la nueva configuraciÃ³n.



---

ğŸ¯ **Â¿Por quÃ© es importante?**

ADF necesita ejecutar procesos **Java** internamente para trabajar con archivos **Parquet u ORC**, y sin estas variables correctamente configuradas, **no podrÃ¡ encontrar ni usar Java**, lo que resulta en el error que viste.

---

ğŸ“ **Nota**

Este error es **especÃ­fico para formatos como Parquet y ORC**, ya que **Azure Data Factory** necesita Java para manejar la **serializaciÃ³n/deserializaciÃ³n** de estos formatos.

#### ğŸ‰ **La ejecuciÃ³n del debug fue exitosa!**

Si ejecutas nuevamente el 'Debug', ahora verÃ¡s que la ejecuciÃ³n ha sido **exitosa**. Ahora vamos a revisar quÃ© sucediÃ³.

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img95.png)

ğŸ” **Detalles de la EjecuciÃ³n**

1. **Actividades en ADF:**

   * ğŸš€ Como puedes ver, el **pipeline** en **Azure Data Factory** tiene la actividad **`Copy data`** y su correspondiente actividad **`Copy address table`**.
   * âœ… Ambas actividades tienen el **Ã­cono verde con la paloma (âœ”ï¸)**, lo que indica que **se ejecutaron correctamente**.

---

2. **EjecuciÃ³n del Debug:**

   * ğŸ“Š En el panel inferior, el **estado del pipeline** estÃ¡ marcado como **"Succeeded"** (Ejecutado con Ã©xito), lo que significa que **la prueba de ejecuciÃ³n fue exitosa**.
   * ğŸ’¬ El mensaje **"Pipeline status: Succeeded"** confirma que **no hubo problemas** y que los datos fueron copiados sin errores.

---

3. **DuraciÃ³n y Runtime:**

   * â±ï¸ La duraciÃ³n de la ejecuciÃ³n fue de **19 segundos**, lo que indica que el proceso de copiar datos fue rÃ¡pido y eficiente.
   * ğŸ”„ El runtime **`integrationRuntime1`** muestra que se utilizÃ³ el **Integration Runtime autoalojado** para ejecutar el pipeline.

---

ğŸš€ **Â¿QuÃ© significa esto?**

1. **VerificaciÃ³n exitosa del debug:**

   * ğŸ¯ Al hacer clic en **Debug**, el proceso se ejecutÃ³ **sin problemas** y los datos fueron copiados correctamente de un origen (base de datos `SalesLT.Address`) a un destino (Azure Data Lake).

2. **Sin errores:**

   * âœ… Al ver el Ã­cono verde con la paloma, puedes estar seguro de que **no hubo errores** relacionados con conexiones, configuraciÃ³n o permisos.

3. **El proceso de debug no afecta la producciÃ³n:**

   * ğŸ›¡ï¸ El **debug** es Ãºtil para verificar el pipeline **sin realizar cambios permanentes**. Este proceso no afecta los datos ni los entornos de producciÃ³n hasta que se decida **publicar** los cambios.

---

ğŸï¸ **ValidaciÃ³n del Contenedor "Bronze" en Azure Data Lake Storage**

Ahora, vamos a validar si el archivo **`SalesLT.Address.parquet`** fue creado correctamente en el contenedor **"bronze"** de **Azure Data Lake Storage**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img96.png)

 ğŸ“ **Detalles del archivo creado:**

1. **Archivo creado:**

   * ğŸ“‚ El archivo **`SalesLT.Address.parquet`** estÃ¡ presente en el contenedor **bronze**.
   * ğŸ“¥ Este archivo es el resultado de la actividad **Copy Data** en el pipeline, que copiÃ³ los datos de la tabla `SalesLT.Address` en formato **Parquet**.

2. **Detalles del archivo:**

   * **TamaÃ±o:** El archivo tiene un tamaÃ±o de **35.08 KiB**.
   * ğŸ”¥ El archivo estÃ¡ en el **tier "Hot"**, lo que significa que es fÃ¡cilmente accesible para **operaciones frecuentes** de lectura/escritura.

---

âœ… **ConfirmaciÃ³n del proceso exitoso:**

* El archivo **Parquet** fue generado correctamente y almacenado en el contenedor adecuado en **Azure Data Lake Storage** (en la zona **"bronze"**), tal como se esperaba.

---

ğŸ“ **Â¿QuÃ© significa esto?**

1. **ValidaciÃ³n de Ã©xito:**

   * El archivo **`SalesLT.Address.parquet`** confirma que el proceso de **transformaciÃ³n y carga de datos** en formato Parquet se completÃ³ **correctamente**.

2. **ConfirmaciÃ³n de que el pipeline funcionÃ³ bien:**

   * ğŸ‰ El **debug en ADF** fue exitoso, y ahora tienes el archivo Parquet almacenado y listo para su uso en **Azure Data Lake**.

---

ğŸ¯ **Resumen:**

* El **debug fue exitoso** ğŸ†, lo que significa que los datos fueron copiados sin errores.
* El archivo **`SalesLT.Address.parquet`** estÃ¡ presente en el contenedor **bronze** y listo para ser utilizado.
* âœ… Todo estÃ¡ funcionando perfectamente.


#### ğŸš€ **Paso Final: Publicar Cambios en Azure Data Factory**

ğŸ¯ **Publish**

Una vez que hayas terminado de crear y configurar tu pipeline, datasets y servicios vinculados, el siguiente paso clave es **publicar** los cambios. Esto es esencial para que los recursos estÃ©n disponibles en el entorno de producciÃ³n.

ğŸŸ¡ **Haz clic en:**
ğŸ”˜ **Publish all (5)** â€“ Esto indica que hay 5 cambios pendientes de publicar.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img97.png)

---

âœ… Â¿QuÃ© sucede al publicar?

Al hacer clic en **Publish all**, estÃ¡s:

* ğŸ’¾ **Guardando permanentemente** los cambios en el entorno de Azure Data Factory.
* â±ï¸ **Habilitando la ejecuciÃ³n programada o manual** de tus pipelines.
* ğŸ§© **Activando** los nuevos datasets y linked services configurados.

---

ğŸ‘ï¸â€ğŸ—¨ï¸ **Pantalla de Proceso de PublicaciÃ³n**

Haz clic en el botÃ³n de **Publishing** para ver el resumen de los cambios:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img98.png)

AquÃ­ verÃ¡s todos los elementos pendientes por publicar:

ğŸ“‹ **Cambios pendientes:**

| Tipo de elemento  | Nombre             | Estado     |
| ----------------- | ------------------ | ---------- |
| ğŸ“¦ Pipeline       | `pipeline1`        | ğŸ†• Nuevo   |
| ğŸ“„ Dataset        | `address`          | ğŸ†• Nuevo   |
| ğŸ“„ Dataset        | `address_parquet`  | ğŸ†• Nuevo   |
| ğŸ”— Linked service | `onpremssqlserver` | âœï¸ Editado |
| ğŸ” Linked service | `AzureKeyVault1`   | ğŸ†• Nuevo   |

---

âœ… AcciÃ³n Final

ğŸ”˜ **Haz clic en:**
ğŸ”µ **Publish**

Este botÃ³n aplicarÃ¡ todos los cambios al entorno **en vivo**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img99.png)

---

ğŸ‰ Resultado

âœ¨ Â¡Listo! Ahora tus cambios han sido publicados y estÃ¡n activos.

ğŸŸ¢ Tus pipelines, datasets y servicios vinculados ya estÃ¡n listos para:

* ğŸ Ejecutarse manualmente
* â° Programarse automÃ¡ticamente
* ğŸ“¦ Integrarse en flujos mÃ¡s complejos

---

ğŸ“ NOTA IMPORTANTE

> Siempre **publica** los cambios en ADF luego de editar o crear nuevos recursos. Si no los publicas, seguirÃ¡n solo en tu entorno de desarrollo y no estarÃ¡n disponibles para su ejecuciÃ³n.


---


#### ğŸ§© Â¿QuÃ© sigue?

ğŸ”§ En los prÃ³ximos pasos, comenzaremos a construir el pipeline paso a paso:

* ğŸ“ **CreaciÃ³n del pipeline**
* ğŸ”— **ConfiguraciÃ³n de orÃ­genes y destinos**
* ğŸ“¤ **Copiado de tablas completas**
* ğŸ” **AutomatizaciÃ³n del proceso para mÃºltiples tablas**
