
# AutomatizaciÃ³n en Azure: Databricks + ADF

## ğŸš€ Volvamos a **Azure Data Factory (ADF)**

Estamos aquÃ­ para completar el pipeline que ejecutarÃ¡ los notebooks de Databricks **de manera automÃ¡tica**. Pero antesâ€¦ ğŸ¤”

**Â¿Por quÃ© usar ADF si desde Databricks tambiÃ©n podemos automatizar los notebooks directamente?**

ğŸ‘‰ **La respuesta: Depende del escenario**.

Aunque Databricks tiene sus propios mecanismos de orquestaciÃ³n, **ADF** sigue siendo la mejor opciÃ³n cuando se necesita **coordinar mÃºltiples servicios de Azure** o integrar **datos de diversas fuentes externas**.

---

### ğŸ¯ El Caso de Nuestro Pipeline

La razÃ³n principal de elegir ADF en nuestro caso es que, en el primer paso de nuestro pipeline, necesitamos conectarnos a un entorno **On-Premises** o **Local**. Esto nos permitirÃ¡ **importar o migrar la base de datos** desde **SQL Server** en la VM.

Y, como ya sabemos, para lograr esto, creamos un **Runtime** que se encarga de establecer la conexiÃ³n entre **SQL Server** y **ADF**. AsÃ­, podremos completar el pipeline de **extremo a extremo** dentro de ADF.

---

## ğŸ“Œ**Databricks vs ADF**

### ğŸ“ˆ **AutomatizaciÃ³n en Databricks**

#### Opciones para ejecutar notebooks automÃ¡ticamente:

1. **Jobs en Databricks** ğŸ§‘â€ğŸ’»  
    Permiten programar notebooks con una frecuencia definida (diaria, semanal, por horas). Estos jobs pueden ejecutarse en **clÃºsteres nuevos o existentes**, y configurarse con **parÃ¡metros, dependencias y alertas**.
    
2. **Databricks REST API** ğŸŒ  
    Permite integrar la ejecuciÃ³n de notebooks en **sistemas externos** o dispararlos desde aplicaciones personalizadas.
    
3. **Databricks Workflows (Jobs + Lakeflow Jobs)** ğŸ”„  
    En los Ãºltimos aÃ±os, Databricks ha fortalecido sus capacidades de orquestaciÃ³n con **Workflows**. Estos permiten crear pipelines de datos mÃ¡s complejos, encadenar tareas (**notebooks, scripts, SQL, ML, etc.**), establecer **dependencias, reintentos y monitoreo**.  
    AdemÃ¡s, en 2025, Databricks introdujo **Lakeflow Jobs**, que mejoran la visibilidad y trazabilidad de los pipelines de datos, utilizando tablas del sistema como:  
    `system.lakeflow.pipelines`.
    

#### âœ… **ConclusiÃ³n sobre Databricks**

Si todo el flujo de trabajo estÃ¡ contenido dentro de **Databricks**, los **Workflows** son suficientes y te permiten reducir la necesidad de herramientas adicionales como ADF.

---

### ğŸ¤– **Â¿Por quÃ© usar Azure Data Factory?**

Aunque **Databricks** tiene opciones robustas, **ADF** sigue siendo la mejor opciÃ³n cuando:

#### ğŸ”§ **Ventajas de ADF:**

|**Beneficio**|**DescripciÃ³n**|
|---|---|
|**OrquestaciÃ³n Multiservicio** ğŸ› ï¸|ADF puede coordinar **Databricks** junto con otros servicios de Azure: SQL Database, Synapse, Cosmos DB, etc.|
|**Monitoreo Centralizado** ğŸ“Š|Un panel Ãºnico para gestionar ejecuciones, alertas y errores en **pipelines** que incluyen mÃºltiples servicios.|
|**Escalabilidad y Flexibilidad** ğŸŒ|Crea flujos de datos complejos y altamente configurables.|
|**IntegraciÃ³n Nativa con Azure** ğŸ”—|ADF es el orquestador ideal cuando tu arquitectura involucra varios servicios de **Azure**.|

---

### ğŸ¯ **Â¿CuÃ¡l Usar? ConclusiÃ³n Final**

ğŸ”¹ **Si tu flujo de trabajo es interno a Databricks** (solo notebooks, tablas, ML, transformaciones), usar **Workflows de Databricks** es suficiente, mÃ¡s **simple y fÃ¡cil de mantener**.

ğŸ”¹ **Si tu flujo de trabajo requiere integrar mÃºltiples sistemas de Azure** o servicios externos, **ADF es la opciÃ³n mÃ¡s robusta** para orquestar todo el proceso de manera **escalable** y **mantenible**.

---

### ğŸ”‘ **Lo Importante**: **Â¡Se complementan!**

Ambas herramientas se complementan perfectamente:

- **Databricks** como motor de procesamiento y Workflows internos.
    
- **ADF** como orquestador multiservicio cuando hay arquitecturas mÃ¡s complejas y distribuidas.
    

**Â¡Elige sabiamente segÃºn tu necesidad!** ğŸ¤

---

ğŸ“Œ **NOTA IMPORTANTE**: Aunque Databricks y ADF pueden hacer tareas similares, la clave estÃ¡ en el **alcance** de tu flujo de trabajo y cuÃ¡ntos servicios deseas integrar..

---

## ğŸ”‘ **Job ADF: Conectando Databricks a Azure Data Factory** ğŸš€

Ahora que tenemos claro **por quÃ© usar ADF** para automatizar los notebooks de Databricks, debemos realizar algunos pasos previos antes de continuar con la integraciÃ³n. ğŸ¯

---

### ğŸ› ï¸ **Pasos Previos: Token de Acceso**

Para que ADF se conecte a **Databricks**, necesitaremos crear un **Linked Service**. Esto nos permitirÃ¡ acceder tanto al cÃ³mputo de Databricks como a los notebooks creados en las etapas anteriores (**Bronze To Silver** y **Silver To Gold**).

ğŸ”” **Nota**: Aunque **Azure Databricks** es un servicio nativo dentro de Azure, no se conecta automÃ¡ticamente con ADF, como lo harÃ­a, por ejemplo, **Azure SQL Database** o **Blob Storage**. Por lo tanto, **debemos configurar la autenticaciÃ³n explÃ­citamente**.

---

### ğŸ—ï¸ **Tokens de Acceso en Databricks**

#### Â¿QuÃ© es un **Personal Access Token (PAT)**?

Los **PATs** son credenciales que permiten a servicios externos (como ADF) autenticarse contra la API de **Databricks**.

âœ… **Ventajas de usar PAT**:

- Se pueden generar fÃ¡cilmente desde la UI de **Databricks** en **User Settings > Access Tokens**.
    
- Los tokens estÃ¡n asociados a un usuario especÃ­fico y heredan sus permisos dentro de Databricks.
    
- Puedes definir fechas de expiraciÃ³n, lo que **obliga a rotar los tokens** periÃ³dicamente.
    

ğŸ”’ Para usar el token en ADF, simplemente lo incluimos en el **Linked Service**, junto con la URL de la instancia de Databricks (ejemplo: `https://adb-1234567890.azuredatabricks.net`).

---

### ğŸ”„ **Alternativas MÃ¡s Actuales (2025)**

Aunque los **PATs** son totalmente vÃ¡lidos, en entornos de producciÃ³n se recomienda usar **autenticaciÃ³n basada en Azure Active Directory (AAD)**. ğŸš€

#### **OpciÃ³n 1: Service Principals con AAD** ğŸŒ

- Permiten crear **identidades de aplicaciÃ³n** para que ADF se conecte a Databricks **sin depender de un usuario humano**.
    
- **Beneficios**: mayor seguridad, **rotaciÃ³n automÃ¡tica de credenciales** y cumplimiento con polÃ­ticas corporativas.
    

#### **OpciÃ³n 2: Managed Identity** ğŸ”‘

- ADF puede usar su propia **identidad administrada** para autenticarse directamente en **Databricks**.
    
- **Ventajas**: Elimina la gestiÃ³n manual de tokens y es ideal para arquitecturas sin credenciales estÃ¡ticas.
    

---

#### âš–ï¸ **Comparativa: PAT vs AAD (2025)**

|**OpciÃ³n**|**Ventajas**|**Uso Recomendado**|
|---|---|---|
|**PAT (Personal Access Token)**|âœ”ï¸ ConfiguraciÃ³n rÃ¡pida y sencilla|âœ… Entornos de desarrollo o prueba|
|**AAD con Service Principals**|âœ”ï¸ Alta seguridad, rotaciÃ³n automÃ¡tica de credenciales, cumplimiento de polÃ­ticas|âœ… Entornos productivos|
|**Managed Identity**|âœ”ï¸ No requiere gestiÃ³n manual de credenciales, ideal para arquitecturas modernas|âœ… Arquitecturas sin credenciales estÃ¡ticas|

---

#### ğŸ¯ **RecomendaciÃ³n Final**

- **Para entornos de prueba o desarrollo**: **Usa tokens personales (PAT)**, ya que son rÃ¡pidos de configurar.
    
- **Para entornos productivos**: **Usa AAD con Service Principals o Managed Identity**. Esto aumenta la seguridad y simplifica la gestiÃ³n de credenciales a largo plazo.
    

---

ğŸ“Œ **NOTA**: La clave estÃ¡ en elegir la opciÃ³n que mejor se ajuste a **tu flujo de trabajo** y **requerimientos de seguridad**.

---

### ğŸ—ï¸ **Generar Token de Acceso para ADF - Databricks**

Para completar la integraciÃ³n con **Azure Data Factory (ADF)**, necesitamos generar un **Token de Acceso Personal (PAT)** en **Databricks**. Sigue estos pasos:

---

#### 1ï¸âƒ£ **Acceder a tu Workspace de Databricks**

* Abre tu **Workspace de Databricks**.
* En la zona lateral superior, haz clic en tu **cuenta de usuario**.
* Luego selecciona la opciÃ³n **Settings**.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img279.png)

---

#### 2ï¸âƒ£ **Acceder a la opciÃ³n Developer**

* Desde el panel **Settings**, selecciona la opciÃ³n **Developer**.
* DespuÃ©s haz clic en **Manage**.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img280.png)

---

#### 3ï¸âƒ£ **Generar Nuevo Token**

* En la secciÃ³n **Manage**, haz clic en **Generate new token**.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img281.png)

---

#### 4ï¸âƒ£ **Configurar el Token**

* **Comentario**: AÃ±ade una descripciÃ³n para tu token (por ejemplo: "Token para ADF").
* **Lifetime**: Establece la duraciÃ³n del token. Por defecto, es de **90 dÃ­as**, pero puedes ajustarlo segÃºn tus necesidades.
* Haz clic en **Generate** para crear el token.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img282.png)

> **Â¡Importante!**: **Copia el Token** inmediatamente y guÃ¡rdalo en un lugar seguro, ya que no podrÃ¡s verlo nuevamente.

---

Â¡Listo! Ahora tienes tu **Token de Acceso Personal (PAT)** generado para usarlo en el **Linked Service** de **Azure Data Factory**. ğŸš€


### ğŸ” **Gestionar el Token de Acceso con Azure Key Vault** ğŸ›¡ï¸

Como ya sabemos, **Azure Key Vault** es una excelente herramienta para administrar y proteger nuestras **credenciales** de manera segura. Ya utilizamos Key Vault para gestionar credenciales de **SQL Server**, Â¡ahora haremos lo mismo con el **Token** de **Databricks** para ADF!

---

#### 1ï¸âƒ£ **Acceder a Azure Key Vault**

* Abre el servicio **Azure Key Vault** desde el **grupo de recursos** de tu proyecto.
* En el panel izquierdo de **Key Vault**, selecciona **Secrets**.
* Luego haz clic en **Generate/Import** para crear un nuevo secreto.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img283.png)

---

#### 2ï¸âƒ£ **Configurar el Secreto del Token**

* **Nombre**: En el campo **Name**, coloca el nombre del secreto, por ejemplo: **ADFToken**.
* **Valor del Secreto (Secret Value)**: Pega el **Token** de acceso que generaste anteriormente en Databricks.
* Haz clic en **Create** para almacenar el secreto de manera segura.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img284.png)


---

#### âœ… **Resultado Final**

Ahora, tu **Token** de Databricks estarÃ¡ almacenado y protegido en **Azure Key Vault**. ADF podrÃ¡ acceder a este secreto para gestionar la autenticaciÃ³n automÃ¡ticamente y de forma segura, evitando la necesidad de manejar el token directamente.

---

#### ğŸ”‘ **Â¿Por quÃ© usar Azure Key Vault?**

* **Seguridad**: Protege credenciales y secretos como el Token de acceso.
* **Accesibilidad**: ADF puede acceder automÃ¡ticamente al token almacenado.
* **AutomatizaciÃ³n**: Gestiona la rotaciÃ³n de credenciales y mejora el cumplimiento de polÃ­ticas de seguridad.


----

## ğŸŒ **Â¿QuÃ© es un Linked Service en ADF?** ğŸ”—

En **Azure Data Factory (ADF)** y servicios similares como **Synapse**, un **Linked Service** es una conexiÃ³n configurada a un **servicio externo**. Â¡Es un componente clave para integrar y coordinar diferentes servicios dentro de tus pipelines!

---

### ğŸ¯ **Funciones de un Linked Service**

#### 1ï¸âƒ£ **ConexiÃ³n a Servicios Externos**

- **Definir cÃ³mo se conecta** ADF con el servicio externo (por ejemplo, la **URL**, las **credenciales** y la **configuraciÃ³n de autenticaciÃ³n**).
    

#### 2ï¸âƒ£ **Uso en Pipelines**

- Permite que **ADF** utilice el servicio para realizar actividades dentro de los pipelines:
    
    - Ejecutar **notebooks** de Databricks
        
    - Ejecutar **jobs**
        
    - Leer y **escribir datos**
        
    - Y mucho mÃ¡s...
        

#### 3ï¸âƒ£ **SeparaciÃ³n de la LÃ³gica**

- **Separa** la lÃ³gica de **quÃ© hacer** (las actividades dentro del pipeline) de la lÃ³gica de **cÃ³mo conectarse** (la configuraciÃ³n de autenticaciÃ³n y acceso a los servicios).
    

---

### ğŸ§© **Linked Service para Azure Databricks**

En el contexto de **Azure Databricks**, el **Linked Service** define cÃ³mo ADF se conecta a un **workspace de Databricks**. Esto permite a **ADF** usar Databricks para ejecutar actividades como:

- **Ejecutar notebooks**.
    
- **Ejecutar trabajos**.
    
- **Acceder a datos** almacenados en Databricks.
    

---

### ğŸš€ **KEYS**

- **Centraliza la configuraciÃ³n** de la conexiÃ³n a servicios externos.
    
- **Facilita la gestiÃ³n de la autenticaciÃ³n**, ya que permite almacenar y gestionar credenciales de manera segura.
    
- Hace que tus pipelines sean **mÃ¡s modulares** y reutilizables, separando la lÃ³gica de conexiÃ³n de la lÃ³gica de procesamiento.
    

---

### ğŸ”‘ **Recuerda**: 

Un **Linked Service** no es una actividad en sÃ­, sino una **configuraciÃ³n** que le dice a ADF cÃ³mo conectarse a otros servicios (como Databricks) para luego usar esos servicios dentro de los pipelines.

## ğŸ”— **Crear un Linked Service en ADF para Azure Databricks** 

Â¡Vamos a conectar **Azure Data Factory (ADF)** con **Azure Databricks**! Este es un paso clave para poder ejecutar **notebooks**, **jobs** y otras actividades desde ADF. AquÃ­ te dejo los pasos con un formato fÃ¡cil de seguir.

---

### 1ï¸âƒ£ **Acceder al Workspace de ADF**

* ğŸ”¹ **Abre el Workspace** de ADF.
* ğŸ”¹ En el **panel izquierdo**, selecciona **Manage**.
* ğŸ”¹ Haz clic en **Linked Services** y luego en **New** para crear un **Nuevo Linked Service**.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img277.png)

---

### 2ï¸âƒ£ **Seleccionar Azure Databricks**

* ğŸ” En el panel de **Nuevo Linked Service**, busca en la secciÃ³n **Compute**.
* ğŸ” Selecciona **Azure Databricks**.
* ğŸ”„ Haz clic en **Continue** para avanzar a la configuraciÃ³n.
  ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img278.png)

---

### 3ï¸âƒ£ **Completar el Formulario de ConfiguraciÃ³n** ğŸ“

Ahora vamos a configurar **paso a paso** todo lo necesario para que ADF se conecte correctamente a **Azure Databricks**.

#### Formulario de ConfiguraciÃ³n:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img240.png)


| **Campo**                                   | **DescripciÃ³n**                                                                                                              |
| ------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| ğŸ–¥ï¸ **Connect via Integration Runtime**     | Usa **AutoResolveIntegrationRuntime**. ADF gestionarÃ¡ el runtime automÃ¡ticamente segÃºn la regiÃ³n.                            |
| ğŸ” **Azure Subscription**                   | Selecciona la suscripciÃ³n de **Azure** donde estÃ¡ desplegado el **workspace de Databricks**.                                 |
| ğŸ¢ **Databricks Workspace**                 | Define el **workspace** especÃ­fico al que ADF se conectarÃ¡ (ej: `z2h-databricks-suffix`).                                    |
| ğŸ’» **Select Cluster**                       | Elige un **Existing interactive cluster** para ejecutar notebooks y jobs.                                                    |
| ğŸŒ **Databricks Workspace URL**             | Introduce la **URL** de la instancia de Databricks (ej: `https://adb-...azuredatabricks.net`).                               |
| ğŸ”‘ **Authentication Type**                  | Selecciona **Access Token**, pero con la integraciÃ³n de **Azure Key Vault**.                                                 |
| ğŸ” **Azure Key Vault (AKV) Linked Service** | Vincula tu **Key Vault** con el **Linked Service** para acceder al **token** de forma segura.                                |
| ğŸ“œ **Secret Name**                          | El nombre del secreto en Key Vault serÃ¡ **ADFToken**, donde ADF obtendrÃ¡ el token de acceso automÃ¡ticamente.                 |
| ğŸ”„ **Choose from Existing Clusters**        | Selecciona un **clÃºster existente** de Databricks para las ejecuciones (ej: `Z2h Cluster 2025-09-11`).                       |
| ğŸ§ª **Test Connection**                      | Haz clic en **Test connection** para verificar que la conexiÃ³n se realizÃ³ correctamente. ğŸŒŸ Con un mensaje verde âœ… de Ã©xito. |
| ğŸ’¾ **Create**                               | Haz clic en **Create** para guardar el **Linked Service**. Â¡Listo para usarlo! ğŸ‰                                            |

---

### âš™ï¸ **Testea la ConexiÃ³n** ğŸ”

* ğŸš€ Haz clic en **Test connection** para asegurarte de que **ADF** puede conectar con **Databricks**.
* âœ… **ConexiÃ³n Exitosa**: Si todo estÃ¡ bien, verÃ¡s un mensaje verde que dice **Connection Successful**.

---

### ğŸ **Â¡Crea el Linked Service!**

* Haz clic en **Create** para **guardar** el **Linked Service**.
* Â¡Listo! Ahora **ADF** estÃ¡ **conectado** a **Databricks** y listo para ejecutar **notebooks** y **jobs**.

---

#### ğŸ¯ **Recapitulando**: Â¿QuÃ© lograste?

* Conectaste **ADF** con **Azure Databricks** de forma segura, usando **Azure Key Vault** para almacenar el **token**.
* Ahora puedes **ejecutar notebooks** y **jobs** directamente desde **ADF**.

---

ğŸ“Œ **NOTA IMPORTANTE** ğŸ› ï¸

â¡ï¸ Â¡Este paso es **esencial** para integrar Databricks en tus pipelines de ADF! Si sigues los pasos correctamente, tendrÃ¡s acceso a todos los beneficios de la automatizaciÃ³n en Databricks, con la seguridad de **Azure Key Vault**.

### âœ”**Guardar los cambios**

âœ…Ahora solo debes guardar los cambios solo debes seleccionar **Publish all** 


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img241.png)


## ğŸš€ **EdiciÃ³n del Pipeline en Azure**

Â¡Todo listo! Ahora vamos a editar el **Pipeline** que creamos en las secciones anteriores. ğŸ’»

ğŸ› ï¸ **Listo, con todo lo anterior realizado**, ahora vamos a **editar el Pipeline** que creamos en las secciones anteriores.

### **1ï¸âƒ£ SelecciÃ³n del Pipeline:**

ğŸ‘ˆ Desde el panel izquierdo, selecciona **Author**.  
ğŸ“‚ En la secciÃ³n **Pipelines**, haz clic en el Pipeline llamado **copy_all_tables**.  
ğŸ” En el _toolbox_, escribe **Notebook**.

ğŸ“¦ Luego, simplemente **arrastra el Notebook de Databricks** hacia la **zona del lienzo**, justo a la derecha del contenedor **ForEach** â¡ï¸ğŸ“

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img242.png)

---

### **2ï¸âƒ£ AsignaciÃ³n de nombre al Notebook:**

Ahora vamos a darle un nombre a nuestro Notebook:

* Haz clic en la actividad **Notebook** desde el lienzo para abrir las opciones.
* En la pestaÃ±a **General**, en **Name**, asigna el nombre: `**Bronze-to-Silver**`.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img243.png)

---

### **3ï¸âƒ£ ConexiÃ³n de actividades (ForEach â†’ Notebook):**

**Conectando el `ForEach` con el `Notebook` (Databricks):**

* ğŸ› ï¸ En la actividad `ForEach`, conecta la actividad **Notebook (Bronze-to-Silver)**.
* **Importante:** La conexiÃ³n de flujo de control es crucial, como se resalta en rojo en la imagen.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img244.png)

#### âœ… **Pasos para conectar actividades:**

| **AcciÃ³n** | **DescripciÃ³n**                                                                      | **Icono** |
| ---------- | ------------------------------------------------------------------------------------ | --------- |
| **Paso 1** | Pasa el mouse por el borde derecho de la actividad interna en `ForEach`.             | â¡ï¸        |
| **Paso 2** | VerÃ¡s varios Ã­conos, selecciona el **Ã­cono verde con check (âœ”ï¸)**: **Success Path**. | âœ”ï¸        |
| **Paso 3** | **Arrastra** la flecha hacia la actividad de tipo `Notebook`.                        | â¬…ï¸        |

ğŸ“ Esto indica que:
ğŸ‘‰ **Si la actividad dentro del `ForEach` termina con Ã©xito**, entonces ejecutar la siguiente actividad (el Notebook).

---

### **4ï¸âƒ£ VerificaciÃ³n de conexiÃ³n exitosa**

Una vez realizado este paso, verÃ¡s cÃ³mo las actividades quedan conectadas correctamente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img245.png)

---

### **5ï¸âƒ£ SelecciÃ³n de Linked Service de Databricks:**

Finalmente, en la pestaÃ±a **Azure Databricks**:

* Ve a la secciÃ³n **Databricks linked service** en el toolbox.
* Selecciona el **Linked Service** que creaste anteriormente (en este caso, **AzureDatabricks1**).

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img246.png)

---

ğŸ”‘ **NOTA IMPORTANTE:**

* AsegÃºrate de que el **Linked Service** que seleccionas estÃ© configurado correctamente para evitar errores de conexiÃ³n mÃ¡s adelante.

---

### **6ï¸âƒ£ SelecciÃ³n de la actividad Notebook:**

* Vuelve a seleccionar la actividad **Notebook**.
* En las opciones, selecciona **Settings** y luego haz clic en **Browse**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img247.png)

---

### **7ï¸âƒ£ Navegar en el Workspace de Databricks:**

Ahora vamos a navegar dentro del **Workspace de Databricks** para encontrar los **Notebooks** que se usarÃ¡n en tu pipeline.

* AparecerÃ¡ una ventana de exploraciÃ³n con tres carpetas principales en el directorio raÃ­z:

  * ğŸ“ **Repos**
  * ğŸ“ **Shared**
  * ğŸ“ **Users** (resaltada en la imagen)

Debes hacer clic en la carpeta **Users**, ya que normalmente ahÃ­ se encuentran los notebooks personales o los compartidos entre equipos.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img248.png)

---

### **8ï¸âƒ£ SelecciÃ³n del Notebook:**

Dentro de la carpeta **Users**, navega a los subdirectorios hasta encontrar el Notebook **Bronze to Silver**.

1. Selecciona el **Notebook** llamado **Bronze to Silver**.
2. Haz clic en **"OK"** para vincularlo a la actividad Notebook en tu pipeline de ADF.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img249.png)

---

### **9ï¸âƒ£ Completado de la actividad Notebook:**

Â¡Ya has completado la configuraciÃ³n para el Notebook **Bronze to Silver** en tu pipeline!

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img250.png)

#### âœ”**Ahora haz completado la actividad Notebook con el acceso al Notebook en Databricks **Bronze To Silver**


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img251.png)


---

## **ğŸ” CreaciÃ³n de una nueva actividad Notebook:**

Repite el mismo proceso anterior, pero esta vez vamos a crear una nueva actividad Notebook. ğŸ’¡

1. **Arrastra** una nueva actividad **Notebook** al lienzo.
2. ConÃ©ctala como lo hicimos entre **ForEach** y **Bronze-to-Silver**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img252.png)

---

### **ğŸ”— ConexiÃ³n de Notebooks:**

Conecta el **Notebook** reciÃ©n creado con el siguiente paso de tu pipeline:

* ConÃ©ctalo tal como conectaste el **ForEach** con el **Bronze-to-Silver**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img253.png)

---

### **ğŸ”– AsignaciÃ³n de nombre al nuevo Notebook:**

1. Selecciona la nueva actividad **Notebook**.
2. Asigna el nombre **Silver-To-Gold**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img254.png)

---

### **ğŸ”„ SelecciÃ³n del Linked Service:**

Recuerda que en la pestaÃ±a **Azure Databricks**:

* En la secciÃ³n **Databricks linked service**, selecciona el **Linked Service** que creaste anteriormente, que en este caso es **AzureDatabricks1**.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img255.png)

---

### **ğŸ” Navegar de nuevo en el Workspace:**

Repitamos el proceso anterior:

1. Selecciona la nueva actividad **Notebook**.
2. Ve a **Settings** y haz clic en **Browse**.
3. Navega nuevamente dentro del **Workspace de Databricks**:

* ğŸ“ **Repos**
* ğŸ“ **Shared**
* ğŸ“ **Users** (resaltada)

Selecciona la carpeta **Users** para encontrar el Notebook correspondiente.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img256.png)

---

### **ğŸ” SelecciÃ³n del Notebook **Silver to Gold**:**

Dentro de la carpeta **Users**, selecciona el Notebook **Silver to Gold** y haz clic en **"OK"** para vincularlo.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img257.png)

---

### **ğŸ” Completado de la actividad Notebook:**

Â¡Has vinculado el Notebook **Silver to Gold** correctamente a tu pipeline!

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img258.png)

---

### **âœ… Guardar los cambios:**

Para finalizar, asegÃºrate de guardar todos los cambios que realizaste:

* Haz clic en **Publish all** para guardar el pipeline.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img259.png)

---

ğŸ‰ Â¡Todo listo! Ahora tu pipeline estÃ¡ completamente configurado con ambos Notebooks: **Bronze to Silver** y **Silver to Gold**. ğŸ…


## ğŸš€ **Test de ConexiÃ³n y EjecuciÃ³n del Pipeline en Azure Data Factory (ADF)**

---

### ğŸ§ª **Paso 1: Verificar la ConexiÃ³n al Contenedor Docker SQL Server**

Antes de lanzar el pipeline, **asegÃºrate de que el contenedor Docker estÃ© activo** en tu mÃ¡quina virtual (Machine-0) ğŸ–¥ï¸. Esto es importante porque el pipeline volverÃ¡ a conectarse para migrar nuevamente las tablas al contenedor **Bronze**.

â¡ï¸ **AcciÃ³n:**
Desde el **Runtime**, completa los campos de conexiÃ³n requeridos y haz clic en **Test**.

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img261.png)

---

### ğŸ” **Paso 2: Validar ConexiÃ³n desde ADF**

â¡ï¸ Selecciona la actividad `look for all tables`
â¡ï¸ En la pestaÃ±a **Settings** â†’ secciÃ³n **Source dataset**
â¡ï¸ Haz clic en **Open**

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img262.png)

ğŸ”„ Esto abrirÃ¡ el dataset **SqlServerTables**
â¡ï¸ Ve a la pestaÃ±a **Connection** â†’ haz clic en **Test connection**

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img263.png)

ğŸ“ **NOTA IMPORTANTE:**
AsegÃºrate de que el puerto `1433` estÃ© expuesto desde **VSCode**. El Runtime necesita ese acceso para que ADF pueda conectarse correctamente.

---

### ğŸ§¨ **Paso 3: Ejecutar el Pipeline**

Â¡Listo! Es momento de ejecutar nuestro _pipeline_ y poner en marcha todas las actividades configuradas

â¡ï¸ En el lienzo, haz clic en el Ã­cono **Add trigger**
â¡ï¸ Selecciona **Trigger now**

Esto iniciarÃ¡ la ejecuciÃ³n del _pipeline_ junto con todas las actividades que lo componen.

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img260.png)

ğŸ†— AparecerÃ¡ una ventana de confirmaciÃ³n: haz clic en **OK**
ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img265.png)

---

### â±ï¸ **Paso 4: Monitorear la EjecuciÃ³n**

Una vez que un pipeline ha sido ejecutado en **Azure Data Factory**, podemos hacerle seguimiento y monitoreo desde la secciÃ³n **Monitor**. Esta funcionalidad permite revisar el estado de ejecuciÃ³n, duraciÃ³n, errores, parÃ¡metros utilizados, y mucho mÃ¡s.

ğŸ§­ Desde el menÃº lateral izquierdo en ADF
â¡ï¸ Haz clic en el Ã­cono de **Monitor (ğŸ•’)**
â¡ï¸ Luego selecciona **Pipeline runs**

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img266.png)

---

#### ğŸ› ï¸ Funciones del Panel de Monitor

| ğŸ”§ Funcionalidad           | ğŸ’¡ DescripciÃ³n                                        |
| -------------------------- | ----------------------------------------------------- |
| **Ver Estado**             | âœ… Ã‰xito / âŒ Fallido / ğŸ”„ En progreso / â¸ï¸ Cancelado   |
| **Info clave**             | Nombre, fecha, duraciÃ³n, ejecutado por, parÃ¡metros    |
| **Actualizar**             | Clic en **Refresh** para ver el estado en tiempo real |
| **Ver detalles**           | Haz clic en el nombre del pipeline                    |
| **Exportar CSV**           | Para auditorÃ­a o anÃ¡lisis externo                     |
| **Re-ejecutar o cancelar** | Con los botones **Rerun** o **Cancel options**        |

---

âœ… **Claves del Panel de Monitoreo:**

* Ideal para depurar errores ğŸ
* Seguimiento completo de ejecuciones ğŸ”
* Evaluar duraciÃ³n de cada actividad ğŸ•

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img268.png)

---

### ğŸ§© **Paso 5: Validar EjecuciÃ³n de Actividades**

Si validamos el estado de las actividades notarÃ¡s que cada actividad se ejecuta una por una en el orden que establecimos para cada actividad. En este caso, vemos que la actividad de "Lookup" se ejecutÃ³ primero con Ã©xito, seguida de la actividad "ForEach Schema Table", que tambiÃ©n fue exitosa. Luego, las actividades "Copy Each Table" se ejecutaron de manera secuencial, y finalmente, el proceso continÃºa con la actividad de notebook llamada "Bronze-to-Silver", que actualmente estÃ¡ en progreso.

Esto demuestra cÃ³mo cada paso depende del anterior para avanzar al siguiente, lo que garantiza que el flujo de trabajo se ejecute correctamente y en el orden deseado. Por ejemplo:

```plaintext
1. Lookup âœ…
2. ForEach Schema Table âœ…
3. Copy Each Table âœ…
4. Bronze-to-Silver ğŸ”„
```

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img267.png)

ğŸ“ **NOTA:**
>A diferencia de cuando ejecutamos los Notebooks en el entorno de Databricks, donde utilizamos el Serverless, este ejecuta las consultas aparentemente mÃ¡s rÃ¡pido, ya que el encendido del clÃºster es mucho mÃ¡s rÃ¡pido que los clÃºsteres creados manualmente. 
>
>En nuestro caso, creamos el clÃºster **z2h-Cluster**, que es el clÃºster que esta utilizando actualmente en ADF para ejecutar las actividades del Notebook. Si te fijas en la actividad **Bronze-to-Silver** en la imagen, marcada como **In Progress**, podemos ver que lleva mÃ¡s de 3 minutos en ejecuciÃ³n, y esto en parte es causado por la demora en el encendido del clÃºster.

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img269.png)

---

### ğŸ—‚ï¸ **Paso 6: Validar la Capa Bronze en el Storage**

Si te diriges al **Storage** y revisas el contenedor **bronze** en la carpeta **SalesLT**, fijÃ©monos en la fecha **9/3/25**. Recordemos que el script en el Notebook **Bronze To Silver** sobreescribe los datos transformados en **Silver** en formato **Delta Lake**, reemplazando lo anterior si existÃ­a.

Revisa el contenedor **bronze**, carpeta **SalesLT**

* Fecha: `9/3/25`
* ValidaciÃ³n de sobreescritura de datos en Delta Lake

ğŸ“¦ CÃ³digo del notebook:

```python
df.write.format("delta").mode("overwrite").save(output_path)
```

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img270.png)

Esto lo podemos confirmar si subes el nivel de la carpeta **SalesLT** a **Address**, donde puedes validar el archivo **Address.parquet**. En la imagen, podemos ver que el archivo tiene la fecha de modificaciÃ³n **9/23/2025**, lo que coincide con el proceso de sobreescritura de datos en el contenedor **bronze**, en contraste con la fecha **9/3/25** del folder **SalesLT**. Esto se debe a que el folder no necesitÃ³ ser creado, pero el archivo **parquet** anterior sÃ­ fue reemplazado por el mÃ¡s actual durante el proceso de transformaciÃ³n y sobreescritura de datos.

ğŸ“ En la carpeta **Address**

* Archivo `Address.parquet` actualizado: `9/23/2025`

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img272.png)

---

### ğŸª™ **Paso 7: Validar la Capa Silver-to-Gold**

Ahora validemos cÃ³mo la actividad **Bronze-to-Silver** ya terminÃ³ de ejecutarse, y ahora se estÃ¡ llevando a cabo la actividad **Silver-to-Gold**, que estÃ¡ en estado **En progreso**.

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img271.png)


ğŸ“¦ Si revisas el contenedor **Silver** en el almacenamiento, notarÃ¡s que tenemos dos archivos Parquet con nombres diferentes. Esto sucede gracias a las capacidades de **Delta Lake**. Como se ve en la imagen, cada archivo tiene un nombre Ãºnico vinculado a la particiÃ³n y versiÃ³n de los datos ğŸ§© â€”algo tÃ­pico de cÃ³mo Delta gestiona la informaciÃ³n para permitir actualizaciones y transacciones eficientes âš¡.

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img273.png)

ğŸš€ **Delta Lake** ofrece funciones avanzadas para gestionar datos de forma eficaz. Una de las mÃ¡s potentes es la **gestiÃ³n de transacciones ACID** ğŸ”, que garantiza lecturas y escrituras consistentes y seguras, incluso en entornos distribuidos ğŸŒ.

ğŸ•’ AdemÃ¡s, **Delta** soporta el **versionado de datos**, lo cual permite hacer actualizaciones incrementales y mantener un historial completo de los cambios ğŸ“š. Esto hace posible recuperar versiones anteriores sin complicaciones.

ğŸ—‚ï¸ Gracias a la **compatibilidad con particiones**, se puede optimizar el rendimiento dividiendo grandes volÃºmenes de datos en secciones lÃ³gicas. A esto se suman las **operaciones de uniÃ³n** y **filtros eficientes**, que aceleran el procesamiento de datos ğŸš….

ğŸ”„ Todo esto se consigue sin sacrificar flexibilidad ni integraciÃ³n con otras herramientas, convirtiendo a **Delta Lake** en una opciÃ³n ideal para manejar grandes volÃºmenes de datos de forma confiable y eficiente ğŸ’¡.

ğŸ“ **NOTA:**
Delta Lake guarda mÃºltiples archivos **Parquet**, cada uno con versiones distintas. Esto permite transacciones ACID y versionado de datos.

ğŸ” **Beneficios de Delta Lake:**

* âœ… Transacciones **ACID**
* ğŸ”„ Versionado de datos
* ğŸ§© Compatibilidad con particiones
* âš¡ Consultas eficientes
* ğŸ”— IntegraciÃ³n con otras herramientas


Las transacciones **ACID** son un conjunto de **principios** que garantizan que las operaciones en bases de datos (como insertar, actualizar o borrar datos) se realicen de manera **segura, confiable y coherente** â€”especialmente en sistemas distribuidos o con muchos usuarios a la vez.

ACID es un acrÃ³nimo que representa:

---
ğŸ“ **NOTA:** âœ… Transacciones **ACID**

ğŸ”’ **A â€“ Atomicidad (Atomicity)**  
Cada transacciÃ³n se ejecuta **completa o no se ejecuta en absoluto**. Si algo falla a mitad de camino, todo se revierte.  
ğŸ‘‰ Ejemplo: Si transfieres dinero de una cuenta a otra, no puede salir el dinero sin entrar en la otra.

ğŸ§  **C â€“ Consistencia (Consistency)**  
La transacciÃ³n debe llevar la base de datos de un **estado vÃ¡lido a otro tambiÃ©n vÃ¡lido**, cumpliendo todas las reglas y restricciones.  
ğŸ‘‰ Ejemplo: Si una regla dice que un saldo no puede ser negativo, ninguna transacciÃ³n debe violarla.

ğŸ”„ **I â€“ Aislamiento (Isolation)**  
Varias transacciones pueden ejecutarse al mismo tiempo, pero deben comportarse como si se ejecutaran **una por una**.  
ğŸ‘‰ Ejemplo: Dos personas comprando el Ãºltimo producto al mismo tiempo: solo una debe lograrlo.

ğŸ›¡ï¸ **D â€“ Durabilidad (Durability)**  
Una vez que una transacciÃ³n ha sido confirmada (commit), **sus cambios persisten**, incluso si ocurre una falla del sistema despuÃ©s.  
ğŸ‘‰ Ejemplo: Si guardas un pedido y luego se apaga el servidor, el pedido sigue estando ahÃ­ cuando vuelve.

---

### âœ… **Resultado Final del Pipeline**

âœ… Cuando finalice la ejecuciÃ³n de las actividades del **Pipeline**, verÃ¡s completado todo el proceso de **transferencia y transformaciÃ³n de datos**, desde el paso inicial **Lookup** ğŸ” hasta la Ãºltima actividad **Silver-to-Gold** âœ¨.

ğŸ–¼ï¸ En la imagen se puede ver que cada actividad ha tenido un **estado exitoso** âœ… y se ha ejecutado dentro del tiempo especificado â±ï¸.

ğŸ“Š Esto confirma que las **tablas fueron correctamente procesadas** y que el flujo de datos entre las capas **Bronce ğŸ¥‰, Plata ğŸ¥ˆ y Oro ğŸ¥‡** se ha realizado como se esperaba, manteniendo la calidad y consistencia del proceso.

Al terminar todas las actividades, verÃ¡s el flujo completo:

* Lookup â†’ ForEach â†’ Copy Table â†’ Bronze-to-Silver â†’ Silver-to-Gold
* Todas con estado **âœ… Satisfactorio**

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img276.png)

---

### ğŸ›‘ **Paso 8: Apagar el ClÃºster Manualmente**

ğŸ›‘ Por Ãºltimo, y no menos importante: **apaga manualmente el clÃºster de Databricks**.

ğŸ•™ Aunque tiene un apagado automÃ¡tico configurado para 10 minutos, como se explicÃ³ antes, hacerlo manualmente te ayuda a **evitar el consumo innecesario de recursos** ğŸ’¸ â€”y eso, al final, significa **menos costos en tu factura de Azure** ğŸ’¼ğŸ’°.

ğŸ‘† Un pequeÃ±o gesto que marca la diferencia en eficiencia y ahorro.

â¡ï¸ Clic en **Stop** en el panel izquierdo del clÃºster

ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img274.png)
ğŸ“·
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img275.png)

---

### ğŸ Â¡Pipeline Ejecutado con Ã‰xito!

