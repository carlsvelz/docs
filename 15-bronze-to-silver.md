

# Bronze To Silver

## ğŸ—‚ï¸ **Explorando el CatÃ¡logo de Databricks**

Ahora que hemos terminado de configurar el **Metastore**, regresemos al **Workspace** en Databricks y hagamos clic en **Catalog**, donde veremos la siguiente estructura de carpetas.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img204.png)

El **Catalog** en Databricks es una secciÃ³n clave para organizar y gestionar tus datos. AquÃ­ puedes explorar tablas, bases de datos, y colaborar con otros usuarios de manera eficiente. A continuaciÃ³n, te explico todo lo que puedes encontrar y cÃ³mo navegar en esta interfaz.

---

### ğŸŒŸ **Secciones Principales del Catalog**

El **Catalog** tiene varias Ã¡reas organizadas que te ayudan a encontrar lo que buscas rÃ¡pidamente. ğŸš€

ğŸ¢ **Mi OrganizaciÃ³n**

AquÃ­ verÃ¡s todos los catÃ¡logos relacionados con una organizaciÃ³n. Normalmente, esto incluye el catÃ¡logo **"system"** (predeterminado) y **"main"** (catÃ¡logo principal).

- ğŸ“‚ **system**
    
- ğŸ“‚ **main**
    

---

ğŸ”„ **Delta Shares Recibidos**

Â¡AquÃ­ es donde se reciben los datos compartidos contigo! Gracias a **Delta Sharing**, puedes acceder fÃ¡cilmente a datos de otros entornos.

- ğŸ“¥ **samples** (Ejemplo de datos compartidos)
    

---

ğŸšï¸ **Legacy**

AquÃ­ encontrarÃ¡s los catÃ¡logos antiguos que forman parte de tu infraestructura de datos.

- ğŸ—ƒï¸ **hive_metastore** (CatÃ¡logo tradicional de Hive)
    

---

ğŸ“Š **Vista de Acceso RÃ¡pido**

### ğŸ” **Quick Access**

Â¡Un Ã¡rea para navegar fÃ¡cilmente! AquÃ­ puedes encontrar:

- **Recientes**: Accede a tablas que has consultado recientemente.
    
- **Favoritos**: Tus tablas favoritas, para tenerlas siempre a mano.
    
- **CatÃ¡logos**: Lista completa de catÃ¡logos disponibles.
    

---

 ğŸ“‹ **Funcionalidades**

|**FunciÃ³n**|**DescripciÃ³n**|
|---|---|
|ğŸ—‚ï¸ **Mis CatÃ¡logos**|Organiza y visualiza catÃ¡logos dentro de tu organizaciÃ³n.|
|ğŸ”„ **Delta Shares**|Accede a los datos que otros comparten contigo usando Delta Sharing.|
|ğŸšï¸ **Legado**|Consulta antiguos catÃ¡logos como el de Hive Metastore.|

---

âœ”ï¸ **Acciones RÃ¡pidas**

- âœ… **Navegar**: Haz clic en los diferentes catÃ¡logos para ver sus tablas.
    
- ğŸ—‚ï¸ **Explorar**: Usa la barra de bÃºsqueda para encontrar lo que necesitas rÃ¡pido.
    
- ğŸ”– **Favoritos**: Marca las tablas que mÃ¡s usas para tenerlas siempre visibles.
    
- ğŸ”„ **Recientes**: Empieza a explorar para llenar esta secciÃ³n con tus tablas mÃ¡s usadas.
    

---

ğŸ”” **Recuerda**:

> Â¡El **Catalog** es tu centro de operaciones! AquÃ­ puedes gestionar todas las bases de datos y explorar datos compartidos. Usa **Recientes** y **Favoritos** para tener todo a mano. Â¡Explora, marca y navega fÃ¡cilmente! ğŸ§­

---

## ğŸ”§ **Pasos para crear un catÃ¡logo en Databricks:**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img205.png)

1. **Ir a la secciÃ³n "Catalog"**  
    En el panel lateral izquierdo del Workspace, haz clic en la opciÃ³n **Catalog**. Esta secciÃ³n te permite gestionar catÃ¡logos, esquemas y tablas de datos gobernados por Unity Catalog.
    
2. **Abrir el menÃº de acciones**  
    Haz clic en el Ã­cono de **engranaje** (âš™ï¸) o en el botÃ³n **"+" (Add data)** en la parte superior derecha del panel Catalog.
    
3. **Seleccionar "Create a catalog"**  
    En el menÃº desplegable que aparece, selecciona la opciÃ³n **Create a catalog** (Crear un catÃ¡logo). Esta acciÃ³n te permitirÃ¡ definir un nuevo catÃ¡logo de datos dentro del Metastore configurado previamente.

---

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img206.png)

Ya que **has configurado tu Metastore**, el **siguiente paso lÃ³gico** es crear la capa organizativa principal dentro del Unity Catalog.


Recordemos que un **Catalog** es el primer nivel jerÃ¡rquico del Unity Catalog:

```
Metastore â¡ï¸ Catalog â¡ï¸ Schema (Database) â¡ï¸ Table / View
```

---

ğŸªª 1ï¸âƒ£ **Catalog name** (obligatorio)

ğŸ“Œ Escribe el **nombre del catÃ¡logo** que estÃ¡s creando.  
ğŸ§  Usa nombres **significativos** segÃºn el proyecto, por ejemplo:

- `ventas_catalog`
    
- `clientes_data`
    
- `proyecto_iot_2025`
    

---

ğŸ§¬ 2ï¸âƒ£ **Type** (obligatorio)

ğŸ”½ Selecciona el tipo de catÃ¡logo. Hay dos opciones comunes:

|Tipo|DescripciÃ³n|
|---|---|
|**Standard**|Usa la configuraciÃ³n por defecto del Metastore. Ideal para la mayorÃ­a.|
|**Managed**|Solo aparece si tienes configuraciones especÃ­ficas para control total.|

ğŸ“Œ Selecciona **Standard** si no tienes requerimientos personalizados.

---

 â˜ï¸ 3ï¸âƒ£ **Storage location** (opcional pero recomendable)

ğŸ“¦ Esta secciÃ³n permite **especificar una ubicaciÃ³n externa** para guardar las **tablas gestionadas** del catÃ¡logo.

â• Opciones:

- **Select external location**: Escoge una ubicaciÃ³n externa ya registrada.
    
- **sub/path**: Especifica la **subcarpeta** dentro de ese storage (por ejemplo: `catalogos/ventas`).
    

ğŸ”— Si no tienes una ubicaciÃ³n, haz clic en:

ğŸ”§ **Create a new external location**  
Esto te llevarÃ¡ a un flujo para crearla vinculando un contenedor de ADLS Gen2 y un Access Connector.

---

ğŸ“Œ ğŸ“ Notas Importantes

- Si **no eliges una ubicaciÃ³n externa**, el catÃ¡logo usarÃ¡ la ubicaciÃ³n por defecto del **Metastore**.
    
- Esto puede ser suficiente porque el Metastore ya tiene un ADLS Gen2 configurado.
    

---

ğŸ§­ **Flujo** 

```plaintext
ğŸŸ¦ Metastore (ya creado)
    â””â”€â”€ ğŸ“ Catalog (tÃº lo creas aquÃ­)
           â””â”€â”€ ğŸ“‚ Schemas (bases de datos dentro del catalog)
                  â””â”€â”€ ğŸ“Š Tablas / ğŸ‘ï¸ Vistas
```

---

Finalmente solo debemos agregar el nombre al catalogo ya que el Type y el Storage location estÃ¡n configurados en el **Metastore**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img207.png)


ğŸ–±ï¸ Ãšltimo paso

Haz clic en **Create** ğŸ’™  
Â¡Y listo! ğŸ‰ Ya tendrÃ¡s tu **nuevo catÃ¡logo configurado**.


Ahora demos un vistazo en **Configure catalog**  

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img208.png)

---

### ğŸ” **ConfiguraciÃ³n de Acceso a un CatÃ¡logo en Databricks**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img209.png)

Este panel de configuraciÃ³n te permite gestionar **quiÃ©n puede ver, acceder o modificar** un catÃ¡logo de datos. Es una parte clave de la **gobernanza de datos** en **Databricks + Unity Catalog**.

---

ğŸ§­ **Acceso General**

 ğŸŒ **Workspaces con Acceso**

- âœ… **Todos los espacios de trabajo tienen acceso**  
    Esto significa que **cualquier workspace** dentro de la cuenta puede visualizar el catÃ¡logo. Ideal si trabajas con mÃºltiples equipos.
    

---

ğŸ‘¤ **Propietario del CatÃ¡logo**

- ğŸ‘‘ **Propietario Asignado**:  
    `[carlosvelasquez@zero2herox.onmicrosoft.com](mailto:carlosvelasquez@zero2herox.onmicrosoft.com)`  
    Este usuario tiene **control total** sobre el catÃ¡logo y puede:
    
    - Asignar permisos
        
    - Gestionar objetos
        
    - Modificar configuraciones de acceso
        

>ğŸ“ **NOTA**: Solo los propietarios o administradores pueden **delegar o revocar** privilegios. Esto tambiÃ©n lo puedes controlar desde el panel https://accounts.azuredatabricks.net/ donde puedes agregar y/o quitar usuarios y asignar permisos.

---

ğŸ” **Privilegios de Acceso**

|ğŸ§‘â€ğŸ¤â€ğŸ§‘ **Grupo/Usuario**|ğŸ¯ **Permiso Otorgado**|
|---|---|
|Todos los usuarios|`BROWSE` (Explorar)|

ğŸ” El permiso **BROWSE** permite:

- Ver el catÃ¡logo y sus objetos
    
- ğŸ”’ **No** permite editar, borrar ni crear objetos
    

---

ğŸ“¦ **Objetos del CatÃ¡logo**

- ğŸ“ **Nombre del catÃ¡logo**: `proyecto_databricks_catalog`
    
- Este es el catÃ¡logo especÃ­fico al que se le estÃ¡n aplicando estos privilegios.
    

---

ğŸ›ï¸ **Controles de AcciÃ³n**

ğŸ”˜ **Botones disponibles:**

|ğŸ› ï¸ AcciÃ³n|ğŸ“‹ DescripciÃ³n|
|---|---|
|âœ… **Conceder**|Asigna un nuevo privilegio a un usuario o grupo|
|âŒ **Revocar**|Quita un privilegio previamente otorgado|

ğŸ” Puedes usarlos para ajustar el acceso en tiempo real sin interrumpir el uso del catÃ¡logo.

---

ğŸ“Œ **Resumen Visual**

|Elemento|Estado / Valor|
|---|---|
|ğŸŒ Acceso Workspace|Todos los espacios de trabajo|
|ğŸ‘‘ Propietario|[carlosvelasquez@zero2herox.onmicrosoft.com](mailto:carlosvelasquez@zero2herox.onmicrosoft.com)|
|ğŸ“¦ CatÃ¡logo|`proyecto_databricks_catalog`|
|ğŸ‘¥ Permisos|Todos los usuarios â†’ `BROWSE`|
|ğŸ› ï¸ Acciones|Conceder / Revocar permisos|

---

âœ… **Checklist de Seguridad**

â˜‘ Â¿El propietario tiene control total?  
â˜‘ Â¿Los permisos estÃ¡n limitados a lo necesario (principio de menor privilegio)?  
â˜‘ Â¿Se otorgÃ³ acceso solo a usuarios o grupos verificados?  
â˜‘ Â¿Se documentaron los cambios de privilegios?

---

ğŸ“£ **KEY**:  
ğŸ’¡ Usa privilegios como **SELECT**, **MODIFY**, **CREATE**, y **USAGE** solo cuando sea necesario. Â¡Evita dar mÃ¡s permisos de los que cada rol necesita! ğŸ”’

---

 ğŸ” **Ventana "Grant" â€“ ConfiguraciÃ³n de Accesos en CatÃ¡logos de Databricks**

Al hacer clic en **"Grant"**, accedes a una ventana fundamental para la **gestiÃ³n de permisos** en tu catÃ¡logo de datos. AquÃ­ te mostramos quÃ© puedes hacer y por quÃ©, en algunos casos, no puedes modificar esta configuraciÃ³n.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img210.png)

---

ğŸ§© **Â¿QuÃ© es la Ventana "Grant"?**

La ventana **"Grant"** te permite:

âœ… **Asignar permisos especÃ­ficos**  
ğŸ‘¥ A **usuarios**, **grupos** o **principals**  
ğŸ” Controlar **quÃ© pueden hacer** con los objetos del catÃ¡logo

---

ğŸ“‹ **Opciones de ConfiguraciÃ³n Dentro de "Grant"**

1ï¸âƒ£ **Principals (Usuarios o Grupos)**

Selecciona a quiÃ©n deseas otorgar permisos:

- ğŸ‘¤ Usuarios individuales
    
- ğŸ‘¥ Grupos de usuarios
    
- ğŸ¤– Principals de servicio
    

---

 2ï¸âƒ£ **Privilege Presets (Preajustes)**

|ğŸ”§ Tipo|ğŸ“‹ DescripciÃ³n|
|---|---|
|ğŸ›ï¸ Custom|Configura permisos manualmente|
|ğŸ“¦ Preajustes|Usa plantillas rÃ¡pidas con privilegios comunes|

---

3ï¸âƒ£ **Permisos Disponibles**

|ğŸ§­ CategorÃ­a|ğŸ”‘ Permisos Incluidos|
|---|---|
|ğŸªœ **Prerequisitos**|`USE CATALOG`, `USE SCHEMA`|
|ğŸ“– **Lectura**|`EXECUTE`, `READ VOLUME`, `SELECT`|
|ğŸ—ï¸ **CreaciÃ³n**|`CREATE FUNCTION`, `CREATE SCHEMA`, `CREATE TABLE`, `CREATE VOLUME`, etc.|
|ğŸ§¾ **Metadatos**|`APPLY TAG`, `BROWSE`|
|âœï¸ **EdiciÃ³n**|`MODIFY`, `REFRESH`, `WRITE VOLUME`|

---

4ï¸âƒ£ **Opciones Avanzadas**

|âš™ï¸ OpciÃ³n|ğŸ“ DescripciÃ³n|
|---|---|
|`ALL PRIVILEGES`|Otorga todos los permisos disponibles|
|`EXTERNAL USE SCHEMA`|Permite acceder a objetos desde motores externos|
|`MANAGE`|Permisos similares a los del propietario (gestionar, eliminar, renombrar)|

---

5ï¸âƒ£ **Acciones Finales**

âœ… **Grant** â€“ Aplica los permisos seleccionados  
âŒ **Cancel** â€“ Descarta los cambios

---

ğŸš« Â¿Por QuÃ© No Puedes Modificar Esta ConfiguraciÃ³n?

Recuerda que no puedes usar el botÃ³n **"Grant"**, puede deberse a uno (o mÃ¡s) de los siguientes motivos:

|âš ï¸ **RazÃ³n**|ğŸ’¬ **DescripciÃ³n**|
|---|---|
|ğŸ”’ **No eres el propietario**|Solo los propietarios pueden editar permisos del catÃ¡logo.|
|ğŸ‘‘ **No tienes rol de administrador**|Algunos permisos requieren ser **Workspace Admin** o **Metastore Admin**.|
|ğŸ›¡ï¸ **Restricciones organizacionales**|La polÃ­tica de tu empresa puede bloquear cambios en catÃ¡logos.|
|ğŸ”„ **Permisos gestionados automÃ¡ticamente**|El catÃ¡logo puede heredar permisos desde otra fuente (como polÃ­ticas globales).|
|ğŸ **Error tÃ©cnico o visual**|Puede ser un bug temporal. Intenta recargar la pÃ¡gina o cerrar sesiÃ³n.|


---

Finalmente haz clic en Next y Save

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img211.png)


### ğŸ“¦**External Location**

Aunque ya configuraste correctamente el contenedor `metastore` en el Unity Catalog de **Databricks**, **esto no da acceso automÃ¡tico** a otros contenedores del mismo Data Lake, como `bronze`. Cada contenedor requiere su propia configuraciÃ³n de **credenciales** y **ubicaciones externas (external locations)** para poder ser accedido desde Databricks.

Pero antes de proceder a crear un **External Location** vamos a crear una nueva credencial que reutilizaremos en el contenedor Bronze y el resto de contenedores que usaremos mÃ¡s adelante. 

**Haz clic en â• y luego en Create a credential**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img212.png)

Ahora simplemente le asignamos un nombre y copiamos el **Resource ID** en En el campo **Access Connector Id** y clic en **Create**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img195.png)


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img213.png)


### **Acceder desde Databricks (External Location)**:

- En **Databricks**, ve a la secciÃ³n de **"Catalog"** como ya vimos anteriormente.

- Selecciona **"Create an external location"**.
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img162.png)

- Completa los siguientes campos:

- **External location name**: Un nombre representativo (ej. `bronze_external_location`).
	
- **Storage type**: **Azure Data Lake Storage**.
	
- **URL**: Usa la URL del contenedor que bronze:
	
	```
	abfss://bronze@z2hdatalakesuffix.dfs.core.windows.net/
	```
	
- Finalmente seleccionamos la credencial que reciÃ©n creamos `my-credential`
	
	![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img164.png)
	
- Finalmente clic en **Create**
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img214.png)

Tambien puedes probar la conexiÃ³n en **Test conection**

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img215.png)

## ğŸŒŸ **Workspace en Databricks** 

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img172.png)

Databricks tiene una interfaz muy completa para gestionar tus proyectos y recursos. En la pestaÃ±a _Workspace_, puedes organizar tu trabajo y colaborar con otros usuarios.

---

### ğŸš€ **1. NavegaciÃ³n de Carpetas** ğŸ“‚

La **navegaciÃ³n de carpetas** en la parte izquierda es tu mapa de recursos. AsÃ­ puedes encontrar rÃ¡pidamente todo lo que necesitas:

|**SecciÃ³n**|**DescripciÃ³n**|
|:-:|:--|
|ğŸ  **Home**|AquÃ­ estÃ¡n tus proyectos y archivos personales.|
|ğŸ“š **Repos**|Tus proyectos y repositorios de cÃ³digo (como los de Git).|
|ğŸ”„ **Shared**|Carpeta donde puedes compartir archivos con otros usuarios.|
|ğŸ‘¤ **Users**|Cada usuario tiene su propia carpeta aquÃ­.|
|â­ **Favorites**|Marca archivos importantes o Ãºtiles como favoritos.|
|ğŸ—‘ï¸ **Trash**|Todos los archivos eliminados van aquÃ­.|

---

### ğŸ“Š **2. Vista de Usuario** ğŸ‘¨â€ğŸ’»ğŸ‘©â€ğŸ’»

La vista central muestra los archivos de un **usuario especÃ­fico** (en este caso, el de "[carlosvelasquez@zero2herox.onmicrosoft.com](mailto:carlosvelasquez@zero2herox.onmicrosoft.com)").

ğŸ“ **NOTA**: Si la carpeta estÃ¡ vacÃ­a, verÃ¡s el mensaje "**This folder is empty**".

---

### ğŸ› ï¸ **3. Opciones de Filtro y BÃºsqueda** ğŸ”

Puedes organizar tus archivos usando filtros o buscando por:

ğŸ”¹ **Nombre**  
ğŸ”¹ **Tipo**  
ğŸ”¹ **Propietario**  
ğŸ”¹ **Fecha de CreaciÃ³n**

---

### ğŸ’¬ **4. Botones de AcciÃ³n** âš™ï¸

### âœ¨ **Acciones Principales**:

|**AcciÃ³n**|**DescripciÃ³n**|
|:-:|:--|
|â• **Crear**|Crea nuevos proyectos, notebooks o carpetas.|
|ğŸ”— **Compartir**|Comparte archivos o recursos con otros usuarios.|
|âš™ï¸ **MÃ¡s Opciones**|Ajustes adicionales como enviar feedback y configuraciÃ³n.|

---

### ğŸ—‚ï¸ **5. Otras Secciones en el MenÃº Lateral** ğŸ”§

En el menÃº lateral tambiÃ©n puedes encontrar otros recursos sÃºper Ãºtiles para tu trabajo:

|**SecciÃ³n**|**DescripciÃ³n**|
|:-:|:--|
|âš™ï¸ **Jobs & Pipelines**|Automatiza tareas con trabajos y pipelines.|
|ğŸ’» **Compute**|Accede a los clÃºsteres y recursos de computaciÃ³n.|
|ğŸ“Š **SQL**|Editor de SQL, consultas, y SQL Warehouses.|
|ğŸ›’ **Marketplace**|Descubre e integra aplicaciones y servicios adicionales.|

---

âœ… **Resumen de Funciones Clave**:

ğŸ”¹ **Espacio personal**: Guarda tus proyectos en tu carpeta "Home".  
ğŸ”¹ **ColaboraciÃ³n**: Comparte recursos y colabora con otros usuarios.  
ğŸ”¹ **AutomatizaciÃ³n**: Utiliza jobs y pipelines para agilizar procesos.  
ğŸ”¹ **SQL**: Gestiona y consulta bases de datos fÃ¡cilmente.  
ğŸ”¹ **ComputaciÃ³n**: Accede a recursos de clÃºsteres para procesamiento de datos.

---

ğŸ¯ **Consejos para Maximizar tu Productividad** ğŸ’¡

- ğŸ’¼ **Organiza tu trabajo**: Crea carpetas y subcarpetas para mantener tus proyectos ordenados.
    
- ğŸ’¬ **Colabora fÃ¡cilmente**: Utiliza la funciÃ³n de compartir para que otros usuarios puedan ver o editar tus archivos.
    
- ğŸš€ **Usa filtros**: Encuentra rÃ¡pidamente lo que necesitas aplicando filtros en el nombre, tipo o propietario de los archivos.

---
## ğŸ›  **Â¿CÃ³mo crear un notebook correctamente?**

1. **Ir a tu carpeta personal**  
    En Databricks â†’ _Workspace_ â†’ _Users_ â†’ _tu usuario_. (Evitas mezclar con recursos compartidos).
    
2. **Crear notebook**
    
    - Haz clic derecho sobre tu **Usuario** luego en **Create** â†’ seleccionar **Notebook**. 
     ![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img173.png)
    
1. **Configurar lenguaje predeterminado**  
    El notebook tomarÃ¡ por defecto Python
    
2. **Adjuntar compute**  
    Al crearlo, normalmente ya se adjunta automÃ¡ticamente al compute que usaste mÃ¡s recientemente. Si no tienes cluster o compute creado, tendrÃ¡s que crearlo antes de ejecutar celdas. 
    
3. **Activar caracterÃ­sticas Ãºtiles del editor**
    
    - **Tabs para notebooks y archivos**: en _Settings > Developer_ puedes activar la experiencia de pestaÃ±as para navegar entre mÃºltiples notebooks y archivos. 
        
    - **Split view**, **minimap**, bÃºsqueda avanzada, entre otras, son ya parte de la experiencia mejorada.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img216.png)

---

## âš¡ï¸ Â¿QuÃ© es â€œServerless Computeâ€ en **Azure Databricks**?

Al hacer clic en la pestaÃ±a **Connect**, encontrarÃ¡s dos opciones principales para conectar tu notebook:

* âœ… **Serverless**
* ğŸ› ï¸ **Tu clÃºster personalizado** (como el que creaste en pasos anteriores)

> Ahora bien, Â¿**cuÃ¡l deberÃ­as usar**?
> Â¡Buena pregunta! Vamos a resolverla antes de seguir.

---

### ğŸ‘€ Vistas en Databricks

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img174.png)

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img175.png)

AquÃ­ puedes ver tanto el **Serverless compute** como el clÃºster **"Z2h cluster..."** que creaste anteriormente. Ambos estÃ¡n disponibles como opciones para tu notebook.

---

## â˜ï¸ **Â¿QuÃ© es Serverless Compute?**

Una forma de ejecutar notebooks sin tener que crear, iniciar o administrar clÃºsteres manualmente.

---

### ğŸ”‘ **CaracterÃ­sticas Clave**

* âš™ï¸ **Sin aprovisionamiento manual**: No tienes que configurar nada, Databricks se encarga de todo.
* âš¡ **Inicio rÃ¡pido**: Los notebooks se conectan mÃ¡s rÃ¡pido a Serverless que a un clÃºster tradicional.
* ğŸ” **Escalabilidad automÃ¡tica**: Los recursos se asignan segÃºn demanda.
* ğŸ§¼ **Ambiente limpio**: Cada ejecuciÃ³n se da en un entorno aislado.

---

### ğŸ§© **Requisitos para poder usarlo**

| âœ… Requisito                   | ğŸ“Œ Detalle                                                            |
| ----------------------------- | --------------------------------------------------------------------- |
| ğŸ” Unity Catalog habilitado   | Necesario para gobernanza unificada                                   |
| ğŸŒ RegiÃ³n compatible de Azure | No todas las regiones lo soportan actualmente                         |
| ğŸ”§ Funcionalidad activada     | Debe estar habilitada en los ajustes de cuenta (â€œFeature enablementâ€) |

ğŸ“ **NOTA**: Si no ves la opciÃ³n â€œServerlessâ€, verifica estos 3 puntos.

---

### ğŸ“Š **Ventajas Operativas**

* âœ… Menor mantenimiento de clÃºsteres
* âš™ï¸ No mÃ¡s configuraciÃ³n de Spark avanzada (ideal para usuarios no tÃ©cnicos)
* ğŸ“ˆ Escala sola, sin intervenciÃ³n manual
* ğŸ•’ Ahorro de tiempo al no tener que iniciar clÃºsteres

---

### âš ï¸ **Limitaciones del Serverless Compute**

| ğŸš« LimitaciÃ³n                  | ğŸ” ExplicaciÃ³n                                                              |
| ------------------------------ | --------------------------------------------------------------------------- |
| ğŸ”§ Sin Spark configs avanzados | No puedes personalizar la configuraciÃ³n de Spark                            |
| ğŸ“œ No soporta init scripts     | No puedes ejecutar scripts automÃ¡ticos al iniciar el entorno                |
| â±ï¸ LÃ­mites de inactividad      | Puede cerrarse si estÃ¡ inactivo por un perÃ­odo de tiempo                    |
| ğŸ’¾ Recursos limitados          | Memoria y cÃ³mputo disponibles pueden ser menores que en un clÃºster dedicado |

ğŸ“ **Ideal para**: notebooks interactivos, anÃ¡lisis ligeros, pruebas, demos, exploraciÃ³n de datos.

---

## âš™ï¸ **â€œMi Clusterâ€ vs â€œServerless Computeâ€**

### Tu escenario actual:

Tenemos el cluster que creamos en pasos enteriores:

* ğŸ› ï¸ Un clÃºster propio: `"Z2h cluster 2025â€‘09â€‘11 ..."`
* âš¡ La opciÃ³n de usar **Serverless**

Entonces... Â¿cuÃ¡l usar?

---

### ğŸ†š ComparaciÃ³n

| ğŸ’¡ CaracterÃ­stica        | âš¡ Serverless               | ğŸ› ï¸ ClÃºster Personalizado      |
| ------------------------ | -------------------------- | ------------------------------ |
| ConfiguraciÃ³n manual     | âŒ No                       | âœ… SÃ­                           |
| Tiempo de inicio         | âš¡ Muy rÃ¡pido               | ğŸ•“ Puede tardar varios minutos |
| Escalabilidad            | âœ… AutomÃ¡tica               | âš ï¸ Manual                      |
| Control de Spark configs | âŒ No disponible            | âœ… Completo                     |
| Uso de init scripts      | âŒ No soportado             | âœ… Soportado                    |
| Ideal para               | Pruebas, notebooks simples | Procesos complejos, producciÃ³n |
| GestiÃ³n de recursos      | ğŸ”„ DinÃ¡mico                | ğŸ§‘â€ğŸ’» Bajo tu control directo  |

---

### ğŸ¯ **Â¿CuÃ¡l deberÃ­as elegir para tu notebook?**

| ğŸ“Œ Escenario                                           | âœ… RecomendaciÃ³n           |
| ------------------------------------------------------ | ------------------------- |
| ğŸ“Š AnÃ¡lisis exploratorio rÃ¡pido o presentaciÃ³n         | **Serverless**            |
| ğŸ§ª Desarrollo inicial de modelos                       | **Serverless**            |
| ğŸ› ï¸ ETLs complejas o procesos en producciÃ³n            | **ClÃºster Personalizado** |
| ğŸ”§ Necesitas Spark configs o scripts de inicializaciÃ³n | **ClÃºster Personalizado** |

---

### âœ… **ConclusiÃ³n**

> Usaremos **Serverless Compute** ya que nos brinda facilidad, rapidez y menos mantenimiento.
> Usaremos el **clÃºster personalizado** en proyectos futuros

ğŸ§  **RecomendaciÃ³n**:
Ahora por temas de costos comenzaremos con **Serverless** para notebooks pequeÃ±os o pruebas, y migraremos a un clÃºster para trabajos mÃ¡s avanzados o crÃ­ticos mÃ¡s adelante.

---

## âš™ï¸ **CÃ³mo acceder desde tu notebook al Lake Bronze** ğŸŒŠ

Si ya tienes configurado lo necesario (workspace Premium, Unity Catalog habilitado, Managed Identity con permisos correctos, ADLS Gen2 con HNS y Metastore configurado), ahora es el momento de acceder a tu **Lake Bronze** desde un notebook en **Databricks**.

---

### ğŸ“‘ **Pasos Conceptuales para Leer Datos desde tu Notebook**

Antes de profundizar en los detalles del cÃ³digo, entendamos el **flujo general**:

---

ğŸ”Œ **Usando Spark para Leer desde ADLS Gen2**

1. **Acceder a un archivo Parquet**  
    El mÃ©todo mÃ¡s comÃºn para leer datos desde tu **Lake Bronze** es usar Spark con el formato adecuado.
    
    AquÃ­ estÃ¡ la idea conceptual de cÃ³mo serÃ­a el cÃ³digo (Â¡sin copiar y pegar directamente, solo para entender el flujo!):
    
    > `spark.read.format("parquet").load("<ruta usando abfss://...>")`
    
    Esto leerÃ­a el archivo en el contenedor de **Azure Data Lake Storage Gen2 (ADLS Gen2)**, siempre y cuando la **External Location** estÃ© correctamente configurada con permisos adecuados.
    
    ğŸ“ **Nota**: Esta operaciÃ³n solo funcionarÃ¡ si tienes los permisos correctos sobre la _External Location_.
    

---

ğŸ”„ **Alternativa: Usando Tablas Externas Registradas en Unity Catalog**

2. **Acceder a Datos a TravÃ©s de Tablas Externas**  
    Si registraste previamente una tabla externa en **Unity Catalog** que apunte a ese contenedor de **Lake Bronze**, el proceso es aÃºn mÃ¡s sencillo.
    
    AquÃ­ el flujo serÃ­a algo como:
    
    > `SELECT * FROM <catalog>.<schema>.<tabla_externa>`
    
    En este caso, **Databricks** se encargarÃ¡ de gestionar la bÃºsqueda de archivos en **ADLS Gen2** y hacer todo el trabajo por ti.
    

---

ğŸ§© **Resumen del Flujo**

1. **ConexiÃ³n a ADLS Gen2 usando Spark**
    
    - Usas **`spark.read.format("parquet")`** con la ruta correcta.
        
    - **Requiere permisos en la External Location.**
        
2. **Acceder mediante tablas registradas en Unity Catalog**
    
    - Usas **`SELECT`** en una tabla externa.
        
    - **Databricks encuentra los archivos por ti**.
        

---

ğŸš€ **Â¿QuÃ© Necesitas Para Que Esto Funcione?**

|ğŸ”‘ **Requisito**|ğŸ“ **DescripciÃ³n**|
|---|---|
|ğŸŒ **Workspace Premium**|Requiere tener un **workspace Premium** activo.|
|ğŸ” **Unity Catalog habilitado**|Debes tener **Unity Catalog** activado y configurado.|
|ğŸ› ï¸ **Managed Identity**|Configura una **Managed Identity** con permisos correctos.|
|ğŸ“¦ **ADLS Gen2 con HNS**|**ADLS Gen2** debe estar habilitado con **Hierarchical Namespace (HNS)**.|
|ğŸ—‚ï¸ **Metastore configurado**|El **Metastore** debe estar configurado y listo.|

---

### ğŸ§ **Â¿Por quÃ© usar tablas registradas en Unity Catalog?**

Registrar tablas externas en **Unity Catalog** es una prÃ¡ctica recomendada ya que:

- **Automatiza la bÃºsqueda de archivos**: No tienes que gestionar las rutas manualmente.
    
- **Mejora la seguridad y el control de acceso**: Puedes aplicar permisos mÃ¡s especÃ­ficos a nivel de catalogo, esquema y tabla.
    
- **Facilita el manejo de datos**: Hace que el trabajo con datos sea mÃ¡s organizado y accesible.

---

### ğŸš€ **CÃ³mo acceder a archivos `.parquet` desde un notebook**

Ya configuraste correctamente tu entorno, asÃ­ que es hora de **validarlo con pruebas prÃ¡cticas.**

---

ğŸ§ª **1. Leer el archivo `.parquet` directamente desde ADLS con Pyspark**

```python
%python
spark.read.format("parquet").load("abfss://bronze@z2hdatalakesuffix.dfs.core.windows.net/SalesLT/Address/Address.parquet").show(10)
```

ğŸ” **ExplicaciÃ³n del cÃ³digo:**

| LÃ­nea de cÃ³digo                | Â¿QuÃ© hace?                                                                                                                                        |
| ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `%python`                      | Indica que el bloque de cÃ³digo se ejecutarÃ¡ en Python.                                                                                            |
| `spark.read.format("parquet")` | Usa Spark para leer datos en formato **Parquet**.                                                                                                 |
| `.load("abfss://...")`         | Carga el archivo desde **ADLS Gen2** usando el esquema `abfss://`, que es tÃ­pico para Data Lakes con autenticaciÃ³n mediante **Managed Identity**. |
| `.show(10)`                    | Muestra los **primeros 10 registros** en el notebook. Ideal para validar lectura.                                                                 |

ğŸ“¸ Resultado esperado:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img217.png)

âœ… Si todo estÃ¡ bien configurado (External Location, permisos, Unity Catalog), verÃ¡s los datos del archivo Parquet.

---

ğŸ§± **2. Crear un esquema y tabla externa en Unity Catalog con SQL**

```sql
-- Create schema within the valid catalog
CREATE SCHEMA IF NOT EXISTS proyecto_databricks_catalog.bronze_schema;
```

ğŸ” **Â¿QuÃ© hace este bloque?**

| Componente                                  | ExplicaciÃ³n                                                                   |
| ------------------------------------------- | ----------------------------------------------------------------------------- |
| `CREATE SCHEMA`                             | Crea un **nuevo esquema (namespace)** dentro del catÃ¡logo.                    |
| `IF NOT EXISTS`                             | Previene errores si el esquema ya existe.                                     |
| `proyecto_databricks_catalog.bronze_schema` | El esquema se crea dentro del catÃ¡logo llamado `proyecto_databricks_catalog`. |

ğŸ“ Este paso organiza tus objetos en Unity Catalog y te permite tener una estructura como:
**`<catalog>.<schema>.<table>`**

---

ğŸ—‚ï¸ Crear la tabla externa

```sql
-- Create external table (parquet in bronze container)
CREATE TABLE IF NOT EXISTS proyecto_databricks_catalog.bronze_schema.address
USING PARQUET
LOCATION 'abfss://bronze@z2hdatalakesuffix.dfs.core.windows.net/SalesLT/Address/Address.parquet';
```

ğŸ” **Â¿QuÃ© hace este bloque?**

| LÃ­nea                                               | ExplicaciÃ³n                                                                      |
| --------------------------------------------------- | -------------------------------------------------------------------------------- |
| `CREATE TABLE IF NOT EXISTS`                        | Crea una nueva tabla externa solo si no existe aÃºn.                              |
| `proyecto_databricks_catalog.bronze_schema.address` | Define el nombre completo de la tabla (`address`) dentro del esquema y catÃ¡logo. |
| `USING PARQUET`                                     | Indica que los datos estÃ¡n almacenados en formato **Parquet**.                   |
| `LOCATION 'abfss://...'`                            | Ruta absoluta al archivo en **ADLS Gen2** (Lake Bronze).                         |

ğŸ“¸ Vista esperada en notebook:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img218.png)

---

ğŸ” **3. Verificar tabla en Unity Catalog**

Una vez creada la tabla, puedes ir a la pestaÃ±a **Catalog** en el panel izquierdo y navegar:

* CatÃ¡logo: `proyecto_databricks_catalog`
* Esquema: `bronze_schema`
* Tabla: `address`

ğŸ“¸ Ejemplo visual:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img219.png)

âœ… Si ves esta estructura, todo fue creado correctamente.

---

ğŸ“Š **4. Consultar la tabla con SQL**

```sql
SELECT * FROM proyecto_databricks_catalog.bronze_schema.address LIMIT 10;
```

ğŸ” **Â¿QuÃ© hace?**

| Elemento                                                 | ExplicaciÃ³n                                                                                |
| -------------------------------------------------------- | ------------------------------------------------------------------------------------------ |
| `SELECT *`                                               | Recupera **todas las columnas** de la tabla.                                               |
| `FROM proyecto_databricks_catalog.bronze_schema.address` | Indica exactamente la tabla completa que quieres consultar, incluyendo catÃ¡logo y esquema. |
| `LIMIT 10`                                               | Muestra solo los **primeros 10 registros** para evitar sobrecargar el notebook.            |
ğŸ“¸ Vista esperada en notebook:

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img220.png)

âœ… Esto confirma que Unity Catalog puede acceder y consultar correctamente el archivo `.parquet`.

---

ğŸ§  **Resumen final**

| ğŸ”¢ Paso | ğŸ› ï¸ AcciÃ³n                        | âœ… Resultado Esperado                                    |
| ------- | --------------------------------- | ------------------------------------------------------- |
| 1ï¸âƒ£     | Leer `.parquet` directo con Spark | ValidaciÃ³n bÃ¡sica de acceso al Lake Bronze              |
| 2ï¸âƒ£     | Crear esquema en Unity Catalog    | OrganizaciÃ³n para tus objetos de datos                  |
| 3ï¸âƒ£     | Registrar tabla externa           | Tabla visible en el catÃ¡logo con datos enlazados a ADLS |
| 4ï¸âƒ£     | Ejecutar `SELECT` SQL             | Verifica lectura vÃ­a Unity Catalog                      |


## **Â¿QuÃ© son las Capas Bronze, Silver y Gold?** ğŸ§

Antes de continuar, ya hemos creado el contenedor **Bronze**, donde almacenamos los datos crudos tal como llegan desde las fuentes de origen. Estos datos pueden estar desordenados o incompletos, pero nos permiten tener un historial completo y confiable.

 **1ï¸âƒ£ Contenedor **Bronze**:**

- **Â¿QuÃ© tiene?**: Datos crudos, tal cual llegan desde las fuentes. Pueden estar desordenados o incompletos.
    
- **Procesamiento**: No se realiza ningÃºn procesamiento, solo se almacenan tal como estÃ¡n.
    
- **Â¿Para quÃ©?**: Para almacenar todo el historial de datos sin modificaciones, garantizando una base confiable de origen.
    

---

**2ï¸âƒ£ Contenedor **Silver**:**

- **Â¿QuÃ© tiene?**: Datos parcialmente limpios, transformados y enriquecidos.
    
- **Procesamiento**:
    
    - Se eliminan duplicados
        
    - Se validan valores nulos
        
    - Se realizan transformaciones bÃ¡sicas
        
- **Â¿Para quÃ©?**: Perfecto para anÃ¡lisis intermedios o operativos, donde los datos estÃ¡n lo suficientemente procesados para ser Ãºtiles.
    

---

**3ï¸âƒ£ Contenedor **Gold**:**

- **Â¿QuÃ© tiene?**: Datos completamente refinados, agregados y listos para su consumo final.
    
- **Procesamiento**:
    
    - OptimizaciÃ³n total
        
    - Datos bien estructurados y organizados
        
- **Â¿Para quÃ©?**: Para informes estratÃ©gicos, dashboards, y modelos de machine learning, proporcionando insights clave para la toma de decisiones.
    

---

**El flujo de los datos** ğŸ”„

1ï¸âƒ£ **Bronze** â¡ï¸ 2ï¸âƒ£ **Silver** â¡ï¸ 3ï¸âƒ£ **Gold**

Cada capa garantiza que los datos sean procesados y transformados en su versiÃ³n mÃ¡s Ãºtil y Ã³ptima para el anÃ¡lisis y las decisiones empresariales. Â¡El flujo de trabajo va de lo mÃ¡s crudo a lo mÃ¡s refinado! 

---

### **Pasos para crear los contenedores Silver y Gold** ğŸ› ï¸

Para continuar con la creaciÃ³n de los contenedores, sigue los mismos pasos que usaste para crear **Bronze**. 

---

**CreaciÃ³n de los contenedores Silver y Gold** ğŸ“‚

1ï¸âƒ£ **DirÃ­gete a tu Storage**: ğŸ“ `z2hdatalakesuffix`
2ï¸âƒ£ **Crea los contenedores Silver y Gold**: Solo repite los pasos de creaciÃ³n de contenedores que usaste para **Bronze**.
3ï¸âƒ£ **Â¡Listo para avanzar!** ğŸ’¡

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img221.png)

**Nota**: AsegÃºrate de que todo estÃ© bien configurado para la correcta creaciÃ³n de los contenedores.

---

### **External Locations para Silver y Gold** ğŸŒ

Â¡Ahora configuramos las ubicaciones externas! AquÃ­ estÃ¡n los detalles para Silver y Gold. De nuevo, sigue los pasos previos para **Bronze**, solo ajustando los nombres y URLs correspondientes.

---

#### **External Location Silver** ğŸ’

* **External Location Name**: silver\_external\_location
* **URL**:

  ```
  abfss://silver@z2hdatalakesuffix.dfs.core.windows.net/
  ```
* **Storage Credential**: my-credential

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img222.png)

---

#### **External Location Gold** ğŸ†

* **External Location Name**: gold\_external\_location
* **URL**:

  ```
  abfss://gold@z2hdatalakesuffix.dfs.core.windows.net/
  ```
* **Storage Credential**: my-credential

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img223.png)

---

âœ… **Â¡Recuerda!**

* **Pasos similares** para la creaciÃ³n de **External Locations**.
* ğŸ’¡ **Consejo**: Verifica que las credenciales estÃ©n bien configuradas para ambos entornos.
* ğŸ“ **Â¡Todo a su tiempo!**: Una vez creados estos, estarÃ¡s listo para continuar con los anÃ¡lisis.

---


## ğŸ’¿âœ¨ **Capa Silver - Transformaciones** 

> ğŸ”„ Â¡De _bronze_ a _silver_! Ahora que ya tienes tus contenedores **Silver** y **Gold** listos en tu **Data Lake**, y los **External Locations** bien definidos en **Databricks**, es hora de meterle mano a los datos ğŸ§ .

---

### ğŸ”§ Â¿QuÃ© vamos a hacer?

Transformaremos archivos **Parquet** ubicados en la capa **Bronze** y los guardaremos como archivos **Delta** en la capa **Silver**, corrigiendo el formato de columnas **fecha** â³.

>NOTA: Aunque esta transformaciÃ³n es bÃ¡sica, puedes adaptar la lÃ³gica a los requisitos de futuros proyectos

---

### âœ… **Pasos del Script de TransformaciÃ³n**

|NÂº|AcciÃ³n|Estado|
|---|---|---|
|1ï¸âƒ£|Inicializar una lista vacÃ­a de tablas|â˜‘ï¸ Completado|
|2ï¸âƒ£|Listar los directorios (tablas) en el contenedor _bronze_|â˜‘ï¸ Completado|
|3ï¸âƒ£|Importar funciones necesarias de PySpark|â˜‘ï¸ Completado|
|4ï¸âƒ£|ğŸ” Para cada tabla:|En progreso|
||â¤ Leer archivo Parquet correspondiente|â˜‘ï¸|
||â¤ Detectar columnas con fechas y convertirlas|â˜‘ï¸|
||â¤ Guardar como Delta Table en _silver_|â˜‘ï¸|

---

### ğŸ‘€ **Ejemplo real**

ğŸ“‚ Revisemos una de las tablas en **Dbeaver** donde podrÃ¡s ver que contiene columna `ModifiedDate` con valores como:

```
2006-07-01 00:00:00.000
2007-04-01 00:00:00.000
...
```

ğŸ“Œ Esta columna estÃ¡ en formato `timestamp`, incluye:

- ğŸ“… Fecha â†’ `yyyy-MM-dd`
    
- â° Hora â†’ `00:00:00.000` (inÃºtil en este contexto)
    
![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img224.png)

---

### â“ Â¿Por quÃ© transformar `timestamp` â†’ `date`?

> ğŸ’¡ Convertir la columna de fecha es clave para tener datos mÃ¡s limpios, eficientes y fÃ¡ciles de consumir.

---

ğŸ›  **Razones de la transformaciÃ³n**

|ğŸ” Motivo|âœ… Detalle|
|---|---|
|1ï¸âƒ£ Formato mÃ¡s limpio|Se elimina la hora que **no aporta valor** (`00:00:00.000`)|
|2ï¸âƒ£ ConversiÃ³n de zona horaria|Usa `from_utc_timestamp(..., "UTC")` si aplica zona horaria|
|3ï¸âƒ£ Mejora compatibilidad|Herramientas como Power BI y Tableau **leen mejor fechas simples**|
|4ï¸âƒ£ EstandarizaciÃ³n medallion|Silver = datos estandarizados âœ¨|

---

### âš ï¸ Â¿QuÃ© pasa si **NO** haces la transformaciÃ³n?

ğŸš« Riesgos de mantener el `timestamp`:

- MÃ¡s espacio de almacenamiento ğŸ’¾
    
- Usuarios confusos al ver `00:00:00.000` ğŸ§â€â™‚ï¸â“
    
- Agrupamientos incorrectos (`group by ModifiedDate` â¤ incluye la hora ğŸ˜¨)
    

---

ğŸ“ NOTA IMPORTANTE

> ğŸ“Œ Siempre que migres datos a una capa **Silver**, pregÃºntate:
> 
> **Â¿Estoy dejando solo lo esencial? Â¿Estoy estandarizando bien los datos?**

âœ”ï¸ Menos es mÃ¡s â†’ una fecha limpia es **mÃ¡s Ãºtil, clara y eficiente**.

---

ğŸ¯ ConclusiÃ³n Final

ğŸ§¹ Transformar la columna `ModifiedDate` te permite:

- âœ… Limpiar el dato innecesario (hora)
    
- âœ… Estandarizar la estructura
    
- âœ… Mejorar interoperabilidad con otras herramientas
    
- âœ… Evitar errores de anÃ¡lisis y agrupamiento
    

> **Una buena capa Silver es la base de un anÃ¡lisis de datos exitoso.** ğŸš€


## âš™ï¸âœ¨ Script de TransformaciÃ³n a la Capa Silver

ğŸ“ _De archivos Parquet (ğŸŸ¤ Bronze) a tablas Delta (ğŸŸ¡ Silver)_

---

### ğŸ§¾ **Objetivo General**

> Transformar archivos **Parquet** almacenados en el contenedor **ğŸŸ¤ Bronze**, aplicando limpieza y formateo de fechas, y guardar los resultados como **ğŸŸ¡ tablas Delta** en el contenedor **Silver**.

ğŸ”§ ConversiÃ³n de columnas tipo `timestamp` â†’ `yyyy-MM-dd`.

---

### ğŸ§  Script Completo en PySpark

```python
# 1ï¸âƒ£ Inicializar lista vacÃ­a de nombres de tabla
table_name = []

# 2ï¸âƒ£ Obtener nombres de carpetas (tablas) en bronze
for i in dbutils.fs.ls("abfss://bronze@z2hdatalakesuffix.dfs.core.windows.net/SalesLT/"):
    if i.isDir():
        table_name.append(i.name.split('/')[0])
        
# Listamos la tablas
table_name

# 3ï¸âƒ£ Importar funciones necesarias
from pyspark.sql.functions import from_utc_timestamp, date_format
from pyspark.sql.types import TimestampType

# 4ï¸âƒ£ Procesar cada tabla
for i in table_name:
    path = f"abfss://bronze@z2hdatalakesuffix.dfs.core.windows.net/SalesLT/{i}/{i}.parquet"
    df = spark.read.format('parquet').load(path)
    columns = df.columns

    # ğŸ”„ Convertir columnas tipo fecha
    for col in columns:
        if "Date" in col or "date" in col:
            df = df.withColumn(
                col,
                date_format(
                    from_utc_timestamp(df[col].cast(TimestampType()), "UTC"),
                    "yyyy-MM-dd"
                )
            )

    # ğŸ’¾ Guardar como tabla Delta en silver
    output_path = f"abfss://silver@z2hdatalakesuffix.dfs.core.windows.net/SalesLT/{i}/"
    df.write.format("delta").mode("overwrite").save(output_path)
```

---

### âœ… **ExplicaciÃ³n Paso a Paso**


|ğŸ§© Paso|AcciÃ³n|Detalle|
|---|---|---|
|1ï¸âƒ£|InicializaciÃ³n|Se crea una lista vacÃ­a para almacenar los nombres de tablas|
|2ï¸âƒ£|Lectura en Bronze|Se listan carpetas en el contenedor _Bronze_|
|3ï¸âƒ£|Imports|Se importan funciones para convertir timestamps|
|4ï¸âƒ£|Loop por tabla|Se procesan una por una: lectura âœ transformaciÃ³n âœ guardado|
|5ï¸âƒ£|Lectura de Parquet|Carga del `.parquet` a un DataFrame|
|6ï¸âƒ£|ConversiÃ³n de fechas|Columnas con `Date`/`date` â†’ `yyyy-MM-dd`|
|7ï¸âƒ£|Guardado en Silver|Se guarda el DataFrame como tabla Delta en modo `overwrite`|

---

âœ… 1. InicializaciÃ³n de una lista vacÃ­a para nombres de tablas

```python
table_name = []
```

Se crea una lista vacÃ­a `table_name` para almacenar los nombres de las tablas a procesar.

---

âœ… 2. RevisiÃ³n del contenedor *bronze* para identificar tablas

```python
for i in dbutils.fs.ls("abfss://bronze@.../SalesLT/"):
    if i.isDir():
        table_name.append(i.name.split('/')[0])
```

ğŸ“Œ Se accede al directorio `SalesLT` del contenedor *bronze* y se recorre su contenido:

* Solo se agregan los **directorios (tablas)**.
* Se eliminan las barras finales (`/`) usando `split`.

âœ”Listamos las **tablas**

```python
table_name
```

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img226.png)

---

âœ… 3. ImportaciÃ³n de funciones de PySpark

```python
from pyspark.sql.functions import from_utc_timestamp, date_format
from pyspark.sql.types import TimestampType
```

ğŸ”§ Estas funciones permiten:

- Convertir de UTC
    
- Dar formato a fechas
    
- Hacer casting a `TimestampType`
    

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img228.png)

---

âœ… 4. Procesamiento de cada tabla identificada

ğŸ“¸ **Ejemplo visual*

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img229.png)

```python
for i in table_name:
```

ğŸ” Se inicia un bucle para procesar cada tabla individualmente.

---

âœ… 5. Lectura del archivo Parquet correspondiente

```python
path = f".../{i}/{i}.parquet"
df = spark.read.format('parquet').load(path)
```

ğŸ“‚ Se construye la ruta del archivo `.parquet` y se carga en un DataFrame Spark.

---

âœ… 6. ConversiÃ³n de columnas que contienen 'Date' o 'date'

```python
for col in columns:
    if "Date" in col or "date" in col:
        df = df.withColumn(
            col, 
            date_format(
                from_utc_timestamp(df[col].cast(TimestampType()), "UTC"),
                "yyyy-MM-dd"
            )
        )
```

ğŸ” Se transforma toda columna que tenga "Date"/"date":

| ğŸ”§ TransformaciÃ³n                | ExplicaciÃ³n                               |
| -------------------------------- | ----------------------------------------- |
| `cast(TimestampType())`          | Convierte el valor a tipo `timestamp`     |
| `from_utc_timestamp()`           | Ajusta zona horaria desde UTC             |
| `date_format(..., "yyyy-MM-dd")` | Deja solo la parte de la **fecha limpia** |
ğŸ“ **Nota:** Esto garantiza que los datos se ajusten al formato estÃ¡ndar esperado por herramientas de BI y otras capas aguas abajo.

---

âœ… 7. Escritura del DataFrame procesado como tabla Delta

```python
output_path = f".../silver@.../SalesLT/{i}/"
df.write.format("delta").mode("overwrite").save(output_path)
```

ğŸ’¾ Los datos transformados se escriben en **Silver** en formato **Delta Lake**, reemplazando lo anterior si existÃ­a.

---

### âš ï¸ Si NO se hace la transformaciÃ³n de fechaâ€¦

| ğŸš¨ Problema                              | ğŸ’¥ Impacto                                     |
| ---------------------------------------- | ---------------------------------------------- |
| `timestamp` con `00:00:00.000`           | Ocupa mÃ¡s espacio y puede confundir a usuarios |
| `group by ModifiedDate` incluye la hora  | Agrupaciones incorrectas por fecha+hora        |
| Problemas con herramientas como Power BI | Interpretaciones errÃ³neas de las fechas        |
| Dificultad de anÃ¡lisis en capas Gold     | Estructura inconsistente y difÃ­cil de depurar  |

---

âœ… ConclusiÃ³n

> Se transforma la columna `ModifiedDate` (y similares) para:
>
> * Limpiar la hora innecesaria (`00:00:00.000`)
> * Mejorar legibilidad y eficiencia
> * Aumentar compatibilidad con herramientas de BI
> * Estandarizar los datos en la arquitectura medallion


### âœ”**Contenedor Silver**

ğŸ“¦ Los datos se encuentran ahora en formato **Delta** en el contenedor Silver:

ğŸ“ `abfss://silver@.../SalesLT/...`

ğŸ“ Ahora puedes **verificar** en el contenedor **Silver** las tablas exportadas en formato **Delta**, lo que indica que el script fue ejecutado exitosamente âœ…


![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img230.png)

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img231.png)


### âœ…**Validando los resultados**

ğŸ” Comparativa: Bronze vs Silver

```python
# Bronze
spark.read.format("parquet").load("abfss://bronze@.../Address/Address.parquet").show(10)

# Silver
spark.read.format("delta").load("abfss://silver@.../Address/").show(10)
```

âœ” Al revisar los datos en Silver, verÃ¡s que `ModifiedDate` ya no tiene hora â†’ solo fecha (`yyyy-MM-dd`).

ğŸ“¸ **Ejemplo visual**

> `2006-07-01` âœ… vs `2006-07-01 00:00:00.000` âŒ

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img234.png)

Como puedes observar el campo `ModifiedDate` ahora contiene el formato correcto


### ğŸ“Œ**Bronze to Silver**

Por Ãºltimo, puedes guardar el Notebook asignÃ¡ndole el nombre _Bronze to Silver_.

ğŸ”– Esto te permitirÃ¡ mantenerlo identificado como parte del flujo de ingestiÃ³n y transformaciÃ³n de datos en la arquitectura medallion.

![](../../../../static/img/0%20-%20Programs/1%20-%20DataEngineering/6%20-%20Azure/img232.png)


### âœ… Â¡TransformaciÃ³n completada con Ã©xito!

ğŸ“Œ Â¡Tus datos estÃ¡n ahora transformados y listos para el anÃ¡lisis en la capa ğŸŸ¡ **Silver**!